---
title: "Dynamic Prediction of Non-Guassian Outcome with fast Generalized Functional Principal Analysis"
output: 
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    latex_engine: pdflatex
fontfamily: mathpazo
fontsize: 11pt
geometry: margin = 1in
bibliography: refs.bib
nocite: '@*'
link-citations: yes
linkcolor: blue
csl: ASA.cls
header-includes:
  - \usepackage{setspace}\doublespacing
  - \usepackage[utf8]{inputenc}
  - \usepackage[T1]{fontenc}
---
<!-- Abstract:  -->
<!-- ================================================================================================================================= -->
<!--   Biomedical investigators are often interested in predicting future observations of subjects based on their historical data, referred to as dynamic prediction. Traditional methods are often limited in flexibility and computationally intensive, especially with non-Gaussian data. To address these issues, we propose a novel method for dynamic prediction based on Generalized Functional Principal Component Analysis (FPCA). Assume the observed outcome follows an exponential family distribution parameterized by a latent Gaussian function, the proposed method consists of the following steps: 1) Bin the data across functional domain into small, equal-length intervals; 2) Fit local generalized mixed models at every bin to estimate individual latent functions; 3) Fit FPCA model to smooth latent functions and 4) Obtain estimates of subject-specific PC scores using partial observations and recover the unobserved part on the binned grid. Our simulation study showed the proposed method achieved significantly better out-of-sample predictive performance compared to existing methods with much shorter computation time, thus has the potential to be widely applicable to large datasets. -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
theme_set(theme_minimal())
library(here)
library(ROCR)
library(kableExtra)
library(knitr)
library(ggpubr)
options(knitr.kable.NA = "")

set.seed(314)

# simulation
load(here("Data/sim_data.RData"))
load(here("Data/SimOutput_fGFPCA.RData"))
load(here("Data/SimOutput_GLMMadaptive.RData"))

# data application
df_appl <- read.csv(here("Data/Daily_Weigh_ins.csv"))
load(here("Data/ApplOutput_fGFPCA.RData"))
load(here("Data/ApplOutput_GLMMadaptive.RData"))

```


Introduction
=================================================================================================================================

Biomedical investigators are often interested in predicting future observations of subjects based on their historical data, typically referred to as dynamic prediction. Traditionally, this type of data has been modeled either marginally, such as generalized estimating equations, or conditionally on specific subjects, such as mixed effect models [@liang1986; @Laird1982; @lindstrom1990; @davidian2003]. For subject-specific models, predictions are made based on the correlation between repeated measures from the same subject and covariates that can be either fixed or varying. Software and methods for dynamic prediction have been developed for Gaussian [@leroux2016; @shang2017; @goldberg2014], non-Gaussian data [@chiou2012; @GLMMadaptive], and time-to-event outcomes [@suresh2017; @jmbook]. However, with the notable exception of @leroux2016, existing methods are either limited in their flexibility (both fixed and random effect structure, e.g. random slope and intercept), computationally unfeasible for large datasets, or both. These issues are particularly acute for data which are measured densely, as is often the case with functional data, or when data are non-Gaussian. 


While methods with accompanying software for modeling complex functional data with non-linear fixed and random effects exist for both Gaussian and non-Gaussian data, software is largely limited to the *refund* package in the statistical software R [@Scheipl2014; @wood2014; @refundpkg]. However, these methods and software implementations do not currently allow for dynamic prediction. In addition to the methods implemented in the refund package, other work has been done on the fitting of functional regression models to non-Gaussian functional data. For example, @chen2013 proposed approaches to fit marginal functional models that is compatible to multi-level, generalized outcomes. @goldsmith2015 established a model framework that takes into account the fixed effect of time-invariant covariates, with parameters estimated with Bayesian method in *Stan*. @gertheiss2016 identified bias introduced by directly applying functional principal analysis (FPCA) methods to generalized functions, and proposed to address this problem using a two-stage joint estimation strategy. @linde2019 used an adapted Bayesian variational algorithm for FPCA of binary and count data. In terms of implementation, @wrobel2019 proposed a fast, efficient way to fit generalized FPCA (GFPCA) on binary data using EM algorithm, accompanied by the an open source R package *registr*. 

Existing research on dynamic prediction with functional data has focused primarily on continuous/Gaussian outcomes, usually modelling subject-specific random effects with FPCA [@chiou2012; @goldberg2014; @shang2017]. @kraus2015 has used this approach to predict missing observations in partially observed function tracks, and @delaigo2016 achieved similar goals using Markov Chains. While methods mentioned above used only partial observations for prediction with an intercept-only model, @leroux2016 proposed Functional Concurrent Regression (FCR) framework which can incorporate the effect of subject-specific predictors. Although methodological development exists, flexible software for implementing these methods is still relatively sparse with the exception of the fcr [@fcrpkg] package accompanying [@leroux2016]. 


In this paper, we aim to develop a fast, scalable method for dynamic prediction of generalized functional outcomes using the principals of local generalized linear mixed effects models combined with functional principal components analysis of the estimated latent processes. The proposed method is designed to handle dense, regularly measured functional data and is extremely computationally efficient relative to the model complexity. Section 2 explains the implementation procedure of the proposed method. In Section 3, we illustrate the performance and efficiency of our proposed method in a simulation study. In Section 4, we apply this method to a real world dataset from a weight loss study. Section 5 presents a discussion of advantages and limitation of the proposed method. 


Method
=================================================================================================================================

The observed data for a single subject $i$ is ($t$, $Y_i(t)$), where $t$ consists of dense, discrete points along the domain of data collection. In practice, $t$ is usually a time index along the duration of study. We hereafter refer to the range of $t$ as the "functional domain" denoted by $T$. For simplicity, we further assume that outcomes are collected on a regular grid over the functional domain, meaning that $t$ are evenly distributed and does not change across subjects. 

$Y_i(t)$ is the non-Gaussian outcome observed at $t$. We assume that the outcome $Y_i(t)$ can be characterized by a latent continuous function $\eta_i(t)$. That is, at a specific t, $Y_i(t)$ follows an exponential family distribution such that:

$$
g[E(Y_i(t))] = \eta_i(t) = \beta_0(t)+b_i(t)
$$


where g is an appropriate link function, $\beta_0(t)$ is the population mean of latent function, and $b_i(t)$ is a subjects-level random effect function following a zero-mean Gaussian process with covariance operator $\Sigma_b$. By the Karhunen-LoÃ¨ve theorem, $b_i(t)$ can be represented as the linear combination of a infinite set of orthogonal functions: $b_i(t)=\sum_{k=1}^{\infty}\xi_{ik}\phi_{k}(t)$. In practice, $b_i(t)$ is approximated using a finite $K$ number of basis functions. 

Estimation of $b_i(t)$ involves estimating both $\{\phi_k(t): t \in T, , 1 \leq k \leq K\}$ and $\{\xi_{ik}: 1 \leq i \leq N, 1 \leq k \leq K\}$. For Gaussian data this may be done by directly estimating the covariance operator $\Sigma_b$ using either method of moments (if the data are relatively small) or faster methods such as fast covariance estimation (FACE) [@face]. However, for non-Gaussian data, direct estimation is not possible. Instead, one may choose to estimate $b_i(t)$ first using a flexible spline basis  (e.g. B-splines)  with dimension $K^B$ such that $K^B > K$, assuming an unstructured correlation between subject-specific spline coefficients, then orthogonalize the resulting estimates via eigendecomposition or singular value decomposition. This general approach forms the basis for estimation for several GFPCA methods [@goldsmith2015; @wrobel2019]. However, this approach results in rapidly escalating computational burden as both the number of subjects, $N$, and the density of the observed functional predictor increases. Note that if the covariance operator $\Sigma_b$ is relatively complex, then $K$ may be large and not well approximated using simple basis functions, $\phi_k$ (e.g. random intercept and slope). 

Beyond the computational burden associated with fitting the model of interest, for dynamic prediction one must then estimate the scores for a new subject $u$ not used in model fitting. Typically for non-Gaussian data, this takes the form of the maximum likelihood estimator (MLE) of the scores $\hat{\xi}_{uk} = \underset{\xi_{uk}}{\mathrm{argmax}} L(\xi_{uk}|\boldsymbol{\Theta}, \boldsymbol{Y}_{u})$, where $\bf{\Theta}$ represents the set of parameters estimated by the model (i.e. parameters associated with $\beta_0(t), \Sigma_b$) and $\boldsymbol{Y}_u$ are the non-Gaussian data for the new subject. Estimation of the random effects for a new subject is non-trivial and not implemented for even very simple models in most random effects software packages such as *lme4* [@lme4] and *nlme* [@nlme]. Our proposed method bypasses this problem by using local generalized linear mixed models (GLMMs) to estimate the covariance operator $\Sigma_b$ directly on the latent scale, then using the MLE of Gaussian random effects. Estimation is made fast for even very large data through the use of the FACE methodology for FPCA on the latent estimates obtained from local models. Because the local models are very simple GLMMs, estimation is also fast for the local models. 


**Fast Generalized FPCA** Based on the problem set up above, we propose the following algorithm for fast, generalizable implementation of FPCA (fGFPCA) on the unobserved latent process $\eta_i(t)$:

1. Bin the observed outcomes in to small, non-overlapping, equal length intervals. We hereafter index the bins by their midpoints $s$. The number of observations from the same subject in bin $s$ is $n_s$, and $t_{sj}$ refers to the jth observation point in bin $s$.

2. Fit a local Generalized Mixed Model at every bin. Specifically, at bin $s$, we fit $g[E(Y_i(t_{sj}))] = \beta_0(s)+b_i(s)$,  $j\in\{1, ..., n_s\}$. From this series of models we can estimate the individual latent functions at every bin: $\hat{\eta}_i(s) = \hat{\beta}_0(s)+\hat{b}_i(s)$. 

3. Fit FPCA on the estimated latent functions: $\hat{\eta}_i(s) = f_0(s)+\sum_{k=1}^K\xi_{ik}\phi_{k}(s)+\epsilon_i(s)$, and obtain $\hat{\boldsymbol{\Theta}}$ which includes the estimates of basis functions $\boldsymbol{\Phi} = \{\phi_1(s), ...,\phi_k(s)\}$, eigenvalues $\hat{\lambda}_1...\hat{\lambda}_k$, mean function $\hat{f}_0(s)$ and residual variance $\hat{\sigma}^2$. 


<!-- **Ying, include the formulas for your conditional expectation here to obtain random effects estimates ** -->

**Dynamic prediction** Now assume we have a new observations $u$ with partially observed outcome $\boldsymbol{Y}_u = \{Y_u(t), t\leq t_m\}$. The maximum observed point is $t_m$, which is a point along the functional domain and belongs to bin $s_m$ on the binned grid from step 1 above. With components extracted from the FPCA model in step 3, we will be able to make predictions on the future, unobserved bins. Specifically, with the partially observed binary function $Y_u(t)$, we will be able to calculate the MLE of the subject-specific PC scores $\hat{\boldsymbol{\xi}}_u= \{\hat{\xi}_{u1},...,\hat{\xi}_{uK}\}$ that maximizes its posterior likelihood: 


$$\hat{\boldsymbol{\xi}}_u = \underset{\boldsymbol{\xi}_u}{\mathrm{argmax}} \ L(\boldsymbol{\xi}_u|\boldsymbol{Y}_u, 
\hat{\boldsymbol{\Theta}})= \frac{p(\boldsymbol{Y}_u|\boldsymbol{\xi}_u, \hat{\boldsymbol{\Theta}})p(\boldsymbol{\xi}_u|\hat{\boldsymbol{\Theta}})}{\int p(\boldsymbol{Y}_u|\boldsymbol{\xi}_u,\hat{\boldsymbol{\Theta}})p(\boldsymbol{\xi}_u|\hat{\boldsymbol{\Theta}})d\boldsymbol{\xi}_u}$$

Then the value of latent functions at unobserved points can be estimated as $\hat{\eta_u}(s)=\hat{f}_0(s)+\sum_{k=1}^K\hat{\xi}_{uk}{\phi}_k(s)$. In fact, since the denominator in the formula above is constant with respect to $\boldsymbol{\xi}_u$, we only need to maximum the numerator. Since $\boldsymbol{Y}_u|\boldsymbol{\xi}_u$ follows an exponential family distribution and $\boldsymbol{\xi}_u$ follows multivariate Guassian distribution, it could be numerically maximized efficiently. 

Following the algorithm above, predictions of individual latent functions are made on the binned grid based on partially observed tracks on the un-binned grid. Since the bins are set up to be small in length, the binned grid would still be dense. However, in situations where predictions on the original, un-binned grid is needed, linear interpolation of estimated latent function tracks turns out to be a fast, convenient way with good performance for prediction at points between bins. 


Simulation
=================================================================================================================================

In this section, we illustrate the predictive performance and computational efficiency of the proposed method through a simulation study. We simulated 50 datasets, each with 500 subjects. For every subject, we generate 1000 binary outcomes $Y_i(t) \in \{0, 1\}$ across functional domain $t \in (0, 1000]$, where the distribution of outcome is characterized by a continuous latent function. The data generation mechanism can be expressed as follows: 

$$\begin{aligned}
Y_i(t) & \sim Binomial(\frac{exp(\eta_i(t))}{1+exp(\eta_i(t))}) \\
\eta_i(t) &= f_0(t)+ \xi_{i1}\sqrt{2}sin(2\pi t)+\xi_{i2}\sqrt{2}cos(2\pi t)+\xi_{i3}\sqrt{2}sin(4\pi t)+\xi_{i4}\sqrt{2}cos(4\pi t)
\end{aligned}$$

In this simulation, we set $f_0(t) = 0$. $\xi_{ik}$ are mutually independent normal random variables $\xi_{ik}\sim N(0, \lambda_k)$. Here we set the values of $\lambda_k$ to be $0.5^{k-1}$, $k \in \{1,..., 4\}$. In addition, for simplicity of presentation, we generate data on a regular grid, which means observations points are equally distributed across $(0, 1000]$ and are the same for all subjects. 

We use two metrics to evaluate the out-of-sample predictive performance: integrated squared error (ISE) and Area-Under-the-Receiver-Operator-Curve (AUC). ISE assesses the prediction accuracy of latent continuous functions. It is evaluated on the binned grid at midpoints of each unobserved bin. With observations up to bin $s_m$, ISE is defined as $\frac{1}{N}\sum_{i=1}^N\sum_{s>s_m} (\hat{\eta}_i(s)-\eta_i(s))^2$. The second metric, AUC, focuses on evaluation of prediction of the binary outcome. Since the binary outcomes are generated on the original, un-binned grid, we evaluated AUC on this grid by estimating values of latent functions between bins with linear interpolation. 

As a reference method, we compare our method to Generalized Linear Mixed Models using Adaptive Gaussian Quadrature (GLMMadaptive)[@GLMMadaptive]. This is one of the fastest existing method developed for dynamic prediction of repeated generalized outcomes. Just like many mixed models, this method is very limited in terms of flexibility if one wishes to obtain estimates in a reasonable time frame. For example, the model used for prediction of our simulated datasets would simply be an linear model with one covariate indicating observation time: $g(E(Y_i)) = \beta_0+\beta_1t+b_{i0}+b_{i1}t$. While the flexibility of this mixed model can be increased using spline functions, the dimension of spline basis is also restricted by computational ability, and is unfeasible to implement under the scale of our simulated data or the complexity of our proposed method.

The average ISE and AUC across all simulation is presented in Table 1, and Figure 1 presents the predicted latent function of four randomly selected subjects from the first simulated dataset. The prediction is made conditioning on different length of observed track (specifically, with observations up to t = 200, 400, 600, 800 respectively), and evaluation is made on equal-length time windows on the unobserved tracks following the maximum observation time. As Table 1 reveals, the proposed method (fGFPCA) outperforms GLMMadaptive under every scenario, which was expected since the linear model structure of GLMM is too simple for the cyclic data generation mechanism. In addition, fGFPCA also took less computation time. For one simulated dataset, fGFPCA spent `r round(mean(runtime),2)` minutes on model fitting and out-of-sample prediction overall, while GLMMadaptive took `r round(mean(runtime_ref),2)` minutes. 

Because of the flexibility of our proposed model framework, the accuracy of prediction at specific time points would improve as more observed data is collected. This is revealed in Table 1 as ISE decreases and AUC increases with maximum observed time (left-to-right), also in Figure 1 as predicted curves get closer to the true latent function with longer observed track. However the same tendency is not observed with GLMMadaptive models, as a result of restricted model flexibility. A linear model that fits well to a specific part can fit very badly to other parts along the underlying latent function tracks, especially when the shapes of functions are wiggly or cyclic. Therefore, more observations do not necessarily make the model more predictive. 


```{r}
# other quantities
mid <- unique(pred_lst[[1]]$bin)
N <- 500
M <- length(pred_lst)
J <- 1000

# for fGFPCA
# code prediction value at observed points as NA
for(m in 1:length(pred_lst)){
  pred_lst[[m]]$pred_t200[pred_lst[[m]]$bin <= 200] <- NA
  pred_lst[[m]]$pred_t400[pred_lst[[m]]$bin <= 400] <- NA
  pred_lst[[m]]$pred_t600[pred_lst[[m]]$bin <= 600] <- NA
  pred_lst[[m]]$pred_t800[pred_lst[[m]]$bin <= 800] <- NA
}

# for GLMM adaptive
# add in observation time index
for(m in 1:length(pred_lst_ref)){
  pred_lst_ref[[m]]$sind_inx <- rep(1:J, N)
}
```


```{r Figure, fig.cap="Predicted tracks of four randomly selected subjects from one simulated dataset. The grey solid line indicates the true latent continuous function. The dashed lines indicate predicted latent function tracks, and color indicates different observation time.", fig.height=6, fig.width=12}
# figure of fGFPCA prediction
cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

# true data
df_true_sim1 <- sim_data[[1]] %>% select(id, sind_inx, eta_i)

# prediction results used for visualization
exp_df_sim1 <- pred_lst[[1]]

exp_df_sim1_ref <- pred_lst_ref[[1]]

# randomly draw four samples for visualization
rand_id <- sample(exp_df_sim1$id, 4)

# fGFPCA
fig1a <- exp_df_sim1 %>%
  left_join(df_true_sim1 %>% filter(sind_inx %in% mid) %>% rename(bin=sind_inx), by = c("id", "bin")) %>%
  filter(id %in% rand_id) %>%
  ggplot()+
  geom_line(aes(x=bin, y=eta_i, col = "True"))+
  geom_line(aes(x=bin, y = pred_t200, col = "Up to 200"), linetype = "dashed", na.rm = T)+
  geom_line(aes(x=bin, y = pred_t400, col = "Up to 400"), linetype = "dashed", na.rm = T)+
  geom_line(aes(x=bin, y = pred_t600, col = "Up to 600"), linetype = "dashed", na.rm = T)+
  geom_line(aes(x=bin, y = pred_t800, col= "Up to 800"), linetype = "dashed", na.rm = T)+
  facet_wrap(~id, scales = "free_y")+
  labs(y = "Latent function", x = "Bin", colour= "Observed time", title = "fGFPCA")+
  scale_color_manual(values=cbPalette)+
  scale_x_continuous(breaks = c(0, 200, 400, 600, 800, 1000))

# GLMMadaptive
fig1b <- exp_df_sim1_ref %>%
  filter(sind_inx %in% mid) %>%
  filter(id %in% rand_id) %>%
  ggplot()+
  geom_line(aes(x=sind_inx, y=eta_i, col = "True"))+
  geom_line(aes(x=sind_inx, y = pred_t200, col = "Up to 200"), linetype = "dashed", na.rm = T)+
  geom_line(aes(x=sind_inx, y = pred_t400, col = "Up to 400"), linetype = "dashed", na.rm = T)+
  geom_line(aes(x=sind_inx, y = pred_t600, col = "Up to 600"), linetype = "dashed", na.rm = T)+
  geom_line(aes(x=sind_inx, y = pred_t800, col= "Up to 800"), linetype = "dashed", na.rm = T)+
  facet_wrap(~id, scales = "free_y")+
  labs(y = "Latent function", x = "Bin", colour= "Observed time", title = "GLMMadaptive")+
  scale_color_manual(values=cbPalette)+
  scale_x_continuous(breaks = c(0, 200, 400, 600, 800, 1000))

ggarrange(fig1a, fig1b, nrow = 1, common.legend = T)


```


```{r ISE_fGFPCA}
# calculate ISE on binned grid
ise_mat <- array(NA, dim = c(5, 4, M)) # dimensions are: predicted time, max obs time, simulation

for(m in 1:M){
  
  # true value at midpoints
  true_eta <- sim_data[[m]] %>% filter(sind_inx %in% mid) %>% select(id, sind_inx, eta_i) %>% rename(bin=sind_inx)
  pred_eta <- pred_lst[[m]] %>% select(-eta_hat)
  pred_eta <- pred_eta %>% left_join(true_eta, by = c("id", "bin"))
  
  # split unobserved time into intervals of length 200
  # [0, 200], (200, 400], (400, 600], (600, 800], (800, 1000]
  pred_eta$bin_cat <- cut(pred_eta$bin, breaks=c(0, 200, 400, 600, 800, 1000), include.lowest = T)
  
  # SE at each observed time
  pred_eta <- pred_eta %>% mutate(ise_t200 = (pred_t200-eta_i)^2,
                                  ise_t400 = (pred_t400-eta_i)^2,
                                  ise_t600 = (pred_t600-eta_i)^2,
                                  ise_t800 = (pred_t800-eta_i)^2)
  
  # subject-average SE
  ise_mat[, , m] <- pred_eta %>% group_by(bin_cat) %>% 
    summarize_at(vars(starts_with("ise")), .fun=function(x)(sum(x)/N)) %>%select(-bin_cat) %>% as.matrix()
}

ave_ise_mat <- apply(ise_mat, MARGIN = c(1, 2), mean)[-1, ]
colnames(ave_ise_mat) <- c("200", "400", "600", "800")
rownames(ave_ise_mat) <- c("(200, 400]", "(400, 600]", "(600, 800]", "(800, 1000]")


```

```{r, ISE_GLMMadaptive}
# calculate ISE on binned grid
ise_mat_ref <- array(NA, dim = c(5, 4, M)) # dimensions are: predicted time, max obs time, simulation

for(m in 1:M){
  
  # true value at midpoints
  pred_eta <- pred_lst_ref[[m]] %>% filter(sind_inx %in% mid)
  
  # split unobserved time into intervals of length 200
  # [0, 200], (200, 400], (400, 600], (600, 800], (800, 1000]
  pred_eta$bin_cat <- cut(pred_eta$sind_inx, breaks=c(0, 200, 400, 600, 800, 1000), include.lowest = T)
  
  # SE at each observed time
  pred_eta <- pred_eta %>% mutate(ise_t200 = (pred_t200-eta_i)^2,
                                  ise_t400 = (pred_t400-eta_i)^2,
                                  ise_t600 = (pred_t600-eta_i)^2,
                                  ise_t800 = (pred_t800-eta_i)^2)
  
  # subject-average SE
  ise_mat_ref[, , m] <- pred_eta %>% group_by(bin_cat) %>% 
    summarize_at(vars(starts_with("ise")), .fun=function(x)(sum(x)/N)) %>%select(-bin_cat) %>% as.matrix()
  
}

ave_ise_mat_ref <- apply(ise_mat_ref, MARGIN = c(1, 2), mean)[-1, ]
colnames(ave_ise_mat_ref) <- c("200", "400", "600", "800")
rownames(ave_ise_mat_ref) <- c("(200, 400]", "(400, 600]", "(600, 800]", "(800, 1000]")

# cbind(ave_ise_mat, ave_ise_mat_ref)
```


```{r AUC_fGFPCA}
# maximum observations time
t_vec <- c(200, 400, 600, 800)

# calculate AUC on unbinned grid
auc_mat <- array(NA, dim = c(4, 4, M)) # row index iteration, col index max observation time

for(m in 1:M){
  
  Y_df <- sim_data[[m]]
  eta_pred <- pred_lst[[m]]
 
  # use interpolation to extend prediction to the unbinned grid
  for(i in 1:N){
   Y_df$pred_200[Y_df$id==i] <- approx(x=eta_pred$bin[eta_pred$id==i], y=eta_pred$pred_t200[eta_pred$id==i], xout = 1:J)$y
   Y_df$pred_400[Y_df$id==i] <- approx(x=eta_pred$bin[eta_pred$id==i], y=eta_pred$pred_t400[eta_pred$id==i], xout = 1:J)$y
   Y_df$pred_600[Y_df$id==i] <- approx(x=eta_pred$bin[eta_pred$id==i], y=eta_pred$pred_t600[eta_pred$id==i], xout = 1:J)$y
   Y_df$pred_800[Y_df$id==i] <- approx(x=eta_pred$bin[eta_pred$id==i], y=eta_pred$pred_t800[eta_pred$id==i], xout = 1:J)$y
  }
  
  # split unobserved time into intervals of length 200
  # [0, 200], (200, 400], (400, 600], (600, 800], (800, 1000]
  Y_df$bin_cat <- cut(Y_df$sind_inx, breaks=c(0, 200, 400, 600, 800, 1000), include.lowest = T)
  Y_df_lst <- split(Y_df, f = Y_df$bin_cat)
  Y_df_lst <- Y_df_lst[-1]
  
  for(j in seq_along(t_vec)){
    
    t <- t_vec[j]
    pred_name <- paste("pred_", t, sep = "")
    
    # calculate AUC
    auc_lst <- lapply(Y_df_lst[j:4], 
                       function(x){
                         row_id <- !is.na(x[, pred_name])
                         this_eta <- x[row_id, pred_name]
                         this_Y <- x[row_id, "Y"]
                         auc_pref <- performance(prediction(this_eta, this_Y), "auc")
                         return(auc_pref@y.values)
                })
      
    auc_vec <- c(rep(NA, 4-length(auc_lst)), unlist(auc_lst))
    auc_mat[ , j, m] <- auc_vec

}}


ave_auc_mat <- apply(auc_mat, MARGIN = c(1, 2), mean)
colnames(ave_auc_mat) <- c("200", "400", "600", "800")
rownames(ave_auc_mat) <- c("(200, 400]", "(400, 600]", "(600, 800]", "(800, 1000]")

```

```{r AUC_GLMMadaptive}
# calculate AUC on unbinned grid
auc_mat_ref <- array(NA, dim = c(4, 4, M)) # row index iteration, col index max observation time

for(m in 1:M){
  
  Y_df <- sim_data[[m]]
  eta_pred <- pred_lst_ref[[m]]
  
  Y_df <- Y_df %>% left_join(eta_pred %>% select(id, sind_inx, starts_with("pred_")), by = c("id", "sind_inx"))
 
  # split unobserved time into intervals of length 200
  # [0, 200], (200, 400], (400, 600], (600, 800], (800, 1000]
  Y_df$bin_cat <- cut(Y_df$sind_inx, breaks=c(0, 200, 400, 600, 800, 1000), include.lowest = T)
  Y_df_lst <- split(Y_df, f = Y_df$bin_cat)
  Y_df_lst <- Y_df_lst[-1]
  
  for(j in seq_along(t_vec)){
    
    t <- t_vec[j]
    pred_name <- paste("pred_t", t, sep = "")
    
    # calculate AUC
    auc_lst <- lapply(Y_df_lst[j:4], 
                       function(x){
                         row_id <- !is.na(x[, pred_name])
                         this_eta <- x[row_id, pred_name]
                         this_Y <- x[row_id, "Y"]
                         auc_pref <- performance(prediction(this_eta, this_Y), "auc")
                         return(auc_pref@y.values)
                })
      
    auc_vec <- c(rep(NA, 4-length(auc_lst)), unlist(auc_lst))
    auc_mat_ref[ , j, m] <- auc_vec

  }
}



ave_auc_mat_ref <- apply(auc_mat_ref, MARGIN = c(1, 2), mean)
colnames(ave_auc_mat_ref) <- c("200", "400", "600", "800")
rownames(ave_auc_mat_ref) <- c("(200, 400]", "(400, 600]", "(600, 800]", "(800, 1000]")


```

```{r format table}
ise_tb <- cbind(ave_ise_mat, ave_ise_mat_ref)
auc_tb <- cbind(ave_auc_mat, ave_auc_mat_ref)


rbind(ise_tb, auc_tb) %>%
  kable(booktabs = T, digit=2,
        caption = "Predictive performance of fGFPCA and GLMMadaptive on the simulated datasets. ISE and AUC are average values across all 50 simulations.",
        align = "lcccccccc") %>%
  kable_styling() %>%
  add_header_above(c(" " = 1, "fGFPCA" = 4, "GLMMadaptive" = 4)) %>%
  add_header_above(c(" " = 1, "Maximum observed time" = 8)) %>% 
  group_rows(index = c("Prediction time window" = 8)) %>%
  group_rows(index = c("ISE" = 4, "AUC" = 4)) 
```



<!-- -	Repeat simulation: repeat 10-100 times -->

<!-- -	Different set-up: -->
<!-- a.	Different eigenfunctions: with or without periodicity (start with the current one) -->
<!-- b.	Outcome: binary or count -->
<!-- c.	Sample size: start with N=500 -->
<!-- d.	Grid density: start with J=1000 -->
<!-- e.	Bin width, overlap or not: start with non-overlap, bin width = 10 (100 bins) -->

Data application
=================================================================================================================================

In this section, we illustrate the performance of our proposed method using data from a randomized trial on weight loss at University of Colorado Anschutz Medical Campus [@bothwell2022; @ostendorf2022]. The trial originally aimed to study the effect of dietary strategies on weight loss, and participants were given a smart scale for daily at-home weighing. This record was then scaled into a binary indicator of participant adherence to the daily-weigh-in procedure, where 1 indicates participants followed the requirement and 0 otherwise. In this section, we will use this series of daily indicators as the binary functional outcome, and use the proposed method to dynamically predict participant adherence on future days given historical track. 

The dataset includes 55 participants aged 22-56 at baseline. Adherence to daily weigh-in is available along 400 consecutive days for all participants. In other words, for each individual, 400 binary measurements were taken on a regular grid along the functional domain (number of days into the study). We will fit both fGFPCA and GLMMadaptive models and compare their performance on this data. For fGFPCA, every 10 observations are binned together, resulting in 40 bins for final model fitting. Linear interpolation was used for extension from the binned grid back to the un-binned grid. Since the latent continuous function tracks are not known in this case, prediction performance is evaluated with AUC alone. 

Table 2 compares the predictive performance of both models. Similar to the simulation study, prediction is made conditioning on observations up to the 100th, 200th and 300th day respectively, and evaluation is made every 100 days following the maximum observation time. As is revealed, when the observed time is short (up to 100th day), the two method performed similarly when predicting recent days, however fGFPCA did better in predicting further days along the trial. As more observations coming in, fGFPCA sees increasingly higher AUC compared to GLMMadaptive model. This is a result of the difference in model flexibility of the two methods. While the linear relationship in underlying latent functions are not likely to change immediately following the observed track, the whole track could be much more complex. Therefore, while fGFPCA performs similarly to GLMMadaptive right after the end of observation, it does a much better job capturing the underlying pattern along the whole track. Figure 2 further illustrate such difference using predicted latent function tracks of four randomly drawn subjects.


```{r}
# data clean
mid2 <- unique(df_est_latent$bin)
N2 <- max(df_est_latent$id)
J2 <- 400

# for fGFPCA
# code prediction value at observed points as NA
df_est_latent$pred_t100[df_est_latent$bin <= 100] <- NA
df_est_latent$pred_t200[df_est_latent$bin <= 200] <- NA
df_est_latent$pred_t300[df_est_latent$bin <= 300] <- NA


# for GLMM adaptive
# add in observation time index
for(m in 1:length(pred_lst_ref)){
  pred_lst_ref[[m]]$sind_inx <- rep(1:J, N)
}
```

```{r AUC_fGFPCA_Appl}
# maximum observations time
t_vec2 <- c(100, 200, 300)

# calculate AUC on unbinned grid
auc_mat2 <- matrix(NA, nrow = 3, ncol = 3) # row index iteration, col index max observation time

# result container for interpolation
Y_df2 <- df_appl %>% select(participant_id, starts_with("day")) %>% 
  pivot_longer(2:401, values_to = "Y", names_to = "sind_inx") %>%
  mutate(sind_inx = gsub("day_", "", sind_inx)) %>%
  mutate(sind_inx = as.numeric(sind_inx)) %>%
  rename("id" = "participant_id")
Y_df2$pred_100 <- Y_df2$pred_200 <- Y_df2$pred_300 <- NA

  # use interpolation to extend prediction to the unbinned grid
for(i in 1:N2){
   Y_df2$pred_100[Y_df2$id==i] <- approx(x=df_est_latent$bin[df_est_latent$id==i], 
                                         y=df_est_latent$pred_t100[df_est_latent$id==i], 
                                         xout = 1:J2)$y
   Y_df2$pred_200[Y_df2$id==i] <- approx(x=df_est_latent$bin[df_est_latent$id==i], 
                                         y=df_est_latent$pred_t200[df_est_latent$id==i], 
                                         xout = 1:J2)$y
   Y_df2$pred_300[Y_df2$id==i] <- approx(x=df_est_latent$bin[df_est_latent$id==i], 
                                         y=df_est_latent$pred_t300[df_est_latent$id==i], 
                                         xout = 1:J2)$y
  }
  
# split unobserved time into intervals of length 100
Y_df2$bin_cat <- cut(Y_df2$sind_inx, breaks=c(0, 100, 200, 300, 400), include.lowest = T)
Y_df_lst2 <- split(Y_df2, f = Y_df2$bin_cat)
Y_df_lst2 <- Y_df_lst2[-1]
  
for(j in seq_along(t_vec2)){
    
    t <- t_vec2[j]
    pred_name <- paste("pred_", t, sep = "")
    
    # calculate AUC
    auc_lst <- lapply(Y_df_lst2[j:3], 
                       function(x){
                         row_id <- !is.na(x[, pred_name])
                         this_eta <- x[row_id, pred_name]
                         this_Y <- x[row_id, "Y"]
                         auc_pref <- performance(prediction(this_eta, this_Y), "auc")
                         return(auc_pref@y.values)
                })
      
    auc_vec <- c(rep(NA, 3-length(auc_lst)), unlist(auc_lst))
    auc_mat2[ , j] <- auc_vec

}


colnames(auc_mat2) <- c("100", "200", "300")
rownames(auc_mat2) <- c("(100, 200]", "(200, 300]", "(300, 400]")

```

```{r AUC_GLMMadaptive_Appl}
# calculate AUC on unbinned grid
auc_mat_ref2 <- matrix(NA, nrow = 3, ncol = 3)  # row index iteration, col index max observation time

# split unobserved time into intervals of length 100
df_est_latent_ref$bin_cat <- cut(df_est_latent_ref$sind_inx, 
                                 breaks=c(0, 100, 200, 300, 400), include.lowest = T)
Y_df_lst2_ref <- split(df_est_latent_ref, f = df_est_latent_ref$bin_cat)
Y_df_lst2_ref <- Y_df_lst2_ref[-1]
  
for(j in seq_along(t_vec2)){
    
    t <- t_vec2[j]
    pred_name <- paste("pred_t", t, sep = "")
    
    # calculate AUC
    auc_lst <- lapply(Y_df_lst2_ref[j:3], 
                       function(x){
                         row_id <- !is.na(x[, pred_name])
                         this_eta <- x[row_id, pred_name]
                         this_Y <- x[row_id, "Y"]
                         auc_pref <- performance(prediction(this_eta, this_Y), "auc")
                         return(auc_pref@y.values)
                })
      
    auc_vec <- c(rep(NA, 3-length(auc_lst)), unlist(auc_lst))
    auc_mat_ref2[ , j] <- auc_vec

  }


colnames(auc_mat_ref2) <- c("100", "200", "300")
rownames(auc_mat_ref2) <- c("(100, 200]", "(200, 300]", "(300, 400]")


```


```{r, Figure_appl, fig.cap="Predicted tracks of four randomly selected subjects from the daily weigh-in dataset. The grey points are observed binary outcomes. The dashed lines indicate predicted latent function tracks, and color indicates different observation time.", fig.height=6, fig.width=12}
rand_id2 <- sample(N2, 4)
fig2a <- Y_df2 %>%
  filter(id %in% rand_id2) %>%
  mutate_at(vars(pred_100, pred_200, pred_300), function(x)exp(x)/(1+exp(x))) %>%
  ggplot()+
  geom_point(aes(x=sind_inx, y=Y, col = "True outcome"), size = 0.5)+
  geom_line(aes(x=sind_inx, y = pred_100, col = "Up to 100"), linetype = "dashed", na.rm = T)+
  geom_line(aes(x=sind_inx, y = pred_200, col = "Up to 200"), linetype = "dashed", na.rm = T)+
  geom_line(aes(x=sind_inx, y = pred_300, col = "Up to 300"), linetype = "dashed", na.rm = T)+
  facet_wrap(~id, scales = "free_y")+
  labs(y = "Predicted latent function on probability scale", x = "Bin", 
       colour= "Observed time", title = "fGFPCA")+
  scale_color_manual(values=cbPalette)+
  scale_x_continuous(breaks = c(0, 100, 200, 300, 400))

fig2b <- df_est_latent_ref %>%
  filter(id %in% rand_id2) %>%
  mutate_at(vars(pred_t100, pred_t200, pred_t300), function(x)exp(x)/(1+exp(x))) %>%
  ggplot()+
  geom_point(aes(x=sind_inx, y=Y, col = "True outcome"), size = 0.5)+
  geom_line(aes(x=sind_inx, y = pred_t100, col = "Up to 100"), linetype = "dashed", na.rm = T)+
  geom_line(aes(x=sind_inx, y = pred_t200, col = "Up to 200"), linetype = "dashed", na.rm = T)+
  geom_line(aes(x=sind_inx, y = pred_t300, col = "Up to 300"), linetype = "dashed", na.rm = T)+
  facet_wrap(~id, scales = "free_y")+
  labs(y = "Predicted latent function on probability scale", x = "Bin", 
       colour= "Observed time", title = "GLMMadaptive")+
  scale_color_manual(values=cbPalette)+
  scale_x_continuous(breaks = c(0, 100, 200, 300, 400))

ggarrange(fig2a, fig2b, nrow = 1, common.legend = T)
```

```{r}
cbind(auc_mat2, auc_mat_ref2) %>%
  kable(booktabs = T, digit=2,
        caption = "AUC of fGFPCA and GLMMadaptive on the daily weigh-in dataset",
        align = "lcccccccc") %>%
  kable_styling() %>%
  add_header_above(c(" " = 1, "fGFPCA" = 3, "GLMMadaptive" = 3)) %>%
  add_header_above(c(" " = 1, "Maximum observed time" = 6)) %>% 
  group_rows(index = c("Prediction time window" = 3))

```

Discussion
=================================================================================================================================

The simulation study and data application above have illustrated the feasibility and utility of the proposed method. Even for large datasets and complex latent processes/random effect structures, fGFPCA can achieve better predictive performance with much less computational burden. Compared to competing methods, fGFPCA can accommodate extremely flexible correlation structure between repeated measure, which is particularly useful when outcome is generated from latent functions with highly non-linear patterns. Due to the comparative flexibility of the fGFPCA approach, fGFPCA has much lower ISE and higher AUC than GLMM. As expected, the dynamic prediction accuracy of fGFPCA improves as more data are available for predictions. Moreover, fGFPCA also significantly reduce time spent on both model fitting and prediction, allowing for scaling of models and predictions which are infeasible with existing approaches. In fact, as far as we are aware of, the proposed method is the only feasible method that can handle the scale of the simulated datasets with compatible flexibility. 


<!-- **Andrew: I would delete this paragraph, honestly** -->

<!-- While the proposed method achieved good accuracy of point prediction, it is more complicated to develop a straightforward, interpretable metric to evaluate the precision of prediction. The challenge arises from the fact that fitting local GLMMs (Step 2) and FPCA (Step 3) can both introduce uncertainty to the final prediction. Since they are implemented consecutively, it is not clear how the uncertainty from both procedures should be integrated into one single interval estimator. In practice, one could consider developing interval predictions conditional on the local GLMMs, quantifying uncertainty of individual score estimates from FPCA model alone. Since the score $\hat{\xi}_{ik}$ is essentially a maximum likelihood estimate, it is natural to estimate its variance based on likelihood theory with observed information ${I}(\hat{\xi}_{ik})$. Therefore, the variation of prediction interval is:  -->

<!-- $$Var(\hat{\eta_i}(s)-\eta_i(s))=\boldsymbol{\Phi}(s)I(\hat{\boldsymbol{\xi}}_i)\boldsymbol{\Phi}^T(s)+\hat{\sigma_{\epsilon}}^2$$ -->
<!-- Where $\boldsymbol{\Phi}(s)=(\phi_1(s)...\phi_K(s))$ and $\hat{\boldsymbol{\xi}}_i=(\hat{\xi}_{i1}, ...,\hat{\xi}_{iK})$. -->


A few open questions remain with the fGFPCA approach. The choices involved in the binning procedure in Step 1 can also affect the final predictive performance, thus further investigations are needed to quantify such effects. Predictive accuracy and computational speed can change with bin width, number of observations in each bin, whether the bins overlap with each other, also whether the bins are equally spaced along the functional grid. Moreover, if the bins are very narrow with only a few observations, local models may be non-identifiable, resulting in bins with no point estimates. While this may not overly impact final point predictions, it is currently unclear as to when this issue warrants concern. In practice, we expect this to happen when the measurements are sparse or sample size is small. However, the methodology here is designed for large, dense functional data and thus not currently appropriate in such scenarios, in which case existing methods may be sufficiently flexible and computationally feasible. 


<!-- -	Score bias: cannot demonstrate without repeat simulation -->

<!-- - Add section of de-bias score: conditional on subject. Need repeat simulation to demonstrate -->

References
=================================================================================================================================
<div id="refs"></div>