---
title: "Dynamic Prediction of Non-Guassian Outcome with fast Generalized Functional Principal Analysis"
author:
  - Ying Jin
  - Andrew Leroux
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    latex_engine: pdflatex
fontfamily: mathpazo
fontsize: 11pt
geometry: margin = 1in
bibliography: refs.bib
nocite: '@*'
link-citations: yes
linkcolor: blue
csl: ASA.cls
header-includes:
  - \usepackage{setspace}\doublespacing

---
<!-- Abstract:  -->
<!-- ================================================================================================================================= -->
<!--   Biomedical investigators are often interested in predicting future observations of subjects based on their historical data, referred to as dynamic prediction. Traditional methods are often limited in flexibility and computationally intensive, especially with non-Gaussian data. To address these issues, we propose a novel method for dynamic prediction based on Generalized Functional Principal Component Analysis (FPCA). Assume the observed outcome follows an exponential family distribution parameterized by a latent Gaussian function, the proposed method consists of the following steps: 1) Bin the data across functional domain into small, equal-length intervals; 2) Fit local generalized mixed models at every bin to estimate individual latent functions; 3) Fit FPCA model to smooth latent functions and 4) Obtain estimates of subject-specific PC scores using partial observations and recover the unobserved part on the binned grid. Our simulation study showed the proposed method achieved significantly better out-of-sample predictive performance compared to existing methods with much shorter computation time, thus has the potential to be widely applicable to large datasets. -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(here)
library(ROCR)

set.seed(314)
```


Introduction
=================================================================================================================================

<!-- -	Overview of dynamic prediction methods -->

Biomedical investigators are often interested in predicting future observations of subjects based on their historical data, typically referred to as dynamic prediction. Traditionally, this type of data has been modeled using marginal models (generalized estimating equations) or conditional models (mixed effect models) [@liang1986; @Laird1982; @lindstrom1990; @davidian2003], and predictions are made based on the correlation between repeated measures from the same subject, and/or covariates that can be either fixed or time-varying. However, these methods are limited in terms of flexibility of correlation structure and the ability to handle out-of-sample prediction. When sample size is large or the density of repeated measures is high, they also tend to cause severe computational burden [@GLMMadaptive]. 

To address these problems, one may turn to functional mixed effect models instead when measures are dense across the domain. Such methods accommodate more flexible correlation structure by modeling subject-specific random effects as a function, and non-parametric smoothing [@Scheipl2014] can be incorporated to speed up the computation, such as spline basis functions or eigenfunctions from functional principal component analysis (fPCA). The introduction of basis functions also makes out-of-sample prediction more straightforward. Instead of estimating subject-specific random effects of new observations, we can simply estimate coefficients/loadings on the basis function used for smoothing. Existing research on dynamic prediction with functional data analysis methods has been focusing on continuous/Gaussian outcomes, modelling subject-specific random effects with FPCA [@chiou2012; @goldberg2014; @shang2017]. @kraus2015 has used this approach to predict missing observations in partially observed function tracks, and @delaigo2016 achieved similar goals using Markov Chains. While methods mentioned above used only partial observations for prediction with an intercept-only model, @leroux2016 proposed Functional Concurrent Regression (FCR) framework which can incorporate the effect of subject-specific predictors. However, little extension was made on prediction of non-Gaussian functions, such as binary and count outcomes.

Unfortunately, fewer papers have focused on its extension to non-Gaussian data, such as series of binary or count outcomes. Existing methods also tend to be very computationally intensive. For example, @chen2013 proposed approaches to fit marginal functional models that is compatible to multi-level, generalized outcomes. @goldsmith2015 established a model framework that takes into account the fixed effect of time-invariant covariates, with parameters estimated with Bayesian method in *Stan*. @gertheiss2016 identified bias introduced by directly applying FPCA methods to generalized functions, and proposed to address this problem using a two-stage, joint estimation strategy. @linde2019 used an adapted Bayesian variational algorithm for FPCA of binary and count data. In terms of implementation, @wrobel2019 proposed a fast, efficient way to fit GFPCA on binary data using EM algorithm, accompanied by the an open source R package *registr*. 

In this paper, we aim to develop a fast, scalable method for dynamic prediction of discrete function tracks based on functional mixed effect model with fPCA smoothing. Section 2 presents the procedure of the proposed method. In Section 3, we illustrate the performance and efficiency of our proposed method in a simulation study. In Section 4, we apply this method to a real-world dataset. Section 5 presents a discussion of davantages and limitation of the proposed method. 



Method
=================================================================================================================================

The observed data for a single subject $i$ is ($t$, $Y_i(t)$), where $t$ consists of dense, discrete points along the functional domain, and $Y_i(t)$ is the non-Gaussian outcome observed at $t$. We assume that the outcome $Y_i(t)$ can be characterized by a latent continuous function $\eta_i(t)$. That is, at a specific t, $Y_i(t)$ follows a exponential family distribution such that:

$$
g[E(Y_i(t))] = \eta_i(t) = \beta_0(t)+b_i(t)
$$


where g is a appropriate link function, $\beta_0(t)$ is the population mean of latent function, and $b_i(t)$ is a subjects-level random effect function follows a zero-mean Gaussian process. 

While $b_i(t)$ is not observed, it can be approximated under the FPCA framework $b_i(t)=\sum_{k=1}^K\xi_{ik}\phi_{ik}(t)+\epsilon_i(t)$. Here $\phi_{ik}(t)$ is a set of orthogonal eigenfunctions that explains the most variation in $b_i(t)$, and $\xi_{ik}$ are subject-specific PC scores (or loadings) on each eigenfunction. Additionally, $\xi_{ik}$ are mutually independent obver both subject ($i$) and eigenfunctions ($k$). That is, each $\xi_{ik}$ follows normal distribution $N(0, \lambda_k)$ where $\lambda_k$ is the kth eigenvalues: $\int \phi_k^2(t)dt=\lambda_k$. $\epsilon_i(t)$ here is a residual function that accounts for the variation not explained by the first K eigenfunctions from FPCA. We assume it follows a zero-mean Gaussian process. At a specific point t, $\epsilon_i(t) \sim N(0, \sigma^2)$.

Based on the problem set up above, we propose the following algorithm for PFCA on the unobserved latent process $\eta_i(t)$:

1. Bin the observed outcomes in to small, non-overlapping, equal length intervals. We hereafter index the bins by their midpoints $s$. 
2. Fit a local Generalized Mixed Model at every bin. Specifically, at bin $s$, we fit $g[E(Y_i(t))] = \beta_0(s)+b_i(s)$ for all $t$ in bin $s$. From this series of models we can get estiamtes of population mean $\hat{\beta}_0(s)$ and subject-level random effect $\hat{b}_i(s)$, thus estimate of the individual latent functions at every bin: $\hat{\eta}_i(s) = \hat{\beta}_0(s)+\hat{b}_i(s)$. 
3. Fit FPCA on the estimated latent functions $\hat{\eta}_i(s)$, and obtain estimates of basis functions $\boldsymbol{\Phi} = \{\phi_1(s), ...,\phi_k(s)\}$, eigenvalues $\hat{\lambda}_1...\hat{\lambda}_k$, population mean $\hat{\beta}_0(s)$ and residual variance $\hat{\sigma}^2$. 
4. With components extracted above, calculate the maximum likelihood estimate (MLE) of the subject-specific PC scores $\hat{\xi}_{ik}$ of new samples based on their partially observed data. Then the value of latent functions at unobserved points can be estimated as $\hat{\eta_i}(s)=\hat{\beta}_0(s)+\sum_{k=1}^K\hat{\xi}_{ik}{\phi}_k(s)$

Following the algorithm above, predictions of individual latent functions are made on the binned grid based on partially observed non-Gaussian functions tracks of new subjects. Since the bins are set up to be small in length, the binned grid would still be dense enough. However, in situations where we need predictions on the original, un-binned grid instead, linear interpolation turns out to be a fast, convenient way with good performance for prediction at points between bins. 

Precision of prediction, usually quantified by the variance of prediction error $Var(\hat{\eta_i}(s)-\eta_i(s))$, is also straightforward under this framework. In step 4 we calculated the MLE of $\hat{\xi}_{ik}$. Based on likelihood theory, its variance can be estimated with observed information ${I}(\hat{\xi}_{ik})$, which is essentially the second derivative of likelihood at  $\hat{\xi}_{ik}$. Therefore, the variation of prediction interval is:

$$Var(\hat{\eta_i}(s)-\eta_i(s))=\boldsymbol{\Phi}(s)I(\hat{\boldsymbol{\xi}}_i)\boldsymbol{\Phi}^T(s)+\hat{\sigma_{\epsilon}}^2$$
Where $\boldsymbol{\Phi}(s)=(\phi_1(s)...\phi_K(s))$ and $\hat{\boldsymbol{\xi}}_i=(\hat{\xi}_{i1}, ...,\hat{\xi}_{iK})$.


<!-- -	Need better notation system -->
<!-- - Change to general exponential family -->
<!-- - Add section of de-bias score: conditional on subject. Need repeat simulation to demonstrate -->

Simulation
=================================================================================================================================

In this section, we illustrate the predictive performance and computational efficiency of the proposed method through a simulation study. We simulated 50 datasets, each with 500 subjects. For every subject, we generate 1000 binary outcomes $Y_i(t) \in (0, 1)$ across functional domain $t \in [0, 1]$, where the distribution of outcome is characterized by a continuous latent function. The data generation mechanism can be expressed as follows: 

$$\begin{aligned}
Y_i(t) & \sim Binomial(\frac{exp(\eta_i(t))}{1+exp(\eta_i(t))}) \\
\eta_i(t) &= f_0(t)+ \xi_{i1}sin(2\pi t)+\xi_{i2}cos(2\pi t)+\xi_{i3}sin(4\pi t)+\xi_{i4}cos(4\pi t)
\end{aligned}$$

In this simulation, we set $f_0(t) = 0$. $\xi_{ik}$ are mutually independent normal random variables $\xi_{ik}\sim N(0, \gamma_k)$. Here we set the values of $\gamma_k$ to be $0.5^{k-1}$, $k \in (1,..., 4)$. In addition, for simplicity of presentation, we generate data on a regular grid, which means observations points are equally distributed across $[0, 1]$ and are the same for all subjects. 

We use two metrics to evaluate the out-of-sample predictive performance: integrated squared error (ISE) and Area-Under-the-Receiver-Operator-Curve (AUC). ISE assess the prediction accucay of latent continuous function. It is evaluated on the binned grid at midpoints of each unobserved bin. If the entire functional domian has S bins, but we have observations up to the mth bin, then ISE is defined as $\frac{1}{N}\sum_{i=1}^N\sum_{s=m+1}^S(\hat{\eta}_i(s)-\eta_i(s))^2$.

The second metric, AUC, focuses on evaluation of prediction of the binary outcome. Since the binary outcomes are generated on the original, un-binned grid, we evaluated AUC on this grid as well and estimated values of latent functions between bins with linear interpolation. Just like ISE, the AUC report is also the average value across the whole sample. 

As a reference method, we compare our method to Generalized Linear Mixed Models using Adaptive Gaussian Quadrature (GLMMadaptive). This is one of the fastest existing method developed for dynamic prediction of repeated generalized outcomes. Just like many mixed models, this method is very limited in terms of flexibility. For example, the model used for prediction of our simulated datset would simply be an linear model with one covariate indicating observation time: $g(E(Y_i)) = \beta_0+\beta_1t+b_{i0}+b_{i1}t$. While the flexibility of this mixed model can be increased using spline functions, the dimension of spline basis is also restricted by computational ability, and is unfeasible to implement under the scale of our simulated data or the complexity of our proposed method.


## Result

```{r}
load(here("Data/sim_data.RData"))
load(here("Data/SimOutput_fGFPCA.RData"))

# other quantities
mid <- unique(pred_lst[[1]]$bin)
N <- 500
M <- length(pred_lst)
J <- 1000

# code prediction value at observed points as NA
for(m in 1:length(pred_lst)){
  pred_lst[[m]]$pred_t200[pred_lst[[m]]$bin <= 200] <- NA
  pred_lst[[m]]$pred_t400[pred_lst[[m]]$bin <= 400] <- NA
  pred_lst[[m]]$pred_t600[pred_lst[[m]]$bin <= 600] <- NA
  pred_lst[[m]]$pred_t800[pred_lst[[m]]$bin <= 800] <- NA
}
  

```


```{r}
# true data
df_true <- sim_data[[1]] %>% select(id, sind_inx, eta_i)

# prediction results used for visualization
exp_df <- pred_lst[[1]]

# randomly draw four samples for visualization
rand_id <- sample(exp_df$id, 4)

exp_df %>%
  left_join(df_true %>% filter(sind_inx %in% mid) %>% rename(bin=sind_inx), by = c("id", "bin")) %>%
  filter(id %in% rand_id) %>%
  ggplot()+
  geom_line(aes(x=bin, y=eta_i))+
  geom_line(aes(x=bin, y = pred_t200, col = "200"), linetype = "dashed", na.rm = T)+
  geom_line(aes(x=bin, y = pred_t400, col = "400"), linetype = "dashed", na.rm = T)+
  geom_line(aes(x=bin, y = pred_t600, col = "600"), linetype = "dashed", na.rm = T)+
  geom_line(aes(x=bin, y = pred_t800, col = "800"), linetype = "dashed", na.rm = T)+
  facet_wrap(~id, scales = "free_y")


```

```{r}
runtime
# calculate ISE on binned grid
ise_mat <- matrix(NA, nrow = M, ncol = 4) # row index iteration, col index max observation time
colnames(ise_mat) <- c("t200", "t400", "t600", "t800")

for(m in 1:M){
  # true value at midpoints
  true_eta <- sim_data[[m]] %>% filter(sind_inx %in% mid) %>% select(id, sind_inx, eta_i) %>% rename(bin=sind_inx)
  pred_eta <- pred_lst[[m]] %>% select(-eta_hat)
  pred_eta <- pred_eta %>% left_join(true_eta, by = c("id", "bin"))
  
  ise_m <- c(
    t200 = sum((pred_eta$pred_t200-pred_eta$eta_i)^2, na.rm = T)/N,
    t400 = sum((pred_eta$pred_t400-pred_eta$eta_i)^2, na.rm = T)/N,
    t600 = sum((pred_eta$pred_t600-pred_eta$eta_i)^2, na.rm = T)/N,
    t800 = sum((pred_eta$pred_t800-pred_eta$eta_i)^2, na.rm = T)/N
  )
  
  ise_mat[m, ] <- ise_m
}
```
```{r}
# calculate AUC on unbinned grid
auc_mat <- matrix(NA, nrow = M, ncol = 4) # row index iteration, col index max observation time
colnames(auc_mat) <- c("t200", "t400", "t600", "t800")

for(m in 1:M){
  # true value as matrix
  Y_mat <- sim_data[[m]]  %>% select(id, sind_inx, Y) %>% rename(bin=sind_inx) %>% 
    pivot_wider(names_from = id, values_from = Y) %>%
    column_to_rownames("bin")
  
  # calculate AUC
  t_vec <- c(200, 400, 600, 800)
  for(i in seq_along(t_vec)){
    
    t <- t_vec[i]
    # predicted value
    pred_name <- paste("pred_t", t, sep = "")
    pred_mat <- pred_lst[[m]] %>% select(id, bin, all_of(pred_name)) %>% 
      pivot_wider(names_from = id, values_from = pred_name) %>%
      column_to_rownames("bin")
    pred_full <- sapply(1:N,
                           function(x){approx(x=mid, y=pred_mat[ , x], xout = 1:J)$y})
    pred_full <- exp(pred_full)/(1+exp(pred_full))
    row_id <- complete.cases(pred_full)
    auc_perf <- performance(prediction(pred_full[row_id, ], Y_mat[row_id, ]), "auc")
    auc_mat[m, i] <- mean(unlist(auc_perf@y.values))
  }
  
}

auc_mat
```



<!-- -	Repeat simulation: repeat 10-100 times -->

<!-- -	Different set-up: -->
<!-- a.	Different eigenfunctions: with or without periodicity (start with the current one) -->
<!-- b.	Outcome: binary or count -->
<!-- c.	Sample size: start with N=500 -->
<!-- d.	Grid density: start with J=1000 -->
<!-- e.	Bin width, overlap or not: start with non-overlap, bin width = 10 (100 bins) -->

Data application
=================================================================================================================================

Discussion
=================================================================================================================================
-	Grid 
-	Score bias: cannot demonstrate without repeat simulation

References
=================================================================================================================================
<div id="refs"></div>