---
title: "Dynamic Prediction of Non-Guassian Outcome with fast Generalized Functional Principal Analysis"
author:
  - Ying Jin
  - Andrew Leroux
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    latex_engine: pdflatex
fontfamily: mathpazo
fontsize: 11pt
geometry: margin = 1in
bibliography: refs.bib
nocite: '@*'
link-citations: yes
linkcolor: blue
csl: ASA.cls
header-includes:
  - \usepackage{setspace}\doublespacing
  - \usepackage[utf8]{inputenc}
  - \usepackage[T1]{fontenc}
---
<!-- Abstract:  -->
<!-- ================================================================================================================================= -->
<!--   Biomedical investigators are often interested in predicting future observations of subjects based on their historical data, referred to as dynamic prediction. Traditional methods are often limited in flexibility and computationally intensive, especially with non-Gaussian data. To address these issues, we propose a novel method for dynamic prediction based on Generalized Functional Principal Component Analysis (FPCA). Assume the observed outcome follows an exponential family distribution parameterized by a latent Gaussian function, the proposed method consists of the following steps: 1) Bin the data across functional domain into small, equal-length intervals; 2) Fit local generalized mixed models at every bin to estimate individual latent functions; 3) Fit FPCA model to smooth latent functions and 4) Obtain estimates of subject-specific PC scores using partial observations and recover the unobserved part on the binned grid. Our simulation study showed the proposed method achieved significantly better out-of-sample predictive performance compared to existing methods with much shorter computation time, thus has the potential to be widely applicable to large datasets. -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
theme_set(theme_minimal())
library(here)
library(ROCR)
library(kableExtra)
library(knitr)
library(ggpubr)
options(knitr.kable.NA = "")

set.seed(314)

# simulation
load(here("Data/sim_data.RData"))
load(here("Data/SimOutput_fGFPCA.RData"))
load(here("Data/SimOutput_GLMMadaptive.RData"))

# data application
df_appl <- read.csv(here("Data/Daily_Weigh_ins.csv"))
load(here("Data/ApplOutput_fGFPCA.RData"))
load(here("Data/ApplOutput_GLMMadaptive.RData"))

```


Introduction
=================================================================================================================================

Biomedical investigators are often interested in predicting future observations of subjects based on their historical data, typically referred to as dynamic prediction. Traditionally, this type of data has been modeled either marginally, such as generalized estimating equations, or constitutionally on specific subjects, such as mixed effect models [@liang1986; @Laird1982; @lindstrom1990; @davidian2003]. For subject-specific models, predictions are made based on the correlation between repeated measures from the same subject and covariates that can be either fixed or varying. However, most of these methods are based on linear model structure, which is very limited in terms of flexibility of correlation. When sample size is large or the density of repeated measures is high, they also tend to cause severe computational burden [@GLMMadaptive]. In addition, with generalized, non-Gaussian outcomes, these methods lack the ability to handle out-of-sample prediction because there is usually no close-form solution for subject-specific random effects

To address these problems, one may turn to functional mixed effect models instead when measures are dense across the domain of measures. Such methods accommodate more flexible correlation structure by modeling subject-specific random effects as a function. Non-parametric smoothing [@Scheipl2014] are often incorporated to speed up the computation, where random effects are approximated by spline basis functions or eigenfunctions from functional principal component analysis (FPCA). The introduction of basis functions also makes out-of-sample prediction more straightforward. Instead of estimating subject-specific random effects of new observations, we can simply estimate coefficients/loadings on the basis function used for smoothing. Existing research on dynamic prediction with functional data analysis methods has been focusing on continuous/Gaussian outcomes, usually modelling subject-specific random effects with FPCA [@chiou2012; @goldberg2014; @shang2017]. @kraus2015 has used this approach to predict missing observations in partially observed function tracks, and @delaigo2016 achieved similar goals using Markov Chains. While methods mentioned above used only partial observations for prediction with an intercept-only model, @leroux2016 proposed Functional Concurrent Regression (FCR) framework which can incorporate the effect of subject-specific predictors. 

Unfortunately, fewer papers have focused on its extension to generalized, non-Gaussian outcomes, such as series of binary or count outcomes. Existing methods also tend to be very computationally intensive. For example, @chen2013 proposed approaches to fit marginal functional models that is compatible to multi-level, generalized outcomes. @goldsmith2015 established a model framework that takes into account the fixed effect of time-invariant covariates, with parameters estimated with Bayesian method in *Stan*. @gertheiss2016 identified bias introduced by directly applying FPCA methods to generalized functions, and proposed to address this problem using a two-stage joint estimation strategy. @linde2019 used an adapted Bayesian variational algorithm for FPCA of binary and count data. In terms of implementation, @wrobel2019 proposed a fast, efficient way to fit GFPCA on binary data using EM algorithm, accompanied by the an open source R package *registr*. 

In this paper, we aim to develop a fast, scalable method for dynamic prediction of generalized functional outcome based on functional mixed effect model with FPCA smoothing. Specifically, "Generalized" indicates that the outcome follows non-Gaussian exponential family distribution, and the term "functional" indicates that repeated measure are collected densely across the study domain (e.g., time). Section 2 explains the implementation procedure of the proposed method. In Section 3, we illustrate the performance and efficiency of our proposed method in a simulation study. In Section 4, we apply this method to a real world dataset from a weight loss study. Section 5 presents a discussion of advantages and limitation of the proposed method. 


Method
=================================================================================================================================

The observed data for a single subject $i$ is ($t$, $Y_i(t)$), where $t$ consists of dense, discrete points along the domain of data collection. In practice, $t$ is usually a time index along the duration of study. We hereafter refer to the range of $t$ as the "functional domain". For simplicity, we further assume that outcomes are collected on a regular grid over the functional domain, meaning that $t$ are evenly distributed and does not change across subjects. 

$Y_i(t)$ is the non-Gaussian outcome observed at $t$. We assume that the outcome $Y_i(t)$ can be characterized by a latent continuous function $\eta_i(t)$. That is, at a specific t, $Y_i(t)$ follows a exponential family distribution such that:

$$
g[E(Y_i(t))] = \eta_i(t) = \beta_0(t)+b_i(t)
$$


where g is an appropriate link function, $\beta_0(t)$ is the population mean of latent function, and $b_i(t)$ is a subjects-level random effect function following a zero-mean Gaussian process. 

According to the Karhunen-LoÃ¨ve theorem, the unobserved random effect function $b_i(t)$ can be represented as the linear combination of a infinite set of orthogonal functions: $b_i(t)=\sum_{k=1}^{\infty}\xi_{ik}\phi_{k}(t)$. We approximate this function by a finite set of $\phi_{k}(t)$, such that $b_i(t)=\sum_{k=1}^{K}\xi_{ik}\phi_{k}(t)$.

In practice, this approximation could be implemented through an FPCA procedure on $\eta_i(t)$. In this case $\phi_{k}(t),\ k \in \{1,...,K\}$ are the first K eigenfunctions that explain the most variation in $\eta_i(t)$. These eigenfunctions, also referred to as Principal Component (PC) functions, are orthogonal are can be orthonormal with proper scaling. $\xi_{ik}$ are subject-specific scores (or loadings) on each PC function. According to properties of FPCA method, these scores are mutually independent over both subject ($i$) and eigenfunctions ($k$). That is, each $\xi_{ik}$ follows normal distribution $N(0, \lambda_k)$, where $\lambda_k$ is the kth eigenvalues.


**Fast Generalized FPCA** Based on the problem set up above, we propose the following algorithm for fast, generalizable implementation of FPCA (fGFPCA) on the unobserved latent process $\eta_i(t)$:

1. Bin the observed outcomes in to small, non-overlapping, equal length intervals. We hereafter index the bins by their midpoints $s$. The number of observations from the same subject in bin $s$ is $n_s$, and $t_{sj}$ refers to the jth observation point in bin $s$.

2. Fit a local Generalized Mixed Model at every bin. Specifically, at bin $s$, we fit $g[E(Y_i(t_{sj}))] = \beta_0(s)+b_i(s)$,  $j\in\{1, ..., n_s\}$. From this series of models we can estimate the individual latent functions at every bin: $\hat{\eta}_i(s) = \hat{\beta}_0(s)+\hat{b}_i(s)$. 

3. Fit FPCA on the estimated latent functions: $\hat{\eta}_i(s) = f_0(s)+\sum_{k=1}^K\xi_{ik}\phi_{k}(s)+\epsilon_i(s)$, where $\epsilon_i(t)$ indicates random noise introduced by the binning process in previous steps. This random noise is assumed to follow a zero-mean Gaussian process, such that at each specific point t, $\epsilon_i(t) \sim N(0, \sigma^2)$. The FPCA output includes estimates of basis functions $\boldsymbol{\Phi} = \{\phi_1(s), ...,\phi_k(s)\}$, eigenvalues $\hat{\lambda}_1...\hat{\lambda}_k$, mean function $\hat{f}_0(s)$ and residual variance $\hat{\sigma}^2$. 


**Dynamic prediction** Now assume we have a new observations with partially observed outcome $Y_{new}(t)$, $t\leq t_m$. The maximum observed point is $t_m$, which is a point along the functional domain and belongs to bin $s_m$ on the binned grid from step 1 above. With components extracted from the FPCA model in step 3, we will be able to make predictions on the future, unobserved bins. Specifically, with the partially observed binary function $Y_{new}(t)$, we will be able to calculate the maximum likelihood estimate (MLE) of the subject-specific PC scores $\hat{\xi}_{ik}$. Then the value of latent functions at unobserved points can be estimated as $\hat{\eta_i}(s)=\hat{f}_0(s)+\sum_{k=1}^K\hat{\xi}_{ik}{\phi}_k(s)$

Following the algorithm above, predictions of individual latent functions are made on the binned grid based on partially observed tracks on the un-binned grid. Since the bins are set up to be small in length, the binned grid would still be dense. However, in situations where predictions on the original, un-binned grid is needed, linear interpolation of estimated latent function tracks turns out to be a fast, convenient way with good performance for prediction at points between bins. 


Simulation
=================================================================================================================================

In this section, we illustrate the predictive performance and computational efficiency of the proposed method through a simulation study. We simulated 50 datasets, each with 500 subjects. For every subject, we generate 1000 binary outcomes $Y_i(t) \in \{0, 1\}$ across functional domain $t \in (0, 1000]$, where the distribution of outcome is characterized by a continuous latent function. The data generation mechanism can be expressed as follows: 

$$\begin{aligned}
Y_i(t) & \sim Binomial(\frac{exp(\eta_i(t))}{1+exp(\eta_i(t))}) \\
\eta_i(t) &= f_0(t)+ \xi_{i1}\sqrt{2}sin(2\pi t)+\xi_{i2}\sqrt{2}cos(2\pi t)+\xi_{i3}\sqrt{2}sin(4\pi t)+\xi_{i4}\sqrt{2}cos(4\pi t)
\end{aligned}$$

In this simulation, we set $f_0(t) = 0$. $\xi_{ik}$ are mutually independent normal random variables $\xi_{ik}\sim N(0, \lambda_k)$. Here we set the values of $\lambda_k$ to be $0.5^{k-1}$, $k \in (1,..., 4)$. In addition, for simplicity of presentation, we generate data on a regular grid, which means observations points are equally distributed across $(0, 1000]$ and are the same for all subjects. 

We use two metrics to evaluate the out-of-sample predictive performance: integrated squared error (ISE) and Area-Under-the-Receiver-Operator-Curve (AUC). ISE assesses the prediction accuracy of latent continuous functions. It is evaluated on the binned grid at midpoints of each unobserved bin. With observations up to bin $s_m$, ISE is defined as $\frac{1}{N}\sum_{i=1}^N\sum_{s>s_m} (\hat{\eta}_i(s)-\eta_i(s))^2$. The second metric, AUC, focuses on evaluation of prediction of the binary outcome. Since the binary outcomes are generated on the original, un-binned grid, we evaluated AUC on this grid by estimating values of latent functions between bins with linear interpolation. 

As a reference method, we compare our method to Generalized Linear Mixed Models using Adaptive Gaussian Quadrature (GLMMadaptive). This is one of the fastest existing method developed for dynamic prediction of repeated generalized outcomes. Just like many mixed models, this method is very limited in terms of flexibility. For example, the model used for prediction of our simulated datset would simply be an linear model with one covariate indicating observation time: $g(E(Y_i)) = \beta_0+\beta_1t+b_{i0}+b_{i1}t$. While the flexibility of this mixed model can be increased using spline functions, the dimension of spline basis is also restricted by computational ability, and is unfeasible to implement under the scale of our simulated data or the complexity of our proposed method.

The average ISE and AUC across all simulation is presented in Table 1, and Figure 1 presents the predicted latent function of four randomly selected subjects from the first simulated dataset. The prediction is made conditioning on different length of observed track (specifically, with observations up to t = 200, 400, 600, 800 respectively), and evaluation is made on equal-length time windows on the unobserved tracks following the maximum observation time. As Table 1 reveals, the proposed method (fGFPCA) outperforms GLMMadaptive under every scenario, which was expected since the linear model structure of GLMM is too simple for the cyclic data generation mechanism. In addition, fGFPCA also took less computation time. For one simulated dataset, fGFPCA spent `r round(mean(runtime),2)` minutes on model fitting and out-of-sample prediction overall, while GLMMadaptive took `r round(mean(runtime_ref),2)` minutes. 

Because of the flexibility of our proposed model framework, the accuracy of prediction at specific time points would improve as more observed data is collected. This is revealed in Table 1 as ISE decreases and AUC increases with maximum observed time (left-to-right), also in Figure 1 as predicted curves get closer to the true latent function with longer observed track. However the same tendency is not observed with GLMMadaptive models, as a result of restricted model flexibility. A linear model that fits well to a specific part can fit very badly to other parts along the underlying latent function tracks, especially when the shapes of functions are wiggly or cyclic. Therefore, more observations do not necessarily make the model more predictive. 


```{r}
# other quantities
mid <- unique(pred_lst[[1]]$bin)
N <- 500
M <- length(pred_lst)
J <- 1000

# for fGFPCA
# code prediction value at observed points as NA
for(m in 1:length(pred_lst)){
  pred_lst[[m]]$pred_t200[pred_lst[[m]]$bin <= 200] <- NA
  pred_lst[[m]]$pred_t400[pred_lst[[m]]$bin <= 400] <- NA
  pred_lst[[m]]$pred_t600[pred_lst[[m]]$bin <= 600] <- NA
  pred_lst[[m]]$pred_t800[pred_lst[[m]]$bin <= 800] <- NA
}

# for GLMM adaptive
# add in observation time index
for(m in 1:length(pred_lst_ref)){
  pred_lst_ref[[m]]$sind_inx <- rep(1:J, N)
}
```


```{r Figure, fig.cap="Predicted track of four randomly selected subjects from one simulated dataset. The grey solid line indicates the true latent continuous function. The dashed lines indicated predicted latent function tracks, and color indicates different observation time.", fig.height=6, fig.width=12}
# figure of fGFPCA prediction
cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

# true data
df_true_sim1 <- sim_data[[1]] %>% select(id, sind_inx, eta_i)

# prediction results used for visualization
exp_df_sim1 <- pred_lst[[1]]

exp_df_sim1_ref <- pred_lst_ref[[1]]

# randomly draw four samples for visualization
rand_id <- sample(exp_df_sim1$id, 4)

# fGFPCA
fig1a <- exp_df_sim1 %>%
  left_join(df_true_sim1 %>% filter(sind_inx %in% mid) %>% rename(bin=sind_inx), by = c("id", "bin")) %>%
  filter(id %in% rand_id) %>%
  ggplot()+
  geom_line(aes(x=bin, y=eta_i, col = "True"))+
  geom_line(aes(x=bin, y = pred_t200, col = "Up to 200"), linetype = "dashed", na.rm = T)+
  geom_line(aes(x=bin, y = pred_t400, col = "Up to 400"), linetype = "dashed", na.rm = T)+
  geom_line(aes(x=bin, y = pred_t600, col = "Up to 600"), linetype = "dashed", na.rm = T)+
  geom_line(aes(x=bin, y = pred_t800, col= "Up to 800"), linetype = "dashed", na.rm = T)+
  facet_wrap(~id, scales = "free_y")+
  labs(y = "Latent function", x = "Bin", colour= "Observed time", title = "fGFPCA")+
  scale_color_manual(values=cbPalette)+
  scale_x_continuous(breaks = c(0, 200, 400, 600, 800, 1000))

# GLMMadaptive
fig1b <- exp_df_sim1_ref %>%
  filter(sind_inx %in% mid) %>%
  filter(id %in% rand_id) %>%
  ggplot()+
  geom_line(aes(x=sind_inx, y=eta_i, col = "True"))+
  geom_line(aes(x=sind_inx, y = pred_t200, col = "Up to 200"), linetype = "dashed", na.rm = T)+
  geom_line(aes(x=sind_inx, y = pred_t400, col = "Up to 400"), linetype = "dashed", na.rm = T)+
  geom_line(aes(x=sind_inx, y = pred_t600, col = "Up to 600"), linetype = "dashed", na.rm = T)+
  geom_line(aes(x=sind_inx, y = pred_t800, col= "Up to 800"), linetype = "dashed", na.rm = T)+
  facet_wrap(~id, scales = "free_y")+
  labs(y = "Latent function", x = "Bin", colour= "Observed time", title = "GLMMadaptive")+
  scale_color_manual(values=cbPalette)+
  scale_x_continuous(breaks = c(0, 200, 400, 600, 800, 1000))

ggarrange(fig1a, fig1b, nrow = 1, common.legend = T)


```


```{r ISE_fGFPCA}
# calculate ISE on binned grid
ise_mat <- array(NA, dim = c(5, 4, M)) # dimensions are: predicted time, max obs time, simulation

for(m in 1:M){
  
  # true value at midpoints
  true_eta <- sim_data[[m]] %>% filter(sind_inx %in% mid) %>% select(id, sind_inx, eta_i) %>% rename(bin=sind_inx)
  pred_eta <- pred_lst[[m]] %>% select(-eta_hat)
  pred_eta <- pred_eta %>% left_join(true_eta, by = c("id", "bin"))
  
  # split unobserved time into intervals of length 200
  # [0, 200], (200, 400], (400, 600], (600, 800], (800, 1000]
  pred_eta$bin_cat <- cut(pred_eta$bin, breaks=c(0, 200, 400, 600, 800, 1000), include.lowest = T)
  
  # SE at each observed time
  pred_eta <- pred_eta %>% mutate(ise_t200 = (pred_t200-eta_i)^2,
                                  ise_t400 = (pred_t400-eta_i)^2,
                                  ise_t600 = (pred_t600-eta_i)^2,
                                  ise_t800 = (pred_t800-eta_i)^2)
  
  # subject-average SE
  ise_mat[, , m] <- pred_eta %>% group_by(bin_cat) %>% 
    summarize_at(vars(starts_with("ise")), .fun=function(x)(sum(x)/N)) %>%select(-bin_cat) %>% as.matrix()
}

ave_ise_mat <- apply(ise_mat, MARGIN = c(1, 2), mean)[-1, ]
colnames(ave_ise_mat) <- c("200", "400", "600", "800")
rownames(ave_ise_mat) <- c("(200, 400]", "(400, 600]", "(600, 800]", "(800, 1000]")


```

```{r, ISE_GLMMadaptive}
# calculate ISE on binned grid
ise_mat_ref <- array(NA, dim = c(5, 4, M)) # dimensions are: predicted time, max obs time, simulation

for(m in 1:M){
  
  # true value at midpoints
  pred_eta <- pred_lst_ref[[m]] %>% filter(sind_inx %in% mid)
  
  # split unobserved time into intervals of length 200
  # [0, 200], (200, 400], (400, 600], (600, 800], (800, 1000]
  pred_eta$bin_cat <- cut(pred_eta$sind_inx, breaks=c(0, 200, 400, 600, 800, 1000), include.lowest = T)
  
  # SE at each observed time
  pred_eta <- pred_eta %>% mutate(ise_t200 = (pred_t200-eta_i)^2,
                                  ise_t400 = (pred_t400-eta_i)^2,
                                  ise_t600 = (pred_t600-eta_i)^2,
                                  ise_t800 = (pred_t800-eta_i)^2)
  
  # subject-average SE
  ise_mat_ref[, , m] <- pred_eta %>% group_by(bin_cat) %>% 
    summarize_at(vars(starts_with("ise")), .fun=function(x)(sum(x)/N)) %>%select(-bin_cat) %>% as.matrix()
  
}

ave_ise_mat_ref <- apply(ise_mat_ref, MARGIN = c(1, 2), mean)[-1, ]
colnames(ave_ise_mat_ref) <- c("200", "400", "600", "800")
rownames(ave_ise_mat_ref) <- c("(200, 400]", "(400, 600]", "(600, 800]", "(800, 1000]")

# cbind(ave_ise_mat, ave_ise_mat_ref)
```


```{r AUC_fGFPCA}
# maximum observations time
t_vec <- c(200, 400, 600, 800)

# calculate AUC on unbinned grid
auc_mat <- array(NA, dim = c(4, 4, M)) # row index iteration, col index max observation time

for(m in 1:M){
  
  Y_df <- sim_data[[m]]
  eta_pred <- pred_lst[[m]]
 
  # use interpolation to extend prediction to the unbinned grid
  for(i in 1:N){
   Y_df$pred_200[Y_df$id==i] <- approx(x=eta_pred$bin[eta_pred$id==i], y=eta_pred$pred_t200[eta_pred$id==i], xout = 1:J)$y
   Y_df$pred_400[Y_df$id==i] <- approx(x=eta_pred$bin[eta_pred$id==i], y=eta_pred$pred_t400[eta_pred$id==i], xout = 1:J)$y
   Y_df$pred_600[Y_df$id==i] <- approx(x=eta_pred$bin[eta_pred$id==i], y=eta_pred$pred_t600[eta_pred$id==i], xout = 1:J)$y
   Y_df$pred_800[Y_df$id==i] <- approx(x=eta_pred$bin[eta_pred$id==i], y=eta_pred$pred_t800[eta_pred$id==i], xout = 1:J)$y
  }
  
  # split unobserved time into intervals of length 200
  # [0, 200], (200, 400], (400, 600], (600, 800], (800, 1000]
  Y_df$bin_cat <- cut(Y_df$sind_inx, breaks=c(0, 200, 400, 600, 800, 1000), include.lowest = T)
  Y_df_lst <- split(Y_df, f = Y_df$bin_cat)
  Y_df_lst <- Y_df_lst[-1]
  
  for(j in seq_along(t_vec)){
    
    t <- t_vec[j]
    pred_name <- paste("pred_", t, sep = "")
    
    # calculate AUC
    auc_lst <- lapply(Y_df_lst[j:4], 
                       function(x){
                         row_id <- !is.na(x[, pred_name])
                         this_eta <- x[row_id, pred_name]
                         this_Y <- x[row_id, "Y"]
                         auc_pref <- performance(prediction(this_eta, this_Y), "auc")
                         return(auc_pref@y.values)
                })
      
    auc_vec <- c(rep(NA, 4-length(auc_lst)), unlist(auc_lst))
    auc_mat[ , j, m] <- auc_vec

}}


ave_auc_mat <- apply(auc_mat, MARGIN = c(1, 2), mean)
colnames(ave_auc_mat) <- c("200", "400", "600", "800")
rownames(ave_auc_mat) <- c("(200, 400]", "(400, 600]", "(600, 800]", "(800, 1000]")

```

```{r AUC_GLMMadaptive}
# calculate AUC on unbinned grid
auc_mat_ref <- array(NA, dim = c(4, 4, M)) # row index iteration, col index max observation time

for(m in 1:M){
  
  Y_df <- sim_data[[m]]
  eta_pred <- pred_lst_ref[[m]]
  
  Y_df <- Y_df %>% left_join(eta_pred %>% select(id, sind_inx, starts_with("pred_")), by = c("id", "sind_inx"))
 
  # split unobserved time into intervals of length 200
  # [0, 200], (200, 400], (400, 600], (600, 800], (800, 1000]
  Y_df$bin_cat <- cut(Y_df$sind_inx, breaks=c(0, 200, 400, 600, 800, 1000), include.lowest = T)
  Y_df_lst <- split(Y_df, f = Y_df$bin_cat)
  Y_df_lst <- Y_df_lst[-1]
  
  for(j in seq_along(t_vec)){
    
    t <- t_vec[j]
    pred_name <- paste("pred_t", t, sep = "")
    
    # calculate AUC
    auc_lst <- lapply(Y_df_lst[j:4], 
                       function(x){
                         row_id <- !is.na(x[, pred_name])
                         this_eta <- x[row_id, pred_name]
                         this_Y <- x[row_id, "Y"]
                         auc_pref <- performance(prediction(this_eta, this_Y), "auc")
                         return(auc_pref@y.values)
                })
      
    auc_vec <- c(rep(NA, 4-length(auc_lst)), unlist(auc_lst))
    auc_mat_ref[ , j, m] <- auc_vec

  }
}



ave_auc_mat_ref <- apply(auc_mat_ref, MARGIN = c(1, 2), mean)
colnames(ave_auc_mat_ref) <- c("200", "400", "600", "800")
rownames(ave_auc_mat_ref) <- c("(200, 400]", "(400, 600]", "(600, 800]", "(800, 1000]")


```

```{r format table}
ise_tb <- cbind(ave_ise_mat, ave_ise_mat_ref)
auc_tb <- cbind(ave_auc_mat, ave_auc_mat_ref)


rbind(ise_tb, auc_tb) %>%
  kable(booktabs = T, digit=2,
        caption = "Predictive performance of fGFPCA and GLMM adaptive on the simulated datasets. ISE and AUC are average values across all 50 simulations.",
        align = "lcccccccc") %>%
  kable_styling() %>%
  add_header_above(c(" " = 1, "fGFPCA" = 4, "GLMMadaptive" = 4)) %>%
  add_header_above(c(" " = 1, "Maximum observed time" = 8)) %>% 
  group_rows(index = c("Prediction time window" = 8)) %>%
  group_rows(index = c("ISE" = 4, "AUC" = 4)) 
```



<!-- -	Repeat simulation: repeat 10-100 times -->

<!-- -	Different set-up: -->
<!-- a.	Different eigenfunctions: with or without periodicity (start with the current one) -->
<!-- b.	Outcome: binary or count -->
<!-- c.	Sample size: start with N=500 -->
<!-- d.	Grid density: start with J=1000 -->
<!-- e.	Bin width, overlap or not: start with non-overlap, bin width = 10 (100 bins) -->

Data application
=================================================================================================================================

In this section, we illustrate the performance of our proposed method using data from a randomized trial on weight loss at University of Colorado Anschutz Medical Campus [@bothwell2022; @ostendorf2022]. The study originally aimed to study the effect of dietary strategies on weight loss, and participants were given a smart scale for daily at-home weighing. This record was then scaled into a binary indicator of participant adherence to the daily-weigh-in procedure, where 1 indicates participants followed the requirement and 0 otherwise. In this section, we will use this series of daily indicators as the binary functional outcome, and use the proposed method to dynamically predict participant adherence on future days given historical track. 

The dataset includes 55 participants aged 22-56 at baseline. Adherence to daily weigh-in is available along 400 consecutive days for all participants. In other words, for each individual, 400 binary measurements were taken on a regular grid along the functional domain (number of days into the study). We will fit both fGFPCA and GLMMadaptive models and compared their performance on this data. For fGFPCA, every 10 observations are binned together, resulting in 40 bins for final model fitting. Linear interpolation was used for extension from the binned grid back to the un-binned grid. Since the latent continuous function tracks are not known in this case, prediction performance on evaluated with AUC alone. 

Table 2 compares the predictive performance of both models. Similar to the simulatio study, prediction is made conditioning on observations up to the 100th, 200th and 300th day respectively, and evaluation is made every 100 days following the maximum observation time. As is revealed, when the observed time is short (up to 100th day), the two method performed similarly when predicting recent days, however fGFPCA did better in predicting further days along the trial (prediction time window of $(300, 400]$). As more observations coming in, fGFPCA sees increasingly higher AUC compared to GLMMadaptive model. This could be a result of the difference in model flexibility of the two methods. While the linear relationship in underlying latent functions are not likely to change immediately following the observed track, the whole track could be much more complex. Therefore, while fGFPCA performs similarly to GLMMadaptive right after the end of observation, it does a much better job capturing the underlying pattern along the whole track. Figure 2 further illustrate such difference using predicted latent function tracks of four randomly drawn subjects.


```{r}
# data clean
mid2 <- unique(df_est_latent$bin)
N2 <- max(df_est_latent$id)
J2 <- 400

# for fGFPCA
# code prediction value at observed points as NA
df_est_latent$pred_t100[df_est_latent$bin <= 100] <- NA
df_est_latent$pred_t200[df_est_latent$bin <= 200] <- NA
df_est_latent$pred_t300[df_est_latent$bin <= 300] <- NA


# for GLMM adaptive
# add in observation time index
for(m in 1:length(pred_lst_ref)){
  pred_lst_ref[[m]]$sind_inx <- rep(1:J, N)
}
```

```{r AUC_fGFPCA_Appl}
# maximum observations time
t_vec2 <- c(100, 200, 300)

# calculate AUC on unbinned grid
auc_mat2 <- matrix(NA, nrow = 3, ncol = 3) # row index iteration, col index max observation time

# result container for interpolation
Y_df2 <- df_appl %>% select(participant_id, starts_with("day")) %>% 
  pivot_longer(2:401, values_to = "Y", names_to = "sind_inx") %>%
  mutate(sind_inx = gsub("day_", "", sind_inx)) %>%
  mutate(sind_inx = as.numeric(sind_inx)) %>%
  rename("id" = "participant_id")
Y_df2$pred_100 <- Y_df2$pred_200 <- Y_df2$pred_300 <- NA

  # use interpolation to extend prediction to the unbinned grid
for(i in 1:N2){
   Y_df2$pred_100[Y_df2$id==i] <- approx(x=df_est_latent$bin[df_est_latent$id==i], 
                                         y=df_est_latent$pred_t100[df_est_latent$id==i], 
                                         xout = 1:J2)$y
   Y_df2$pred_200[Y_df2$id==i] <- approx(x=df_est_latent$bin[df_est_latent$id==i], 
                                         y=df_est_latent$pred_t200[df_est_latent$id==i], 
                                         xout = 1:J2)$y
   Y_df2$pred_300[Y_df2$id==i] <- approx(x=df_est_latent$bin[df_est_latent$id==i], 
                                         y=df_est_latent$pred_t300[df_est_latent$id==i], 
                                         xout = 1:J2)$y
  }
  
# split unobserved time into intervals of length 100
Y_df2$bin_cat <- cut(Y_df2$sind_inx, breaks=c(0, 100, 200, 300, 400), include.lowest = T)
Y_df_lst2 <- split(Y_df2, f = Y_df2$bin_cat)
Y_df_lst2 <- Y_df_lst2[-1]
  
for(j in seq_along(t_vec2)){
    
    t <- t_vec2[j]
    pred_name <- paste("pred_", t, sep = "")
    
    # calculate AUC
    auc_lst <- lapply(Y_df_lst2[j:3], 
                       function(x){
                         row_id <- !is.na(x[, pred_name])
                         this_eta <- x[row_id, pred_name]
                         this_Y <- x[row_id, "Y"]
                         auc_pref <- performance(prediction(this_eta, this_Y), "auc")
                         return(auc_pref@y.values)
                })
      
    auc_vec <- c(rep(NA, 3-length(auc_lst)), unlist(auc_lst))
    auc_mat2[ , j] <- auc_vec

}


colnames(auc_mat2) <- c("100", "200", "300")
rownames(auc_mat2) <- c("(100, 200]", "(200, 300]", "(300, 400]")

```

```{r AUC_GLMMadaptive_Appl}
# calculate AUC on unbinned grid
auc_mat_ref2 <- matrix(NA, nrow = 3, ncol = 3)  # row index iteration, col index max observation time

# split unobserved time into intervals of length 100
df_est_latent_ref$bin_cat <- cut(df_est_latent_ref$sind_inx, 
                                 breaks=c(0, 100, 200, 300, 400), include.lowest = T)
Y_df_lst2_ref <- split(df_est_latent_ref, f = df_est_latent_ref$bin_cat)
Y_df_lst2_ref <- Y_df_lst2_ref[-1]
  
for(j in seq_along(t_vec2)){
    
    t <- t_vec2[j]
    pred_name <- paste("pred_t", t, sep = "")
    
    # calculate AUC
    auc_lst <- lapply(Y_df_lst2_ref[j:3], 
                       function(x){
                         row_id <- !is.na(x[, pred_name])
                         this_eta <- x[row_id, pred_name]
                         this_Y <- x[row_id, "Y"]
                         auc_pref <- performance(prediction(this_eta, this_Y), "auc")
                         return(auc_pref@y.values)
                })
      
    auc_vec <- c(rep(NA, 3-length(auc_lst)), unlist(auc_lst))
    auc_mat_ref2[ , j] <- auc_vec

  }


colnames(auc_mat_ref2) <- c("100", "200", "300")
rownames(auc_mat_ref2) <- c("(100, 200]", "(200, 300]", "(300, 400]")


```


```{r, Figure_appl, fig.cap="Predicted track of four randomly selected subjects from the daily weigh-in dataset. The grey solid line indicates the true latent continuous function. The dashed lines indicated predicted latent function tracks, and color indicates different observation time.", fig.height=6, fig.width=12}
rand_id2 <- sample(N2, 4)
fig2a <- Y_df2 %>%
  filter(id %in% rand_id2) %>%
  mutate_at(vars(pred_100, pred_200, pred_300), function(x)exp(x)/(1+exp(x))) %>%
  ggplot()+
  geom_point(aes(x=sind_inx, y=Y, col = "True outcome"), size = 0.5)+
  geom_line(aes(x=sind_inx, y = pred_100, col = "Up to 100"), linetype = "dashed", na.rm = T)+
  geom_line(aes(x=sind_inx, y = pred_200, col = "Up to 200"), linetype = "dashed", na.rm = T)+
  geom_line(aes(x=sind_inx, y = pred_300, col = "Up to 300"), linetype = "dashed", na.rm = T)+
  facet_wrap(~id, scales = "free_y")+
  labs(y = "Predicted latent function on probability scale", x = "Bin", 
       colour= "Observed time", title = "fGFPCA")+
  scale_color_manual(values=cbPalette)+
  scale_x_continuous(breaks = c(0, 100, 200, 300, 400))

fig2b <- df_est_latent_ref %>%
  filter(id %in% rand_id2) %>%
  mutate_at(vars(pred_t100, pred_t200, pred_t300), function(x)exp(x)/(1+exp(x))) %>%
  ggplot()+
  geom_point(aes(x=sind_inx, y=Y, col = "True outcome"), size = 0.5)+
  geom_line(aes(x=sind_inx, y = pred_t100, col = "Up to 100"), linetype = "dashed", na.rm = T)+
  geom_line(aes(x=sind_inx, y = pred_t200, col = "Up to 200"), linetype = "dashed", na.rm = T)+
  geom_line(aes(x=sind_inx, y = pred_t300, col = "Up to 300"), linetype = "dashed", na.rm = T)+
  facet_wrap(~id, scales = "free_y")+
  labs(y = "Predicted latent function on probability scale", x = "Bin", 
       colour= "Observed time", title = "GLMMadaptive")+
  scale_color_manual(values=cbPalette)+
  scale_x_continuous(breaks = c(0, 100, 200, 300, 400))

ggarrange(fig2a, fig2b, nrow = 1, common.legend = T)
```

```{r}
cbind(auc_mat2, auc_mat_ref2) %>%
  kable(booktabs = T, digit=2,
        caption = "AUC of fGFPCA and GLMM adaptive on the daily weigh-in dataset",
        align = "lcccccccc") %>%
  kable_styling() %>%
  add_header_above(c(" " = 1, "fGFPCA" = 3, "GLMMadaptive" = 3)) %>%
  add_header_above(c(" " = 1, "Maximum observed time" = 6)) %>% 
  group_rows(index = c("Prediction time window" = 3))

```

Discussion
=================================================================================================================================

The simulation study and case study of daily weigh-in data above have revealed the proposed method can achieve better predictive performance with much less computational burden. Compared to existing methods based on linear mixed model, fGFPCA can accommodate flexible correlation structure between repeated measure, which is particularly useful when outcome is generated from latent functions with wiggly or cyclic pattern. As in the simulation setting in Section 3, the binary outcomes are generated from a cyclic process. fGFPCA has much lower ISE and higher AUC than GLMM in this case, and its prediction accuracy also improves as more outcomes are collected, which is a desirable property for dynamic prediction where the datasets would be periodically updated with newly collected data. Moreover, fGFPCA also significantly reduce time spent on model fitting and predict, which is beneficial when repeated measures are dense or sample size is large. In fact, as far as we are aware of, the proposed method is the only feasible method that can handle the scale of the simulated datasets with compatible flexibility. 

While the proposed method achieved good accuracy of point prediction, it is more complicated to develop a straightforward, interpretable metric to evaluate the precision of prediction. The challenge arises from the fact that fitting local GLMMs (Step 2) and FPCA (Step 3) can both introduce uncertainty to the final prediction. Since they are implemented consecutively, it is not clear how the uncertainty from both procedures should be integrated into one single interval estimator. In practice, one could consider developing interval predictions conditional on the local GLMMs, quantifying uncertainty of individual score estimates from FPCA model alone. Since the score $\hat{\xi}_{ik}$ is essentially a maximum likelihood estimate, it is natural to estimate its variance based on likelihood theory with observed information ${I}(\hat{\xi}_{ik})$. Therefore, the variation of prediction interval is: 

$$Var(\hat{\eta_i}(s)-\eta_i(s))=\boldsymbol{\Phi}(s)I(\hat{\boldsymbol{\xi}}_i)\boldsymbol{\Phi}^T(s)+\hat{\sigma_{\epsilon}}^2$$
Where $\boldsymbol{\Phi}(s)=(\phi_1(s)...\phi_K(s))$ and $\hat{\boldsymbol{\xi}}_i=(\hat{\xi}_{i1}, ...,\hat{\xi}_{iK})$.


The hyperparameters introduced by the binning procedure in Step 1 can also affect the final predictive performance, thus further investigations are needed to quantify such effects. Predictive accuracy and computational speed can change with bin width, number of observations in each bin, whether the bins overlap with each other, also whether the bins are equally spaced along the functional grid. Moreover, if the bins are very narrow with only a few observations, unidentifiability issues can happen when fitting local mixed models. While this may not necessarily affect point predictions, it can cause numeric failure to estimate standard error and prediction intervals. In practice, we expect this to happen when the measurements are sparse or sample size is small. Under such scenarios, one may choose traditional mixed model framework instead to get valid estimates at the expense of computational time. 

It should also be noted that the binning procedure in Step 1 changes the density of repeated measures. We essentially assume that latent functions are smooth in small intervals, and treat adjacent observations as if they are generated from the same time point. The final prediction of latent continuous function tracks are made on the binned grid, integrating information from several adjacent observations in the same bin. In Section 3 and 4, this sparser grid was transformed to the original, un-binned grid using linear interpolation of the predicted latent function. However, this is by no means the only method to be considered. Such transformation can also be made by projection to a higher-dimensional space, similar as general additive model with a large number of basis functions. We suspect that this method would work better than linear interpolation when the original grid is sparse or irregular, but take longer time for computation.


<!-- -	Score bias: cannot demonstrate without repeat simulation -->

<!-- - Add section of de-bias score: conditional on subject. Need repeat simulation to demonstrate -->

References
=================================================================================================================================
<div id="refs"></div>