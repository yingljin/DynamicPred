---
title: "fGFPCA dynamic prediction (In-sample)"
output: 
  html_document:
    toc: true
    toc_float: 
      collapsed: false
    toc_depth: 3
date: "2022-11-03"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(here)
library(tidyverse)
library(ggplot2)
library(fcr)
library(refund)
library(lme4)
library(ggpubr)
set.seed(1025)
```


# Rationale

We assume that a binary function $Y(t)$ is generated by a latent Gaussian function $\eta(t)$, as follows: 

$$Y_i(t) \sim Binomial(\frac{exp(\eta_i(t))}{1+exp(\eta_i(t))})$$
$$\eta_i(t) = \beta_0(t)+b_i(t)$$

And this latent funciton can be approximated by fPCA:

$$\eta_i(t) = f_0(t)+\sum_{k=1}^{K}\xi_k\phi_k(t)+\epsilon_i(t)$$


# Toy simulation

## Simulation set-up

```{r, message=FALSE}
source(here("Code/ToySimulation.R"))
```

- For a random sample i: 

$$\eta_i(t) =f_0(t)+ \xi_{i1}sin(2\pi t)+\xi_{i2}cos(2\pi t)+\xi_{i3}sin(4\pi t)+\xi_{i4}cos(4\pi t)$$

where $\boldsymbol{\xi}_{i} \sim N(0, \boldsymbol{\Gamma})$. In the fPCA framework, $\boldsymbol{\Gamma}$ is a diagonal matrix and diagonal elements corresponds to eigenvalues. 

$$Y_i(t) \sim Binomial(\frac{exp(\eta_i(t))}{1+exp(\eta_i(t))})$$

In this simulation, we set $f_0(t) = 0$, $\boldsymbol{\Gamma}$ is a diagonal matrix with diagonal elements {1, 0.5, 0.25, 0.125}. 

```{r}
df %>% 
  filter(id %in% 1:4) %>% 
  mutate(probs = exp(eta_i)/(1+exp(eta_i))) %>%
  ggplot()+
  geom_line(aes(x=sind, y=eta_i, color = "True Gaussian function"))+
  geom_line(aes(x=sind, y=probs, color = "Probability scale"))+
  geom_point(aes(x=sind, y=Y, color = "Binary outcome"), size = 0.05)+
  facet_wrap(~id)+
  labs(title = "Four simulated subjects", color = "Legend")+
  scale_color_manual(values = c("True Gaussian function" = "blue", "Probability scale" = "red", "Binary outcome" = "black"))
```

## Local GLMMs

The entire time grid into 100 small intervals with equal length. The cut is made on the order index of measurement time, but not the actually time itself. Each bin its labeled by its midpoint (5, 15, 25, ..., 995). 

Within each interval labeled as $s$, we fit a local GLMM model: 

$$logit(E(Y_i^s)) = \beta_0^s+b_i^s$$
Then we use this series of models to estimate the latent Gaussian function. Each subject will have a different value of latent function at each bin. 


```{r}
source(here("Code/GLMM-FPCA.R"))
```

Figure below is the "mean" latent function across subject, which essential is $\beta_0^s$ from the series of mixed models. 

```{r,fig.height=4, fig.width=4}
ggplot(data.frame(x=mid, y=mean_latent))+
  geom_line(aes(x=x, y=y))
```


Figure below shows the estimated latent function for subjects 1-4:

```{r}
ggplot(df_pred_latent %>% filter(id %in% 1:4))+
  geom_line(aes(x = sind_inx, y=eta_hat, color = "Estimated"))+
  geom_line(aes(x=sind_inx, y = eta_i, color = "True"))+
  facet_wrap(~id)+
  labs(title = "Predicted latent functions on original grid", color = "Lgend")+
  scale_color_manual(values = c("Estimated" = "red", "True" = "Black"))
```
```{r}
df_pred_latent %>% filter(id %in% 1:4 & sind_inx %in% mid) %>%
  ggplot()+
  geom_line(aes(x = sind_inx, y=eta_hat, col = "Estimated"))+
  geom_line(aes(x=sind_inx, y = eta_i, col = "True"))+
  # geom_point(aes(x=sind_inx, y = Y, col = "blue"), size = 0.01)+
  facet_wrap(~id)+
  labs(title = "Predicted latent functions on binned grid", color = "Lgend")+
  scale_color_manual(values = c("Estimated" = "red", "True" = "Black"))
```


## FPCA

Here we fit FPCA on the estimated latent function from the previous step:

```{r, fig.height=4, fig.width=4}
## mean functions
plot(mid, fpca_fit$mu, ylab = "mean")
```

```{r, fig.height=8, fig.width=8}

## eigenfunctions
par(mfrow=c(2, 2))
plot(mid, fpca_fit$efunctions[, 1], ylab='PC1')
plot(mid, fpca_fit$efunctions[, 2], ylab='PC2')
plot(mid, fpca_fit$efunctions[, 3], ylab='PC3')
plot(mid, fpca_fit$efunctions[, 4], ylab='PC4')
```

## In-sample prediction

Now let's try to predict future outcomes based on observations $\boldsymbol{Y}_m$ up to $t_m$.

For in-sample prediction, we have observed the full outcome track, thus have estimated the full latent Gaussian function $\eta(s)$ on binned grid. 
For now, we will take the quantities up to $t_m$ and re-estimate the scores with empirical Bayes:

$$\hat{\boldsymbol{\xi}} = E(\boldsymbol{\xi}|\boldsymbol{Y}_m) =  \boldsymbol{\hat{\Gamma}\Phi ^T_m}(\boldsymbol{\Phi_m\hat{\Gamma}\Phi^T_m}+\hat{\sigma_{\epsilon}}^2\boldsymbol{I_m})^{-1}(\hat{\boldsymbol{\eta}}_m-\hat{\boldsymbol{f}}_0^m)$$
Where:

- $\hat{\boldsymbol{\Gamma}}$ is the diagonal covariance matrix of eigenvalues
- $\boldsymbol{\Phi}_m$ is the matrix of eigenfunctions up to $t_m$. These should be constant across all subjects.
- $\hat{\sigma_{\epsilon}}^2$ is the residual variance from FPCA
- $\hat{\boldsymbol{\eta}}_m$ is the estimated latent Gaussian function up to $t_m$
- $\hat{\boldsymbol{f}}_0^m$ is the FPCA mean function up to $t_m$

And then, we use the FPCA model for point prediction on the entire binned grid: 

$$\hat{\boldsymbol{\eta}} = \hat{\boldsymbol{f}}_0+\boldsymbol{\Phi}\hat{\boldsymbol{\xi}}$$

Here, $\boldsymbol{\Phi}$ is the eigenfunctions on the entire binned grid. For $\hat{\boldsymbol{\eta}}$, even though it is calculated for the entire binned grid, we will take values after $t_m$.


Let 

$$\boldsymbol{H} = \boldsymbol{\hat{\Gamma}\Phi ^T_m}(\boldsymbol{\Phi_m\hat{\Gamma}\Phi^T_m}+\hat{\sigma_{\epsilon}}^2\boldsymbol{I_m})^{-1}$$

The variance of $\hat{\boldsymbol{\eta}}$ can be estimated by:

$$Var(\hat{\boldsymbol{\eta}}-\boldsymbol{\eta})=\boldsymbol{\Phi}(\hat{\boldsymbol{\Gamma}}-\boldsymbol{H}\boldsymbol{\Phi_m}\hat{\boldsymbol{\Gamma}})\boldsymbol{\Phi}^T+\hat{\sigma_{\epsilon}}^2$$

It should be a $S \times S$ matrix; diagonals are variance at a specific time and off-diagonals are covariance between different times on binned grid. 


Below we try to do that with a few subjects:



```{r}
source(here("code/InSampPred.R"))
```

```{r}
# function to clean results
# make predictions with partial track up to 195, 395, 595

in_pred_clean <- function(subjID){
  # make predictions
  pred_t195 <- in_samp_dyn_pred(fpca_fit, df_pred_unique %>% filter(id==subjID & bin<=195))
  pred_t395 <- in_samp_dyn_pred(fpca_fit, df_pred_unique %>% filter(id==subjID & bin<=395))
  pred_t595 <- in_samp_dyn_pred(fpca_fit, df_pred_unique %>% filter(id==subjID & bin<=595))

  df_pred_t195 <- data.frame(mid,
                             eta=pred_t195$eta_pred,
                             eta_std = pred_t195$eta_std_var) %>%
    mutate(eta_lower = eta-eta_std, 
             eta_upper = eta+eta_std) %>% 
    filter(mid > 195)
  
  df_pred_t395 <- data.frame(mid,
                             eta=pred_t395$eta_pred,
                             eta_std = pred_t395$eta_std_var) %>%
    mutate(eta_lower = eta-eta_std, 
             eta_upper = eta+eta_std) %>% 
    filter(mid > 395)
  
  df_pred_t595 <- data.frame(mid,
                             eta=pred_t595$eta_pred,
                             eta_std = pred_t595$eta_std_var) %>%
    mutate(eta_lower = eta-eta_std, 
             eta_upper = eta+eta_std) %>% 
  filter(mid > 595)
  
  return(list(df_pred_t195, df_pred_t395, df_pred_t595))
}

# true latent function on the binned grid
df_true_eta <- df_pred_latent %>% 
  filter(sind_inx %in% mid) %>%
  select(id, bin, eta_i) %>% rename("mid" = "bin", "true" = "eta_i")

```


```{r, fig.width=12, fig.height=4}
# Try on subjects 1-4
bind_rows(in_pred_clean(1), .id = "type") %>%
  mutate(type=factor(type, levels=1:3, 
                     labels = c("Up to 195", "Up to 395", "Up to 595"))) %>%
  left_join(df_true_eta %>% filter(id==1), by = "mid") %>% 
  ggplot()+
  geom_line(aes(x=mid, y = true, col = "True"))+
  geom_line(aes(x=mid, y = eta), na.rm = T)+
  geom_line(aes(x=mid, y=eta_lower), linetype = "dashed", na.rm=T)+
  geom_line(aes(x=mid, y = eta_upper), linetype = "dashed", na.rm=T)+
  labs(title = "Subject 1", x = "bin", y="")+
  facet_wrap(~type)

bind_rows(in_pred_clean(2), .id = "type") %>%
  mutate(type=factor(type, levels=1:3, 
                     labels = c("Up to 195", "Up to 395", "Up to 595"))) %>%
  left_join(df_true_eta %>% filter(id==2), by = "mid") %>% 
  ggplot()+
  geom_line(aes(x=mid, y = true, col = "True"))+
  geom_line(aes(x=mid, y = eta), na.rm = T)+
  geom_line(aes(x=mid, y=eta_lower), linetype = "dashed", na.rm=T)+
  geom_line(aes(x=mid, y = eta_upper), linetype = "dashed", na.rm=T)+
  labs(title = "Subject 2", x = "bin", y="")+
  facet_wrap(~type)
```

## Some clarification points

1. When fitting local GLMMs, it seems that the intervals are essentially independent. We do not consider the effect of neighborhood intervals on a specific interval. 

## Out of sample prediction

Now let's say if we have a new subject $\boldsymbol{Y}$ with observations up to $t_m$ and $t_m$ is in mth bin. I can think of a few ways to  predict the following track, as below:

### Refit local GLMMs to estimate latent function

1. Local GLMMs: we will be able to estimate random effect of this subject up to mth bin, thus estimate the latent function up to the mth bin ($\hat{\boldsymbol{\eta}}_m$).

2. fPCA: We will have an incomplete function for this new observation. I think fpca.face can handle that.

3. Use the save formulas as in-sample prediction

### Do not estimate latent function

Let say on the binned grid, at each interval s, new sample has $n_s$ observations, among which $h_s$ are successes. $h_s = \sum_{j=1}^{n_S} I(Y_{sj}=1)$, j is the index for observation in a specific interval. 

We use the expectation of latent function conditonal on fPCA score to estimate the true latent function, so that the error term can be removed from following calculation:

$$\hat{\eta_s} = \hat{f}_{0}(s)+\phi(s)\boldsymbol{\xi}$$

Here, $\hat{f}_{0}(s)$ is the mean function from fPCA and $\phi(s)$ are eigenfunctions at time s, which is a K $\times$ 1 vector. 

Then the data is generated from a binomial distribution:

$$\begin{aligned}
 \boldsymbol{Y}_s & \sim Binomial(p_s) \\
 logit(p_s) & = \hat{\eta}_s = \hat{f}_{0}(s)+\phi(s)\boldsymbol{\xi}
\end{aligned}$$

#### Empirical Bayes

Now let's find the empirical bayes estimator of score $E(\boldsymbol{\xi}|\boldsymbol{Y})$, which is not likely to have closed form solution. 

$$E(\boldsymbol{\xi}|\boldsymbol{Y})=\int_{\boldsymbol{\xi}} \boldsymbol{\xi} p(\boldsymbol{\xi}|\boldsymbol{Y})d\boldsymbol{\xi}$$
$$p(\boldsymbol{\xi}|\boldsymbol{Y}) = \frac{p(\boldsymbol{Y}|\boldsymbol{\xi})p(\boldsymbol{\xi})}{\int_{\boldsymbol{\xi}}p(\boldsymbol{Y}|\boldsymbol{\xi})p(\boldsymbol{\xi})d\boldsymbol{\xi}}$$

Thus:

$$E(\boldsymbol{\xi}|\boldsymbol{Y})=\frac{\int_{\boldsymbol{\xi}}\boldsymbol{\xi}p(\boldsymbol{Y}|\boldsymbol{\xi})p(\boldsymbol{\xi})d\boldsymbol{\xi}}{\int_{\boldsymbol{\xi}}p(\boldsymbol{Y}|\boldsymbol{\xi})p(\boldsymbol{\xi})d\boldsymbol{\xi}}$$

We know that fPCA scores follow a multivariate normal distribution:

$$p(\boldsymbol{\xi}) = (2\pi)^{-K/2}det(\boldsymbol{\Gamma})^{-1/2}exp(-\boldsymbol{\xi^T \Gamma^{-1} \xi}/2)$$

And the binary outcome, conditional on score, follows a Binomial distribution:

$$\begin{aligned}
p(\boldsymbol{Y}|\boldsymbol{\xi}) &= \prod_{s=1}^{m}p(\boldsymbol{Y}_s|\boldsymbol{\xi}) \\
& = \prod_{s=1}^{m} \binom{h_s}{n_s}p_s^{h_s}(1-p_s)^{n_s-h_s}\\
logit(p_s) & = \hat{\eta}_s = \hat{f}_{0}(s)+\phi(s)\boldsymbol{\xi}
\end{aligned}$$

Please note that here, we are assuming $\boldsymbol{Y}_s$ is independent from each other given fPCA score. That is, all correlation across time within the same subject is introduced by underlying eigenfunctions. 

If we write this down for a single score: 

$$E(\xi_k|\boldsymbol{Y})=\frac{\int \xi_k \int p(\boldsymbol{Y}|\boldsymbol{\xi})p(\boldsymbol{\xi})d\boldsymbol{\xi_{(-k)}}d\xi_k}{\int_{\boldsymbol{\xi}}p(\boldsymbol{Y}|\boldsymbol{\xi})p(\boldsymbol{\xi})d\boldsymbol{\xi}}$$


In this case, I ran into the problem of numeric integration being too slow (adaptIntegrate multidimensional). The function value is also very small. Therefore, I turned to MLE instead. 


#### MLE

We utilize the same conditional likelihood of scores:


$$p(\boldsymbol{\xi}|\boldsymbol{Y}) = \frac{p(\boldsymbol{Y}|\boldsymbol{\xi})p(\boldsymbol{\xi})}{\int_{\boldsymbol{\xi}}p(\boldsymbol{Y}|\boldsymbol{\xi})p(\boldsymbol{\xi})d\boldsymbol{\xi}}$$

But now we don't need to calculated the marginal distribution of $\boldsymbol{Y}$ anymore (denominator). So we'll get:

$$\begin{aligned}
p(\boldsymbol{\xi}|\boldsymbol{Y}) & \propto p(\boldsymbol{Y}|\boldsymbol{\xi})p(\boldsymbol{\xi}) \\

l(\boldsymbol{\xi}|\boldsymbol{Y}) & \propto logp (\boldsymbol{Y}|\boldsymbol{\xi})+log p(\boldsymbol{\xi})
\end{aligned}$$

Plug in the conditional log-likelihood of $\boldsymbol{Y}$ and marginal log-likelihood of $\boldsymbol{\xi}$:

$$\begin{aligned}
l(\boldsymbol{\xi}|\boldsymbol{Y}) & \propto \sum_{s=1}^mh_s\eta_s-\sum_{s=1}^mn_slog(1+exp(\eta_s))-\boldsymbol{\xi^T\Gamma^{-1}\xi}/2\\
\frac{dl(\boldsymbol{\xi}|\boldsymbol{Y})}{d\boldsymbol{\xi}}& =\sum_{s=1}^mh_s\phi(s)-\sum_{s=1}^mn_s\frac{exp(\eta_s)}{1+exp(\eta_s)}\phi(s)-\boldsymbol{\xi^T\Gamma^{-1}} = 0
\end{aligned}$$

It is not easy to get a closed-form solution for the score equation above. We can try to get a numeric solution instead. Below are some visualization of the prediction performances

#### In- and out-of-sample prediction of latent function

```{r}
source(here("Code/OutSampMLE.R"))
```


```{r}
# out-of-sample
pred1_t195 <- out_samp_dyn_pred(df_new = df %>% filter(id == 1 & sind_inx <= 195) %>%
                                  select(-eta_i),
                                fpca_fit = fpca_fit)
pred1_t395 <- out_samp_dyn_pred(df_new = df %>% filter(id == 1 & sind_inx <= 395) %>%
                                  select(-eta_i),
                                fpca_fit = fpca_fit)

pred1_t595 <- out_samp_dyn_pred(df_new = df %>% filter(id == 1 & sind_inx <= 595) %>%
                                  select(-eta_i),
                                fpca_fit = fpca_fit)

df_out_pred1 <- data.frame(mid, pred1_t195,pred1_t395, pred1_t595)
df_out_pred1$pred1_t195[1:which(mid==195)] <- NA
df_out_pred1$pred1_t395[1:which(mid==395)] <- NA
df_out_pred1$pred1_t595[1:which(mid==595)] <- NA
df_out_pred1$true <- df[df$id==1 & df$sind_inx%in%mid, "eta_i"]  

```


```{r}
# in-sample  
df_in_pred1 <- data.frame(mid,
                          pred_t195 = in_samp_dyn_pred(fpca_fit, df_pred_unique %>% filter(id==1 & bin<=195))$eta_pred,
                          pred_t395 = in_samp_dyn_pred(fpca_fit, df_pred_unique %>% filter(id==1 & bin<=395))$eta_pred,
                          pred_t595 = in_samp_dyn_pred(fpca_fit, df_pred_unique %>% filter(id==1 & bin<=595))$eta_pred)

df_in_pred1$pred_t195[1:which(mid==195)] <- NA
df_in_pred1$pred_t395[1:which(mid==395)] <- NA
df_in_pred1$pred_t595[1:which(mid==595)] <- NA
df_in_pred1$true <- df[df$id==1 & df$sind_inx%in%mid, "eta_i"]  

```


```{r}
# compare point prediction with in-sample

## out-of-sample
ggplot(df_out_pred1)+
  geom_line(aes(x=mid, y=true, col="True"))+
  geom_line(aes(x=mid, y=pred1_t195, col="Up to 195"), na.rm = T)+
  geom_line(aes(x=mid, y=pred1_t395, col="Up to 395"), na.rm = T)+
  geom_line(aes(x=mid, y=pred1_t595, col="Up to 595"), na.rm = T)+
  labs(y = "Out-of-sample prediction", x = "Binned time")

## in-sample
ggplot(df_in_pred1)+
  geom_line(aes(x=mid, y=true, col="True"))+
  geom_line(aes(x=mid, y=pred_t195, col="Up to 195"), na.rm = T)+
  geom_line(aes(x=mid, y=pred_t395, col="Up to 395"), na.rm = T)+
  geom_line(aes(x=mid, y=pred_t595, col="Up to 595"), na.rm = T)+
  labs(y = "In-sample prediction", x = "Binned time")

```

- Repeat this for more subjects
- Interval prediction for MLE
- Bias of score for both in-sample and out-of-sample estimation



# Next steps:

1. Write out the formula for the BLUPs in the local GLMMs. Since we have a random intercept only model this should not be too complicated. (this is for out of sample prediction only)

2. Approximate these values numerically via numeric integration (like we did witht he AUC paper) and/or explicit laplace approximation 
3. Evaluate performance of dynamic predictions (both using "in sample" data -- those individuals whose BLUPs we can extract directly from the local glmm fits and out of sample data)
4. Write an R package (see Julia's fastGFPCA and Andrew's fcr pacakge as inspiration) which has functions for performing dynamic prediction using our framework
5. Implement a brief simulation study showing both that the approach works 

6. Deriving unbiased scores
 
As in our first paper (attached), the scores we obtain from the approach you're using are biased. We can show that the scores obtained from the local models (i.e. Step 3 in the fastGFPCA approach) are a linear function of the true scores.
1) Derive the result that the binned scores are linear in the true scores (I have already derived this result, but you should attempt to do this yourself.)
- Attempt to find an analytic solution for the bias factor. This may be challenging, so don't spend too much time here, but it would be an amazing result if we could do so

2) Assuming you cannot find an analytic solution for the bias factor, attempt to estimate this factor yourself. To do so implement the full fastGFPCA approach (i.e. Step 4, where we re-estimate the scores), then:
- Take the estimated scores for each subject from step 4 and and regress them on the scores from step 3 using linear regression. Do this separately for each eigenfunction
-  Identify and report the estimated coefficients and R^2 for each eigenfunction separately
- Use these models to create a de-biasing model for the Step 3 scores
Show these scores are closer (lower MSE) to the true scores than those obtained from Step 3, and that these result in lower MSE for the latent subject specific functoions
- Incorporate this result into the dynamic prediction framework you developed per the instructions above


7. FPCA eigenfunctions: how to extend to the original grid? Interpolation or projection?


Thoughts on 1: 

1. If we'd like to make predictions on more than one sample, we may be able to fit GLMM on test samples and predict $\eta(t)$
2. Do a different set of derivation with MLE+EB like [this paper](https://www.jstor.org/stable/pdf/2533715.pdf?refreqid=excelsior%3A135dd765c41e4b46eb39bb6ef7a71053&ab_segments=&origin=&acceptTC=1)

