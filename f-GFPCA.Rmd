---
title: "fGFPCA dynamic prediction (In-sample)"
output: 
  html_document:
    toc: true
    toc_float: 
      collapsed: false
    toc_depth: 3
date: "2022-11-03"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(here)
library(tidyverse)
library(ggplot2)
library(fcr)
library(refund)
library(lme4)
library(ggpubr)
set.seed(1025)
```

# Toy simulation

## Simulation set-up

```{r, message=FALSE}
source(here("Code/ToySimulation.R"))
```

- For a random sample i: 

$$\eta_i(t) =f_0(t)+ \xi_{i1}sin(2\pi t)+\xi_{i2}cos(2\pi t)+\xi_{i3}sin(4\pi t)+\xi_{i4}cos(4\pi t)$$

where $\boldsymbol{\xi}_{i} \sim N(0, \boldsymbol{\Gamma})$. In the fPCA framework, $\boldsymbol{\Gamma}$ is a diagonal matrix and diagonal elements corresponds to eigenvalues. 

$$Y_i(t) \sim Binomial(\frac{exp(\eta_i(t))}{1+exp(\eta_i(t))})$$

In this simulation, we set $f_0(t) = 0$, $\boldsymbol{\Gamma}$ is a diagonal matrix with diagonal elements {1, 0.5, 0.25, 0.125}. 

```{r}
df %>% 
  filter(id %in% 1:4) %>% 
  mutate(probs = exp(eta_i)/(1+exp(eta_i))) %>%
  ggplot()+
  geom_line(aes(x=sind, y=eta_i, color = "True Gaussian function"))+
  geom_line(aes(x=sind, y=probs, color = "Probability scale"))+
  geom_point(aes(x=sind, y=Y, color = "Binary outcome"), size = 0.05)+
  facet_wrap(~id)+
  labs(title = "Four simulated subjects", color = "Legend")+
  scale_color_manual(values = c("True Gaussian function" = "blue", "Probability scale" = "red", "Binary outcome" = "black"))
```

## Local GLMMs

The entire time grid into 100 small intervals with equal length. The cut is made on the order index of measurement time, but not the actually time itself. Each bin its labeled by its midpoint (5, 15, 25, ..., 995). 

Within each interval labeled as $s$, we fit a local GLMM model: 

$$logit(E(Y_i^s)) = \beta_0^s+b_i^s$$
Then we use this series of models to estimate the latent Gaussian function. Each subject will have a different value of latent function at each bin. 


```{r}
source(here("Code/GLMM-FPCA.R"))
```

Figure below is the "mean" latent function across subject, which essential is $\beta_0^s$ from the series of mixed models. 

```{r,fig.height=4, fig.width=4}
ggplot(data.frame(x=mid, y=mean_latent))+
  geom_line(aes(x=x, y=y))
```


Figure below shows the estimated latent function for subjects 1-4:

```{r}
ggplot(df_pred_latent %>% filter(id %in% 1:4))+
  geom_line(aes(x = sind_inx, y=eta_hat, color = "Estimated"))+
  geom_line(aes(x=sind_inx, y = eta_i, color = "True"))+
  facet_wrap(~id)+
  labs(title = "Predicted latent functions on original grid", color = "Lgend")+
  scale_color_manual(values = c("Estimated" = "red", "True" = "Black"))
```
```{r}
df_pred_latent %>% filter(id %in% 1:4 & sind_inx %in% mid) %>%
  ggplot()+
  geom_line(aes(x = sind_inx, y=eta_hat, col = "Estimated"))+
  geom_line(aes(x=sind_inx, y = eta_i, col = "True"))+
  # geom_point(aes(x=sind_inx, y = Y, col = "blue"), size = 0.01)+
  facet_wrap(~id)+
  labs(title = "Predicted latent functions on binned grid", color = "Lgend")+
  scale_color_manual(values = c("Estimated" = "red", "True" = "Black"))
```


## FPCA

Here we fit FPCA on the estimated latent function from the previous step:

```{r, fig.height=4, fig.width=4}
## mean functions
plot(mid, fpca_fit$mu, ylab = "mean")
```

```{r, fig.height=8, fig.width=8}

## eigenfunctions
par(mfrow=c(2, 2))
plot(mid, fpca_fit$efunctions[, 1], ylab='PC1')
plot(mid, fpca_fit$efunctions[, 2], ylab='PC2')
plot(mid, fpca_fit$efunctions[, 3], ylab='PC3')
plot(mid, fpca_fit$efunctions[, 4], ylab='PC4')
```

## In-sample prediction

Now let's try to predict future outcomes based on observations $\boldsymbol{Y}_m$ up to $t_m$.

For in-sample prediction, we have observed the full outcome track, thus have estimated the full latent Gaussian function $\eta(s)$ on binned grid. 
For now, we will take the quantities up to $t_m$ and re-estimate the scores with empirical Bayes:

$$\hat{\boldsymbol{\xi}} = E(\boldsymbol{\xi}|\boldsymbol{Y}_m) =  \boldsymbol{\hat{\Gamma}\Phi ^T_m}(\boldsymbol{\Phi_m\hat{\Gamma}\Phi^T_m}+\hat{\sigma_{\epsilon}}^2\boldsymbol{I_m})^{-1}(\hat{\boldsymbol{\eta}}_m-\hat{\boldsymbol{f}}_0^m)$$
Where:

- $\hat{\boldsymbol{\Gamma}}$ is the diagonal covariance matrix of eigenvalues
- $\boldsymbol{\Phi}_m$ is the matrix of eigenfunctions up to $t_m$. These should be constant across all subjects.
- $\hat{\sigma_{\epsilon}}^2$ is the residual variance from FPCA
- $\hat{\boldsymbol{\eta}}_m$ is the estimated latent Gaussian function up to $t_m$
- $\hat{\boldsymbol{f}}_0^m$ is the FPCA mean function up to $t_m$

And then, we use the FPCA model for point prediction on the entire binned grid: 

$$\hat{\boldsymbol{\eta}} = \hat{\boldsymbol{f}}_0+\boldsymbol{\Phi}\hat{\boldsymbol{\xi}}$$

Here, $\boldsymbol{\Phi}$ is the eigenfunctions on the entire binned grid. For $\hat{\boldsymbol{\eta}}$, even though it is calculated for the entire binned grid, we will take values after $t_m$.


Let 

$$\boldsymbol{H} = \boldsymbol{\hat{\Gamma}\Phi ^T_m}(\boldsymbol{\Phi_m\hat{\Gamma}\Phi^T_m}+\hat{\sigma_{\epsilon}}^2\boldsymbol{I_m})^{-1}$$

The variance of $\hat{\boldsymbol{\eta}}$ can be estimated by:

$$Var(\hat{\boldsymbol{\eta}}-\boldsymbol{\eta})=\boldsymbol{\Phi}(\hat{\boldsymbol{\Gamma}}-\boldsymbol{H}\boldsymbol{\Phi_m}\hat{\boldsymbol{\Gamma}})\boldsymbol{\Phi}^T+\hat{\sigma_{\epsilon}}^2$$

It should be a $S \times S$ matrix; diagonals are variance at a specific time and off-diagonals are covariance between different times on binned grid. 


Below we try to do that with a few subjects:



```{r}
source(here("code/DynPred.R"))

#in_samp_dyn_pred(fpca_fit, 20, df_pred_unique, 1)

```

```{r}
# function to clean results
# make predictions with partial track up to 20, 40, 60
clean_pred <- function(subj = 1){
  
  pred20 <- in_samp_dyn_pred(fpca_fit, 20, df_pred_unique, subj)
  df_pred20 <- data.frame(bin = mid, 
                          eta_i = pred20$eta_pred,
                          eta_i_std=pred20$eta_std_var,
                          type = "up to 195") 
  df_pred20$eta_i[df_pred20$bin<=195] <- NA
  df_pred20$eta_i_std[df_pred20$bin<=195] <- NA
  
  pred40 <- in_samp_dyn_pred(fpca_fit, 40, df_pred_unique, subj)
  df_pred40 <- data.frame(bin = mid, 
                          eta_i = pred40$eta_pred,
                          eta_i_std=pred40$eta_std_var,
                          type = "up to 395") 
  df_pred40$eta_i[df_pred20$bin<=395] <- NA
  df_pred40$eta_i_std[df_pred20$bin<=395] <- NA
  
  pred60 <- in_samp_dyn_pred(fpca_fit, 60, df_pred_unique, subj)
  df_pred60 <- data.frame(bin = mid, 
                          eta_i = pred60$eta_pred,
                          eta_i_std=pred60$eta_std_var,
                          type = "up to 595")
  df_pred60$eta_i[df_pred20$bin<=595] <- NA
  df_pred60$eta_i_std[df_pred20$bin<=595] <- NA
  
  
  df_pred <- df_pred20 %>%
    add_row(df_pred40) %>% 
    add_row(df_pred60) %>%
    mutate(eta_i_lower = eta_i-eta_i_std, 
           eta_i_upper = eta_i+eta_i_std) %>%
    left_join(df_pred_latent %>% 
                 filter(id==subj & sind_inx==bin) %>%
                 select(bin, eta_i) %>% 
                 rename(true_eta = eta_i), by = "bin")
  
  return(df_pred)
}



```

```{r, fig.width=12, fig.height=4}
# Try on subjects 1-4
clean_pred(1) %>%
  ggplot()+
  geom_line(aes(x=bin, y = true_eta), col = "red")+
  geom_line(aes(x=bin, y = eta_i), na.rm = T)+
  geom_line(aes(x=bin, y=eta_i_lower), linetype = "dashed", na.rm=T)+
  geom_line(aes(x=bin, y = eta_i_upper), linetype = "dashed", na.rm=T)+
  labs(title = "Subject 1", x = "bin", y="")+
  facet_wrap(~type)

clean_pred(2) %>%
  ggplot()+
  geom_line(aes(x=bin, y = true_eta), col = "red")+
  geom_line(aes(x=bin, y = eta_i), na.rm=T)+
  geom_line(aes(x=bin, y=eta_i_lower), linetype = "dashed", na.rm=T)+
  geom_line(aes(x=bin, y = eta_i_upper), linetype = "dashed", na.rm=T)+
  labs(title = "Subject 2", x = "bin", y="")+
  facet_wrap(~type)

clean_pred(3) %>%
  ggplot()+
  geom_line(aes(x=bin, y=true_eta), col = "red")+
  geom_line(aes(x=bin, y = eta_i), na.rm = T)+
  geom_line(aes(x=bin, y=eta_i_lower), linetype = "dashed", na.rm=T)+
  geom_line(aes(x=bin, y = eta_i_upper), linetype = "dashed", na.rm=T)+
  labs(title = "Subject 3", x = "bin", y="")+
  facet_wrap(~type)


clean_pred(4) %>%
  ggplot()+
  geom_line(aes(x=bin, y=true_eta), col = "red")+
  geom_line(aes(x=bin, y = eta_i), na.rm = T)+
  geom_line(aes(x=bin, y=eta_i_lower), linetype = "dashed", na.rm=T)+
  geom_line(aes(x=bin, y = eta_i_upper), linetype = "dashed", na.rm=T)+
  labs(title = "Subject 4", x = "bin", y="")+
  facet_wrap(~type)
```


## Out of sample prediction

Now let's say if we have a new subject $\boldsymbol{Y}_{new}$ with observations up to $t_m$ and $t_m$ is in mth bin. Also, we do not wish to refit any model, including the local GLMMs. In the first step, we need to use the series of local GLMMs that's already been fitted to get the random intercepts for this new observations ($\boldsymbol{b}_{new}$).

We first focus on one specific interval s. 

1. Local GLMMs: we will be able to estimate random effect of this subject up to mth bin, thus estimate the latent function up to the mth bin ($\hat{\boldsymbol{\eta}}_m$).

2. fPCA: We will have an incomplete function for this new observation. I think fpca.face can handle that.

3. Use the save formulas as in-sample prediction


```{r}
```


# Next steps:

1. Write out the formula for the BLUPs in the local GLMMs. Since we have a random intercept only model this should not be too complicated. (this is for out of sample prediction only)
2. Approximate these values numerically via numeric integration (like we did witht he AUC paper) and/or explicit laplace approximation 
3. Evaluate performance of dynamic predictions (both using "in sample" data -- those individuals whose BLUPs we can extract directly from the local glmm fits and out of sample data)
4. Write an R package (see Julia's fastGFPCA and Andrew's fcr pacakge as inspiration) which has functions for performing dynamic prediction using our framework
5. Implement a brief simulation study showing both that the approach works 

6. Deriving unbiased scores
 
As in our first paper (attached), the scores we obtain from the approach you're using are biased. We can show that the scores obtained from the local models (i.e. Step 3 in the fastGFPCA approach) are a linear function of the true scores.
1) Derive the result that the binned scores are linear in the true scores (I have already derived this result, but you should attempt to do this yourself.)
- Attempt to find an analytic solution for the bias factor. This may be challenging, so don't spend too much time here, but it would be an amazing result if we could do so

2) Assuming you cannot find an analytic solution for the bias factor, attempt to estimate this factor yourself. To do so implement the full fastGFPCA approach (i.e. Step 4, where we re-estimate the scores), then:
- Take the estimated scores for each subject from step 4 and and regress them on the scores from step 3 using linear regression. Do this separately for each eigenfunction
-  Identify and report the estimated coefficients and R^2 for each eigenfunction separately
- Use these models to create a de-biasing model for the Step 3 scores
Show these scores are closer (lower MSE) to the true scores than those obtained from Step 3, and that these result in lower MSE for the latent subject specific functoions
- Incorporate this result into the dynamic prediction framework you developed per the instructions above


7. FPCA eigenfunctions: how to extend to the original grid? Interpolation or projection?


Thoughts on 1: 

1. If we'd like to make predictions on more than one sample, we may be able to fit GLMM on test samples and predict $\eta(t)$
2. Do a different set of derivation with MLE+EB like [this paper](https://www.jstor.org/stable/pdf/2533715.pdf?refreqid=excelsior%3A135dd765c41e4b46eb39bb6ef7a71053&ab_segments=&origin=&acceptTC=1)

