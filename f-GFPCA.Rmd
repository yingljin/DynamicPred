---
title: "fGFPCA dynamic prediction"
output: 
  html_document:
    toc: true
    toc_float: 
      collapsed: false
    toc_depth: 4
date: "2022-11-03"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(here)
library(tidyverse)
library(ggplot2)
theme_set(theme_minimal())
library(fcr)
library(refund)
library(lme4)
library(ggpubr)
library(gridExtra)
set.seed(1025)
```


# Rationale

We assume that a binary function $Y(t)$ is generated by a latent Gaussian function $\eta(t)$, as follows: 

$$Y_i(t) \sim Binomial(\frac{exp(\eta_i(t))}{1+exp(\eta_i(t))})$$
$$\eta_i(t) = \beta_0(t)+b_i(t)$$

And this latent funciton can be approximated based on Karhunen–Loève expansion:

$$\eta_i(t) = f_0(t)+\sum_{k=1}^{K}\xi_k\phi_k(t)+\epsilon_i(t)$$


# Toy simulation

## Simulation set-up

```{r, message=FALSE}
source(here("Code/ToySimulation.R"))
```

- For a random sample i: 

$$\eta_i(t) =f_0(t)+ \xi_{i1}sin(2\pi t)+\xi_{i2}cos(2\pi t)+\xi_{i3}sin(4\pi t)+\xi_{i4}cos(4\pi t)$$

where $\boldsymbol{\xi}_{i} \sim N(0, \boldsymbol{\Gamma})$. 

$$Y_i(t) \sim Binomial(\frac{exp(\eta_i(t))}{1+exp(\eta_i(t))})$$

In this simulation, we set $f_0(t) = 0$, $\boldsymbol{\Gamma}$ is a diagonal matrix with diagonal elements {1, 0.5, 0.25, 0.125}. 

```{r}
df %>% 
  filter(id %in% 1:4) %>% 
  mutate(probs = exp(eta_i)/(1+exp(eta_i))) %>%
  ggplot()+
  geom_line(aes(x=sind, y=eta_i, color = "True Gaussian function"))+
  geom_line(aes(x=sind, y=probs, color = "Probability scale"))+
  geom_point(aes(x=sind, y=Y, color = "Binary outcome"), size = 0.05)+
  facet_wrap(~id)+
  labs(title = "Four simulated subjects", color = "Legend")+
  scale_color_manual(values = c("True Gaussian function" = "blue", "Probability scale" = "red", "Binary outcome" = "black"))
```

<span style="color: red;"> Question 1: in FPCA framework, $\boldsymbol{\Gamma}$ is a diagonal matrix of eigenvalues. However in my inplemetation, the eigenvalues are much larger than true $\boldsymbol{\Gamma}$. I am not sure why this happened. Does it has to do with the GLMM step, or decreased density when binning observations? </span>


## Local GLMMs

The entire time grid is binned into 100 small intervals with equal length. The cut is made on the order index of measurement time, but not the actually time itself. Each bin its labeled by its midpoint (5, 15, 25, ..., 995). 

Within each interval labeled as $s$, we fit a local GLMM model: 

$$logit(E(Y_i^s)) = \beta_0^s+b_i^s$$
Then we use this series of models to estimate the latent Gaussian function. Each subject will have a different value of latent function at each bin. 

<span style="color: red;">Question 2: When fitting local GLMMs, it seems that the intervals are essentially independent. We do not consider the effect of neighborhood intervals on a specific interval. This means that correlation between repeated measures from the same subject is only introduced by the latent function? </span>


```{r}
source(here("Code/GLMM-FPCA.R"))
```

Figure below is the "mean" latent function across subject, which essential is $\beta_0^s$ from the series of mixed models. 

```{r,fig.height=4, fig.width=4}
ggplot(data.frame(x=mid, y=mean_latent))+
  geom_line(aes(x=x, y=y))
```


Figure below shows the estimated latent function for subjects 1-4:

```{r}
ggplot(df_pred_latent %>% filter(id %in% 1:4))+
  geom_line(aes(x = sind_inx, y=eta_hat, color = "Estimated"))+
  geom_line(aes(x=sind_inx, y = eta_i, color = "True"))+
  facet_wrap(~id)+
  labs(title = "Predicted latent functions on original grid", color = "Lgend")+
  scale_color_manual(values = c("Estimated" = "red", "True" = "Black"))
```
```{r}
df_pred_latent %>% filter(id %in% 1:4 & sind_inx %in% mid) %>%
  ggplot()+
  geom_line(aes(x = sind_inx, y=eta_hat, col = "Estimated"))+
  geom_line(aes(x=sind_inx, y = eta_i, col = "True"))+
  # geom_point(aes(x=sind_inx, y = Y, col = "blue"), size = 0.01)+
  facet_wrap(~id)+
  labs(title = "Predicted latent functions on binned grid", color = "Lgend")+
  scale_color_manual(values = c("Estimated" = "red", "True" = "Black"))
```


## FPCA

Here we fit FPCA on the estimated latent function from the previous step:

```{r, fig.height=4, fig.width=4}
## mean functions
plot(mid, fpca_fit$mu, ylab = "mean")
```

```{r, fig.height=8, fig.width=8}

## eigenfunctions
par(mfrow=c(2, 2))
plot(mid, fpca_fit$efunctions[, 1], ylab='PC1')
plot(mid, fpca_fit$efunctions[, 2], ylab='PC2')
plot(mid, fpca_fit$efunctions[, 3], ylab='PC3')
plot(mid, fpca_fit$efunctions[, 4], ylab='PC4')
```


<span style="color: red;">Follow-up on question 1: It looks like the PC function ranges are different from true PC functions. Maybe some of its variation is absorbed by the score, causing $\boldsymbol{\Gamma}$ to be much greater than its true value?</span>

## In-sample prediction

Now let's try to predict future outcomes based on observations $\boldsymbol{Y}_m$ up to $t_m$.

For in-sample prediction, we have observed the full outcome track, thus have estimated the full latent Gaussian function $\eta(s)$ on binned grid. 
For now, we will take the quantities up to $t_m$ and re-estimate the scores with empirical Bayes:

$$\hat{\boldsymbol{\xi}} = E(\boldsymbol{\xi}|\boldsymbol{Y}_m) =  \boldsymbol{\hat{\Gamma}\Phi ^T_m}(\boldsymbol{\Phi_m\hat{\Gamma}\Phi^T_m}+\hat{\sigma_{\epsilon}}^2\boldsymbol{I_m})^{-1}(\hat{\boldsymbol{\eta}}_m-\hat{\boldsymbol{f}}_0^m)$$
Where:

- $\boldsymbol{\hat{\Gamma}}$ is the diagonal covariance matrix of eigenvalues
- $\boldsymbol{\Phi}_m$ is the matrix of eigenfunctions up to $t_m$. These should be constant across all subjects.
- $\hat{\sigma_{\epsilon}}^2$ is the residual variance from FPCA
- $\hat{\boldsymbol{\eta}}_m$ is the estimated latent Gaussian function up to $t_m$
- $\hat{\boldsymbol{f}}_0^m$ is the FPCA mean function up to $t_m$

And then, we use the FPCA model for point prediction on the entire binned grid: 

$$\boldsymbol{\hat{\eta}} = \hat{\boldsymbol{f}}_0+\boldsymbol{\Phi}\boldsymbol{\hat{\xi}}$$

Here, $\boldsymbol{\Phi}$ is the eigenfunctions on the entire binned grid. For $\hat{\boldsymbol{\eta}}$, even though it is calculated for the entire binned grid, we will take values after $t_m$.


Let 

$$\boldsymbol{H} = \boldsymbol{\hat{\Gamma}\Phi ^T_m}(\boldsymbol{\Phi_m\hat{\Gamma}\Phi^T_m}+\hat{\sigma_{\epsilon}}^2\boldsymbol{I_m})^{-1}$$

The variance of $\hat{\boldsymbol{\eta}}$ can be estimated by:

$$Var(\hat{\boldsymbol{\eta}}-\boldsymbol{\eta})=\boldsymbol{\Phi}(\hat{\boldsymbol{\Gamma}}-\boldsymbol{H}\boldsymbol{\Phi_m}\hat{\boldsymbol{\Gamma}})\boldsymbol{\Phi}^T+\hat{\sigma_{\epsilon}}^2$$

It should be a $S \times S$ matrix; diagonals are variance at a specific time and off-diagonals are covariance between different times on binned grid. 


Below we try to do that with a few subjects:



```{r}
source(here("code/InSampPred.R"))
```

```{r}
# function to clean results
# make predictions with partial track up to 195, 395, 595

in_pred_clean <- function(subjID=1, mid=mid){
  # make predictions
  pred_t195 <- in_samp_dyn_pred(fpca_fit, df_pred_unique %>% filter(id==subjID & bin<=195))
  pred_t395 <- in_samp_dyn_pred(fpca_fit, df_pred_unique %>% filter(id==subjID & bin<=395))
  pred_t595 <- in_samp_dyn_pred(fpca_fit, df_pred_unique %>% filter(id==subjID & bin<=595))
  # put predicted function and standard error in to a data frame
  # calculate confidence interval
  df_pred_t195 <- data.frame(mid,
                             eta=pred_t195$eta_pred,
                             eta_std = pred_t195$eta_std_var) %>%
    mutate(eta_lower = eta-eta_std, 
             eta_upper = eta+eta_std)
  
  df_pred_t395 <- data.frame(mid,
                             eta=pred_t395$eta_pred,
                             eta_std = pred_t395$eta_std_var) %>%
    mutate(eta_lower = eta-eta_std, 
             eta_upper = eta+eta_std) 
  
  df_pred_t595 <- data.frame(mid,
                             eta=pred_t595$eta_pred,
                             eta_std = pred_t595$eta_std_var) %>%
    mutate(eta_lower = eta-eta_std, 
             eta_upper = eta+eta_std) 
  # remove observed part
  df_pred_t195[df_pred_t195$mid<=195, c("eta", "eta_std", "eta_lower", "eta_upper")] <- NA
  df_pred_t395[df_pred_t195$mid<=395, c("eta", "eta_std", "eta_lower", "eta_upper")] <- NA
  df_pred_t595[df_pred_t195$mid<=595, c("eta", "eta_std", "eta_lower", "eta_upper")] <- NA
  # put into one data frame
  df_pred <- bind_rows(df_pred_t195, df_pred_t395, df_pred_t595, .id="type") %>%
    mutate(type = factor(type, levels=1:3, 
                         labels = c("Up to 195", "Up to 395", "Up to 595")))
  
  
  return(df_pred)
}

# true latent function on the binned grid
df_true_eta <- df_pred_latent %>% 
  filter(sind_inx %in% mid) %>%
  select(id, bin, eta_i) %>% rename("mid" = "bin", "true" = "eta_i")

```


```{r}
# Try on subjects 1-4
in_pred_subj1 <- in_pred_clean(1, mid) %>% 
  left_join(df_true_eta %>% filter(id==1), by = "mid") 
in_pred_subj2 <- in_pred_clean(2, mid) %>% 
  left_join(df_true_eta %>% filter(id==2), by = "mid")
in_pred_subj3 <- in_pred_clean(3, mid) %>% 
  left_join(df_true_eta %>% filter(id==3), by = "mid") 
in_pred_subj4 <- in_pred_clean(4, mid) %>% 
  left_join(df_true_eta %>% filter(id==4), by = "mid") 
```


```{r, fig.width=12, fig.height=4}
in_pred_subj1 %>%
  ggplot()+
  geom_line(aes(x=mid, y = true), col = "red")+
  geom_line(aes(x=mid, y = eta), na.rm = T)+
  geom_line(aes(x=mid, y=eta_lower), linetype = "dashed", na.rm=T)+
  geom_line(aes(x=mid, y = eta_upper), linetype = "dashed", na.rm=T)+
  labs(title = "Subject 1", x = "bin", y="")+
  facet_wrap(~type)

in_pred_subj2 %>% 
  ggplot()+
  geom_line(aes(x=mid, y = true), col = "red")+
  geom_line(aes(x=mid, y = eta), na.rm = T)+
  geom_line(aes(x=mid, y=eta_lower), linetype = "dashed", na.rm=T)+
  geom_line(aes(x=mid, y = eta_upper), linetype = "dashed", na.rm=T)+
  labs(title = "Subject 2", x = "bin", y="")+
  facet_wrap(~type)

in_pred_subj3 %>% 
  ggplot()+
  geom_line(aes(x=mid, y = true), col = "red")+
  geom_line(aes(x=mid, y = eta), na.rm = T)+
  geom_line(aes(x=mid, y=eta_lower), linetype = "dashed", na.rm=T)+
  geom_line(aes(x=mid, y = eta_upper), linetype = "dashed", na.rm=T)+
  labs(title = "Subject 3", x = "bin", y="")+
  facet_wrap(~type)

in_pred_subj4 %>% 
  ggplot()+
  geom_line(aes(x=mid, y = true), col = "red")+
  geom_line(aes(x=mid, y = eta), na.rm = T)+
  geom_line(aes(x=mid, y=eta_lower), linetype = "dashed", na.rm=T)+
  geom_line(aes(x=mid, y = eta_upper), linetype = "dashed", na.rm=T)+
  labs(title = "Subject 4", x = "bin", y="")+
  facet_wrap(~type)
```


## Out of sample prediction

Now let's say if we have a new subject $\boldsymbol{Y}$ with observations up to $t_m$ and $t_m$ is in mth bin. It seems there are more than one ways to  predict the following track. The first thing coming to mind was in fact to 1) refit local GLMMs to estimate a incomplete latent function; 2) refit local FPCA with this additional incomplete latent function track; and 3) repeat score estimation from in-sample procedure.

However, if we wish to make prediction without re-estimating any model (neither local GLMMs nor FPCA), we need to use the fitted FPCA model (specifically, $\boldsymbol{\hat{f}}_0$, $\boldsymbol{\Phi}$, $\boldsymbol{\hat{\Gamma}}$, $\hat{\sigma}_{\epsilon}$) to estimate the scores for this new observations (out-of-sample scores). I can think of two ways of doing this:

1. Empirical Bayes: similar to in-sample procedure, estimate the posterior mean 
2. MLE, as often used in longitudinal models. 

Below I experimented with both procedures. 

#### Empirical Bayes

Let say on the binned grid, at each interval s, new sample has $n_s$ observations, among which $h_s$ are successes. $h_s = \sum_{j=1}^{n_S} I(Y_{sj}=1)$, j is the index for observation in a specific interval. 

Now let's find the empirical bayes estimator of score $E(\boldsymbol{\xi}|\boldsymbol{Y})$, which is not likely to have closed form solution. 

$$E(\boldsymbol{\xi}|\boldsymbol{Y})=\int \boldsymbol{\xi} p(\boldsymbol{\xi}|\boldsymbol{Y})d\boldsymbol{\xi}$$
$$p(\boldsymbol{\xi}|\boldsymbol{Y}) = \frac{p(\boldsymbol{Y}|\boldsymbol{\xi})p(\boldsymbol{\xi})}{\int p(\boldsymbol{Y}|\boldsymbol{\xi})p(\boldsymbol{\xi})d\boldsymbol{\xi}}$$

Thus:

$$E(\boldsymbol{\xi}|\boldsymbol{Y})=\frac{\int \boldsymbol{\xi}p(\boldsymbol{Y}|\boldsymbol{\xi})p(\boldsymbol{\xi})d\boldsymbol{\xi}}{\int p(\boldsymbol{Y}|\boldsymbol{\xi})p(\boldsymbol{\xi})d\boldsymbol{\xi}}$$

We know that FPCA scores follow a multivariate normal distribution:

$$p(\boldsymbol{\xi}) = (2\pi)^{-K/2}det(\boldsymbol{\Gamma})^{-1/2}exp(-\boldsymbol{\xi^T \Gamma^{-1} \xi}/2)$$

And the binary outcome, conditional on score, follows a Binomial distribution:

$$\begin{aligned}
p(\boldsymbol{Y}|\boldsymbol{\xi}) &= \prod_{s=1}^{m}p(\boldsymbol{Y}_s|\boldsymbol{\xi}) \\
& = \prod_{s=1}^{m} \binom{h_s}{n_s}p_s^{h_s}(1-p_s)^{n_s-h_s}\\
logit(p_s) & = \hat{\eta}_s = \hat{f}_{0}(s)+\phi(s)\boldsymbol{\xi}
\end{aligned}$$

<span style="color:red;">Follow up on question 2: Please note that here, we are assuming $\boldsymbol{Y}_s$ is independent from each other given fPCA score. That is, all correlation across time within the same subject is introduced by underlying eigenfunctions. </span>

<!-- If we write this down for a single score:  -->

<!-- $$E(\xi_k|\boldsymbol{Y})=\frac{\int \xi_k \int p(\boldsymbol{Y}|\boldsymbol{\xi})p(\boldsymbol{\xi})d\boldsymbol{\xi_{(-k)}}d\xi_k}{\int_{\boldsymbol{\xi}}p(\boldsymbol{Y}|\boldsymbol{\xi})p(\boldsymbol{\xi})d\boldsymbol{\xi}}$$ -->

It is very difficult to get a closed-form solution of this formula. Therefore, I tried to approximate the integrals with numeric integration (*adaptIntegrate* function from *cubature* package). In this case, I ran into the problem of computation time. Numeric integration was very, very slow, taking roughly an hour for just the denominator of one subject. The function value is also very small.

#### MLE

We maximize the same conditional likelihood of scores (conditional on observations):


$$p(\boldsymbol{\xi}|\boldsymbol{Y}) = \frac{p(\boldsymbol{Y}|\boldsymbol{\xi})p(\boldsymbol{\xi})}{\int p(\boldsymbol{Y}|\boldsymbol{\xi})p(\boldsymbol{\xi})d\boldsymbol{\xi}}$$

But now we don't need to calculated the marginal distribution of $\boldsymbol{Y}$ anymore (denominator), since it would be constant wrt $\boldsymbol{\xi}$:


$$\begin{aligned}
p(\boldsymbol{\xi}|\boldsymbol{Y}) & \propto p(\boldsymbol{Y}|\boldsymbol{\xi})p(\boldsymbol{\xi}) \\
l(\boldsymbol{\xi}|\boldsymbol{Y}) & \propto logp (\boldsymbol{Y}|\boldsymbol{\xi})+log p(\boldsymbol{\xi})
\end{aligned}$$


Plug in the conditional log-likelihood of $\boldsymbol{Y}$ and marginal log-likelihood of $\boldsymbol{\xi}$:

$$\begin{aligned}
l(\boldsymbol{\xi}|\boldsymbol{Y}) & \propto \sum_{s=1}^mh_s\eta_s-\sum_{s=1}^mn_slog(1+exp(\eta_s))-\boldsymbol{\xi^T\Gamma^{-1}\xi}/2\\
\frac{dl(\boldsymbol{\xi}|\boldsymbol{Y})}{d\boldsymbol{\xi}}& =\sum_{s=1}^mh_s\phi(s)-\sum_{s=1}^mn_s\frac{exp(\eta_s)}{1+exp(\eta_s)}\phi(s)-\boldsymbol{\xi^T\Gamma^{-1}} = 0
\end{aligned}$$

It is not easy to get a closed-form solution for the score equation above. We can try to get a numeric solution instead (*multiroot* function from *rootSolve* package). 

#### Laplace approximation

It seems that $E(\boldsymbol{\xi}|\boldsymbol{Y})$ and $p(\boldsymbol{\xi}|\boldsymbol{Y})$ can be connected through Laplace approximation. Specifically, posterior mean estimated from Laplace method is the same as posterior mode. In this sense, the two methods above are essentially equivalent.

Below I have experimented with *LaplaceDemon::LaplaceApproximation*: https://search.r-project.org/CRAN/refmans/LaplacesDemon/html/LaplaceApproximation.html

<!-- https://bookdown.org/rdpeng/advstatcomp/laplace-approximation.html -->

### In-/Out-of-sample prediction results of latent function

```{r, results='hide', message=FALSE}
source(here("Code/OutSampMLE.R"))
source(here("Code/OutSampBayes.R"))
```


```{r, MLE_pred}
# clean MLE out-of-sample prediction results
out_pred_clean <- function(subjID, mid=mid){
  # Prediction
  pred_t195 <- out_samp_dyn_pred(df_new = df %>% filter(id==subjID & sind_inx<=195) %>%
                                  select(-eta_i),
                                fpca_fit = fpca_fit)
  pred_t395 <- out_samp_dyn_pred(df_new = df %>% filter(id==subjID & sind_inx<=395) %>%
                                  select(-eta_i),
                                fpca_fit = fpca_fit)
  pred_t595 <- out_samp_dyn_pred(df_new = df %>% filter(id==subjID & sind_inx<=595) %>%
                                  select(-eta_i),
                                fpca_fit = fpca_fit)
  
  # clean
  df_pred <- data.frame(mid, 
                        pred_t195=pred_t195$eta_pred,
                        pred_t395=pred_t395$eta_pred, 
                        pred_t595=pred_t595$eta_pred)
  df_pred[df_pred$mid<=195, "pred_t195"] <- NA
  df_pred[df_pred$mid<=395, "pred_t395"] <- NA
  df_pred[df_pred$mid<=595, "pred_t595"] <- NA
  
  df_pred <- df_pred %>% pivot_longer(starts_with("pred_t"), 
                                      names_to = "type", values_to = "eta") %>%
    mutate(type = factor(type, levels=c("pred_t195", "pred_t395", "pred_t595"),
                         labels=c("Up to 195", "Up to 395", "Up to 595")))
  
  return(df_pred)
}

```


```{r}
out_pred_subj1 <- out_pred_clean(1, mid) %>% 
  left_join(df_true_eta %>% filter(id==1), by = "mid")
out_pred_subj2 <- out_pred_clean(2, mid) %>% 
  left_join(df_true_eta %>% filter(id==2), by = "mid")
out_pred_subj3 <- out_pred_clean(3, mid) %>% 
  left_join(df_true_eta %>% filter(id==3), by = "mid")
out_pred_subj4 <- out_pred_clean(4, mid) %>% 
  left_join(df_true_eta %>% filter(id==4), by = "mid")
```


```{r, laplace_ped}
# clean MLE out-of-sample prediction results
out_pred_clean_laplace <- function(subjID=1, mid=mid){
  # Prediction
  pred_t195 <- out_pred_laplace(fpca_fit = fpca_fit, df_new = df %>% filter(id==subjID & sind_inx<=195) %>% select(-eta_i))
  pred_t395 <- out_pred_laplace(fpca_fit = fpca_fit, df_new = df %>% filter(id==subjID & sind_inx<=395) %>% select(-eta_i))
  pred_t595 <- out_pred_laplace(fpca_fit = fpca_fit, df_new = df %>% filter(id==subjID & sind_inx<=595) %>% select(-eta_i))
  
  # clean
  df_pred <- data.frame(mid, 
                        pred_t195=pred_t195$eta_pred,
                        pred_t395=pred_t395$eta_pred, 
                        pred_t595=pred_t595$eta_pred)
  df_pred[df_pred$mid<=195, "pred_t195"] <- NA
  df_pred[df_pred$mid<=395, "pred_t395"] <- NA
  df_pred[df_pred$mid<=595, "pred_t595"] <- NA
  
  df_pred <- df_pred %>% pivot_longer(starts_with("pred_t"), 
                                      names_to = "type", values_to = "eta") %>%
    mutate(type = factor(type, levels=c("pred_t195", "pred_t395", "pred_t595"),
                         labels=c("Up to 195", "Up to 395", "Up to 595")))
  
  return(df_pred)
}

```

```{r, results='hide', message=FALSE}
out_pred_lap_subj1 <- out_pred_clean_laplace(1, mid) %>% 
  left_join(df_true_eta %>% filter(id==1), by = "mid")
out_pred_lap_subj2 <- out_pred_clean_laplace(2, mid) %>% 
  left_join(df_true_eta %>% filter(id==2), by = "mid")
out_pred_lap_subj3 <- out_pred_clean_laplace(3, mid) %>% 
  left_join(df_true_eta %>% filter(id==3), by = "mid")
out_pred_lap_subj4 <- out_pred_clean_laplace(4, mid) %>% 
  left_join(df_true_eta %>% filter(id==4), by = "mid")
```


```{r, fig.width=12, fig.height=4}
# compare point prediction with in-sample
# subject 1
grid.arrange(
  out_pred_subj1 %>% 
    group_by("type") %>%
    ggplot()+
    geom_line(aes(x=mid, y=true), col = "black")+
    geom_line(aes(x=mid, y=eta, col= type, group = type), linetype="dashed", na.rm = T,
              show.legend = FALSE)+
    labs(x="Binned time", title = "Subject 1 out-of-sample MLE"),
  
  out_pred_lap_subj1 %>% 
    group_by("type") %>%
    ggplot()+
    geom_line(aes(x=mid, y=true), col = "black")+
    geom_line(aes(x=mid, y=eta, col= type, group = type), linetype="dashed", na.rm = T,
              show.legend = FALSE)+
    labs(x="Binned time", title = "Subject 1 out-of-sample Laplace"),
  
  in_pred_subj1 %>%
    ggplot()+
    geom_line(aes(x=mid, y=true), col="black")+
    geom_line(aes(x=mid, y=eta, col=type, group=type), linetype="dashed", na.rm = T,
              show.legend = FALSE)+
    labs(x="Binned time", title = "Subject 1 in-sample"),
  nrow=1)

# subject 2
grid.arrange(
  out_pred_subj2 %>% 
    group_by("type") %>%
    ggplot()+
    geom_line(aes(x=mid, y=true), col = "black")+
    geom_line(aes(x=mid, y=eta, col= type, group = type), linetype="dashed", na.rm = T,
              show.legend = FALSE)+
    labs(x="Binned time", title = "Subject 2 out-of-sample MLE"),
  
  out_pred_lap_subj2 %>% 
    group_by("type") %>%
    ggplot()+
    geom_line(aes(x=mid, y=true), col = "black")+
    geom_line(aes(x=mid, y=eta, col= type, group = type), linetype="dashed", na.rm = T,
              show.legend = FALSE)+
    labs(x="Binned time", title = "Subject 2 out-of-sample Laplace"),
  
  in_pred_subj2 %>%
    ggplot()+
    geom_line(aes(x=mid, y=true), col="black")+
    geom_line(aes(x=mid, y=eta, col=type, group=type), linetype="dashed", na.rm = T,
              show.legend = FALSE)+
    labs(x="Binned time", title = "Subject 2 in-sample"),
  nrow=1)

# subject 3
grid.arrange(
  out_pred_subj3 %>% 
    group_by("type") %>%
    ggplot()+
    geom_line(aes(x=mid, y=true), col = "black")+
    geom_line(aes(x=mid, y=eta, col= type, group = type), linetype="dashed", na.rm = T,
              show.legend = FALSE)+
    labs(x="Binned time", title = "Subject 3 out-of-sample MLE"),
  
  out_pred_lap_subj3 %>% 
    group_by("type") %>%
    ggplot()+
    geom_line(aes(x=mid, y=true), col = "black")+
    geom_line(aes(x=mid, y=eta, col= type, group = type), linetype="dashed", na.rm = T,
              show.legend = FALSE)+
    labs(x="Binned time", title = "Subject 3 out-of-sample Laplace"),
  
  in_pred_subj3 %>%
    ggplot()+
    geom_line(aes(x=mid, y=true), col="black")+
    geom_line(aes(x=mid, y=eta, col=type, group=type), linetype="dashed", na.rm = T,
              show.legend = FALSE)+
    labs(x="Binned time", title = "Subject 3 in-sample"),
  nrow=1)

# subject 4
grid.arrange(
  out_pred_subj4 %>% 
    group_by("type") %>%
    ggplot()+
    geom_line(aes(x=mid, y=true), col = "black")+
    geom_line(aes(x=mid, y=eta, col= type, group = type), linetype="dashed", na.rm = T,
              show.legend = FALSE)+
    labs(x="Binned time", title = "Subject 4 out-of-sample MLE"),
  
  out_pred_lap_subj4 %>% 
    group_by("type") %>%
    ggplot()+
    geom_line(aes(x=mid, y=true), col = "black")+
    geom_line(aes(x=mid, y=eta, col= type, group = type), linetype="dashed", na.rm = T,
              show.legend = FALSE)+
    labs(x="Binned time", title = "Subject 4 out-of-sample Laplace"),
  
  in_pred_subj4 %>%
    ggplot()+
    geom_line(aes(x=mid, y=true), col="black")+
    geom_line(aes(x=mid, y=eta, col=type, group=type), linetype="dashed", na.rm = T,
              show.legend = FALSE)+
    labs(x="Binned time", title = "Subject 4 in-sample"),
  nrow=1)

```



### In-/Out-of-sample PC scores

Here I explored the estimated scores, comparing them with the true scores from simulation setup. The figures actually didn't look biased to me. Perhaps it is an artifact of small functional range. Need to think about that. 

```{r, results='hide', message=FALSE}
# true scores:xi

# scores
in_score <- matrix(NA, nrow=N, ncol=4)
out_score_MLE <- matrix(NA, nrow=N, ncol=4)
out_score_laplace <- matrix(NA, nrow=N, ncol=4)

for(i in 1:N){
  in_score[i, ] <- in_samp_dyn_pred(fpca_fit=fpca_fit, 
                   df_part=df_pred_unique %>% filter(id==i & bin<=595))$score
  out_score_MLE[i,] <- out_samp_dyn_pred(df %>% filter(id==i & sind_inx<=595) %>%
                                  select(-eta_i),
                                  fpca_fit=fpca_fit)$score_out
  out_score_laplace[i,] <- out_pred_laplace(fpca_fit=fpca_fit, 
                                            df_new = df %>% filter(id==i & sind_inx<=595) %>% select(-eta_i))$score_out
}

# bias
in_bias <- in_score-xi
out_bias_MLE <- out_score_MLE-xi
out_bias_laplace <- out_score_laplace-xi
```

```{r, fig.height=4, fig.width=8}
grid.arrange(
  out_bias_MLE %>% data.frame() %>%
    pivot_longer(1:4, names_to = "PC", values_to = "Bias") %>%
    ggplot()+
    geom_boxplot(aes(x=PC, y=Bias))+
    stat_summary(aes(x=PC, y=Bias), fun = mean, geom = "point", col = "red")+
    labs(title = "Out-of-sample MLE"),
  
  out_bias_laplace %>% data.frame() %>%
    pivot_longer(1:4, names_to = "PC", values_to = "Bias") %>%
    ggplot()+
    geom_boxplot(aes(x=PC, y=Bias))+
    stat_summary(aes(x=PC, y=Bias), fun = mean, geom = "point", col = "red")+
    labs(title = "Out-of-sample laplace"),
  
  in_bias %>% data.frame() %>%
    pivot_longer(1:4, names_to = "PC", values_to = "Bias") %>%
    ggplot()+
    geom_boxplot(aes(x=PC, y=Bias))+
    stat_summary(aes(x=PC, y=Bias), fun = mean, geom = "point", col = "red")+
    labs(title = "In-sample"), 
nrow=1)

```

### Prediction interval

<span style="color:red;"> Questions 3: a lot of problems on this part </span>

With an estimate of scores $\hat{\boldsymbol{\xi}}$

$$\boldsymbol{\hat{\eta}}=\boldsymbol{\hat{f_0}}+\boldsymbol{\Phi\hat{\xi}}$$
$$\begin{aligned}
\hat{\boldsymbol{\eta}}-\boldsymbol{\eta}&=\boldsymbol{\hat{f_0}}+\boldsymbol{\Phi\hat{\xi}}-\boldsymbol{\eta}\\
&= \boldsymbol{\hat{f_0}-f_0}+\boldsymbol{\Phi(\hat{\xi}-\xi)}-\boldsymbol{\epsilon}
\end{aligned}$$

$$Var(\hat{\boldsymbol{\eta}}-\boldsymbol{\eta})=Var(\boldsymbol{\hat{f_0}-f_0})+\boldsymbol{\Phi Var(\hat{\xi}-\xi) \Phi^T}+Var(\epsilon)$$

- The three components should be mutually independent? 
- Through some derivation, I think $Var(\boldsymbol{f_0})=\frac{Var(\boldsymbol{Y})}{N} = \frac{1}{N}[\boldsymbol{\Phi\Gamma \Phi^T}+\sigma_{\epsilon}^2\boldsymbol{I}_m]$.
- Variance of $\hat{\boldsymbol{\xi}}$ will depend on the method used for its estimation. But isn't the estimated done conditioning on $Var(\boldsymbol{\xi}) = \boldsymbol{\Gamma}$?
- $Var(\epsilon) =\hat{\sigma}_{\epsilon}^2$ from FPCA
- May need to circle back to in-sample estimation and dig into where that formula came from


# Next steps:

- Interval prediction for MLE
- Bias of PC scores
- FPCA eigenfunctions: how to extend to the original grid? Interpolation or projection?


