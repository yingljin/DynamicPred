---
title: "fGFPCA dynamic prediction"
output: 
  html_document:
    toc: true
    toc_float: 
      collapsed: false
    toc_depth: 4
date: "2022-11-03"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(here)
library(tidyverse)
library(ggplot2)
theme_set(theme_minimal())
library(fcr)
library(refund)
library(lme4)
library(ggpubr)
library(gridExtra)
set.seed(1025)
```


# Rationale

We assume that a binary function $Y(t)$ is generated by a latent Gaussian function $\eta(t)$, as follows: 

$$Y_i(t) \sim Binomial(\frac{exp(\eta_i(t))}{1+exp(\eta_i(t))})$$
$$\eta_i(t) = \beta_0(t)+b_i(t)$$

And this latent funciton can be approximated by fPCA:

$$\eta_i(t) = f_0(t)+\sum_{k=1}^{K}\xi_k\phi_k(t)+\epsilon_i(t)$$


# Toy simulation

## Simulation set-up

```{r, message=FALSE}
source(here("Code/ToySimulation.R"))
```

- For a random sample i: 

$$\eta_i(t) =f_0(t)+ \xi_{i1}sin(2\pi t)+\xi_{i2}cos(2\pi t)+\xi_{i3}sin(4\pi t)+\xi_{i4}cos(4\pi t)$$

where $\boldsymbol{\xi}_{i} \sim N(0, \boldsymbol{\Gamma})$. In the fPCA framework, $\boldsymbol{\Gamma}$ is a diagonal matrix and diagonal elements corresponds to eigenvalues. 

$$Y_i(t) \sim Binomial(\frac{exp(\eta_i(t))}{1+exp(\eta_i(t))})$$

In this simulation, we set $f_0(t) = 0$, $\boldsymbol{\Gamma}$ is a diagonal matrix with diagonal elements {1, 0.5, 0.25, 0.125}. 

```{r}
df %>% 
  filter(id %in% 1:4) %>% 
  mutate(probs = exp(eta_i)/(1+exp(eta_i))) %>%
  ggplot()+
  geom_line(aes(x=sind, y=eta_i, color = "True Gaussian function"))+
  geom_line(aes(x=sind, y=probs, color = "Probability scale"))+
  geom_point(aes(x=sind, y=Y, color = "Binary outcome"), size = 0.05)+
  facet_wrap(~id)+
  labs(title = "Four simulated subjects", color = "Legend")+
  scale_color_manual(values = c("True Gaussian function" = "blue", "Probability scale" = "red", "Binary outcome" = "black"))
```

## Local GLMMs

The entire time grid into 100 small intervals with equal length. The cut is made on the order index of measurement time, but not the actually time itself. Each bin its labeled by its midpoint (5, 15, 25, ..., 995). 

Within each interval labeled as $s$, we fit a local GLMM model: 

$$logit(E(Y_i^s)) = \beta_0^s+b_i^s$$
Then we use this series of models to estimate the latent Gaussian function. Each subject will have a different value of latent function at each bin. 

<span style="color: red;">When fitting local GLMMs, it seems that the intervals are essentially independent. We do not consider the effect of neighborhood intervals on a specific interval. This means that correlation between repeated measures from the same subject is only introduced by the latent function.</span>


```{r}
source(here("Code/GLMM-FPCA.R"))
```

Figure below is the "mean" latent function across subject, which essential is $\beta_0^s$ from the series of mixed models. 

```{r,fig.height=4, fig.width=4}
ggplot(data.frame(x=mid, y=mean_latent))+
  geom_line(aes(x=x, y=y))
```


Figure below shows the estimated latent function for subjects 1-4:

```{r}
ggplot(df_pred_latent %>% filter(id %in% 1:4))+
  geom_line(aes(x = sind_inx, y=eta_hat, color = "Estimated"))+
  geom_line(aes(x=sind_inx, y = eta_i, color = "True"))+
  facet_wrap(~id)+
  labs(title = "Predicted latent functions on original grid", color = "Lgend")+
  scale_color_manual(values = c("Estimated" = "red", "True" = "Black"))
```
```{r}
df_pred_latent %>% filter(id %in% 1:4 & sind_inx %in% mid) %>%
  ggplot()+
  geom_line(aes(x = sind_inx, y=eta_hat, col = "Estimated"))+
  geom_line(aes(x=sind_inx, y = eta_i, col = "True"))+
  # geom_point(aes(x=sind_inx, y = Y, col = "blue"), size = 0.01)+
  facet_wrap(~id)+
  labs(title = "Predicted latent functions on binned grid", color = "Lgend")+
  scale_color_manual(values = c("Estimated" = "red", "True" = "Black"))
```


## FPCA

Here we fit FPCA on the estimated latent function from the previous step:

```{r, fig.height=4, fig.width=4}
## mean functions
plot(mid, fpca_fit$mu, ylab = "mean")
```

```{r, fig.height=8, fig.width=8}

## eigenfunctions
par(mfrow=c(2, 2))
plot(mid, fpca_fit$efunctions[, 1], ylab='PC1')
plot(mid, fpca_fit$efunctions[, 2], ylab='PC2')
plot(mid, fpca_fit$efunctions[, 3], ylab='PC3')
plot(mid, fpca_fit$efunctions[, 4], ylab='PC4')
```

## In-sample prediction

Now let's try to predict future outcomes based on observations $\boldsymbol{Y}_m$ up to $t_m$.

For in-sample prediction, we have observed the full outcome track, thus have estimated the full latent Gaussian function $\eta(s)$ on binned grid. 
For now, we will take the quantities up to $t_m$ and re-estimate the scores with empirical Bayes:

$$\hat{\boldsymbol{\xi}} = E(\boldsymbol{\xi}|\boldsymbol{Y}_m) =  \boldsymbol{\hat{\Gamma}\Phi ^T_m}(\boldsymbol{\Phi_m\hat{\Gamma}\Phi^T_m}+\hat{\sigma_{\epsilon}}^2\boldsymbol{I_m})^{-1}(\hat{\boldsymbol{\eta}}_m-\hat{\boldsymbol{f}}_0^m)$$
Where:

- $\hat{\boldsymbol{\Gamma}}$ is the diagonal covariance matrix of eigenvalues
- $\boldsymbol{\Phi}_m$ is the matrix of eigenfunctions up to $t_m$. These should be constant across all subjects.
- $\hat{\sigma_{\epsilon}}^2$ is the residual variance from FPCA
- $\hat{\boldsymbol{\eta}}_m$ is the estimated latent Gaussian function up to $t_m$
- $\hat{\boldsymbol{f}}_0^m$ is the FPCA mean function up to $t_m$

And then, we use the FPCA model for point prediction on the entire binned grid: 

$$\hat{\boldsymbol{\eta}} = \hat{\boldsymbol{f}}_0+\boldsymbol{\Phi}\hat{\boldsymbol{\xi}}$$

Here, $\boldsymbol{\Phi}$ is the eigenfunctions on the entire binned grid. For $\hat{\boldsymbol{\eta}}$, even though it is calculated for the entire binned grid, we will take values after $t_m$.


Let 

$$\boldsymbol{H} = \boldsymbol{\hat{\Gamma}\Phi ^T_m}(\boldsymbol{\Phi_m\hat{\Gamma}\Phi^T_m}+\hat{\sigma_{\epsilon}}^2\boldsymbol{I_m})^{-1}$$

The variance of $\hat{\boldsymbol{\eta}}$ can be estimated by:

$$Var(\hat{\boldsymbol{\eta}}-\boldsymbol{\eta})=\boldsymbol{\Phi}(\hat{\boldsymbol{\Gamma}}-\boldsymbol{H}\boldsymbol{\Phi_m}\hat{\boldsymbol{\Gamma}})\boldsymbol{\Phi}^T+\hat{\sigma_{\epsilon}}^2$$

It should be a $S \times S$ matrix; diagonals are variance at a specific time and off-diagonals are covariance between different times on binned grid. 


Below we try to do that with a few subjects:



```{r}
source(here("code/InSampPred.R"))
```

```{r}
# function to clean results
# make predictions with partial track up to 195, 395, 595

in_pred_clean <- function(subjID=1, mid=mid){
  # make predictions
  pred_t195 <- in_samp_dyn_pred(fpca_fit, df_pred_unique %>% filter(id==subjID & bin<=195))
  pred_t395 <- in_samp_dyn_pred(fpca_fit, df_pred_unique %>% filter(id==subjID & bin<=395))
  pred_t595 <- in_samp_dyn_pred(fpca_fit, df_pred_unique %>% filter(id==subjID & bin<=595))
  # put predicted function and standard error in to a data frame
  # calculate confidence interval
  df_pred_t195 <- data.frame(mid,
                             eta=pred_t195$eta_pred,
                             eta_std = pred_t195$eta_std_var) %>%
    mutate(eta_lower = eta-eta_std, 
             eta_upper = eta+eta_std)
  
  df_pred_t395 <- data.frame(mid,
                             eta=pred_t395$eta_pred,
                             eta_std = pred_t395$eta_std_var) %>%
    mutate(eta_lower = eta-eta_std, 
             eta_upper = eta+eta_std) 
  
  df_pred_t595 <- data.frame(mid,
                             eta=pred_t595$eta_pred,
                             eta_std = pred_t595$eta_std_var) %>%
    mutate(eta_lower = eta-eta_std, 
             eta_upper = eta+eta_std) 
  # remove observed part
  df_pred_t195[df_pred_t195$mid<=195, c("eta", "eta_std", "eta_lower", "eta_upper")] <- NA
  df_pred_t395[df_pred_t195$mid<=395, c("eta", "eta_std", "eta_lower", "eta_upper")] <- NA
  df_pred_t595[df_pred_t195$mid<=595, c("eta", "eta_std", "eta_lower", "eta_upper")] <- NA
  # put into one data frame
  df_pred <- bind_rows(df_pred_t195, df_pred_t395, df_pred_t595, .id="type") %>%
    mutate(type = factor(type, levels=1:3, 
                         labels = c("Up to 195", "Up to 395", "Up to 595")))
  
  
  return(df_pred)
}

# true latent function on the binned grid
df_true_eta <- df_pred_latent %>% 
  filter(sind_inx %in% mid) %>%
  select(id, bin, eta_i) %>% rename("mid" = "bin", "true" = "eta_i")

```


```{r}
# Try on subjects 1-4
in_pred_subj1 <- in_pred_clean(1, mid) %>% 
  left_join(df_true_eta %>% filter(id==1), by = "mid") 
in_pred_subj2 <- in_pred_clean(2, mid) %>% 
  left_join(df_true_eta %>% filter(id==2), by = "mid")
in_pred_subj3 <- in_pred_clean(3, mid) %>% 
  left_join(df_true_eta %>% filter(id==3), by = "mid") 
in_pred_subj4 <- in_pred_clean(4, mid) %>% 
  left_join(df_true_eta %>% filter(id==4), by = "mid") 
```


```{r, fig.width=12, fig.height=4}
in_pred_subj1 %>%
  ggplot()+
  geom_line(aes(x=mid, y = true), col = "red")+
  geom_line(aes(x=mid, y = eta), na.rm = T)+
  geom_line(aes(x=mid, y=eta_lower), linetype = "dashed", na.rm=T)+
  geom_line(aes(x=mid, y = eta_upper), linetype = "dashed", na.rm=T)+
  labs(title = "Subject 1", x = "bin", y="")+
  facet_wrap(~type)

in_pred_subj2 %>% 
  ggplot()+
  geom_line(aes(x=mid, y = true), col = "red")+
  geom_line(aes(x=mid, y = eta), na.rm = T)+
  geom_line(aes(x=mid, y=eta_lower), linetype = "dashed", na.rm=T)+
  geom_line(aes(x=mid, y = eta_upper), linetype = "dashed", na.rm=T)+
  labs(title = "Subject 2", x = "bin", y="")+
  facet_wrap(~type)

in_pred_subj3 %>% 
  ggplot()+
  geom_line(aes(x=mid, y = true), col = "red")+
  geom_line(aes(x=mid, y = eta), na.rm = T)+
  geom_line(aes(x=mid, y=eta_lower), linetype = "dashed", na.rm=T)+
  geom_line(aes(x=mid, y = eta_upper), linetype = "dashed", na.rm=T)+
  labs(title = "Subject 3", x = "bin", y="")+
  facet_wrap(~type)

in_pred_subj4 %>% 
  ggplot()+
  geom_line(aes(x=mid, y = true), col = "red")+
  geom_line(aes(x=mid, y = eta), na.rm = T)+
  geom_line(aes(x=mid, y=eta_lower), linetype = "dashed", na.rm=T)+
  geom_line(aes(x=mid, y = eta_upper), linetype = "dashed", na.rm=T)+
  labs(title = "Subject 4", x = "bin", y="")+
  facet_wrap(~type)
```


## Out of sample prediction

Now let's say if we have a new subject $\boldsymbol{Y}$ with observations up to $t_m$ and $t_m$ is in mth bin. I can think of a few ways to  predict the following track, as below:

### Refit local GLMMs to estimate latent function

1. Local GLMMs: we will be able to estimate random effect of this subject up to mth bin, thus estimate the latent function up to the mth bin ($\hat{\boldsymbol{\eta}}_m$).

2. fPCA: We will have an incomplete function for this new observation. I think fpca.face can handle that.

3. Use the save formulas as in-sample prediction

### Do not estimate latent function

Let say on the binned grid, at each interval s, new sample has $n_s$ observations, among which $h_s$ are successes. $h_s = \sum_{j=1}^{n_S} I(Y_{sj}=1)$, j is the index for observation in a specific interval. 

We use the expectation of latent function conditonal on fPCA score to estimate the true latent function, so that the error term can be removed from following calculation:

$$\hat{\eta_s} = \hat{f}_{0}(s)+\phi(s)\boldsymbol{\xi}$$

Here, $\hat{f}_{0}(s)$ is the mean function from fPCA and $\phi(s)$ are eigenfunctions at time s, which is a K $\times$ 1 vector. 

Then the data is generated from a binomial distribution:

$$\begin{aligned}
 \boldsymbol{Y}_s & \sim Binomial(p_s) \\
 logit(p_s) & = \hat{\eta}_s = \hat{f}_{0}(s)+\phi(s)\boldsymbol{\xi}
\end{aligned}$$

#### Empirical Bayes

Now let's find the empirical bayes estimator of score $E(\boldsymbol{\xi}|\boldsymbol{Y})$, which is not likely to have closed form solution. 

$$E(\boldsymbol{\xi}|\boldsymbol{Y})=\int_{\boldsymbol{\xi}} \boldsymbol{\xi} p(\boldsymbol{\xi}|\boldsymbol{Y})d\boldsymbol{\xi}$$
$$p(\boldsymbol{\xi}|\boldsymbol{Y}) = \frac{p(\boldsymbol{Y}|\boldsymbol{\xi})p(\boldsymbol{\xi})}{\int_{\boldsymbol{\xi}}p(\boldsymbol{Y}|\boldsymbol{\xi})p(\boldsymbol{\xi})d\boldsymbol{\xi}}$$

Thus:

$$E(\boldsymbol{\xi}|\boldsymbol{Y})=\frac{\int_{\boldsymbol{\xi}}\boldsymbol{\xi}p(\boldsymbol{Y}|\boldsymbol{\xi})p(\boldsymbol{\xi})d\boldsymbol{\xi}}{\int_{\boldsymbol{\xi}}p(\boldsymbol{Y}|\boldsymbol{\xi})p(\boldsymbol{\xi})d\boldsymbol{\xi}}$$

We know that fPCA scores follow a multivariate normal distribution:

$$p(\boldsymbol{\xi}) = (2\pi)^{-K/2}det(\boldsymbol{\Gamma})^{-1/2}exp(-\boldsymbol{\xi^T \Gamma^{-1} \xi}/2)$$

And the binary outcome, conditional on score, follows a Binomial distribution:

$$\begin{aligned}
p(\boldsymbol{Y}|\boldsymbol{\xi}) &= \prod_{s=1}^{m}p(\boldsymbol{Y}_s|\boldsymbol{\xi}) \\
& = \prod_{s=1}^{m} \binom{h_s}{n_s}p_s^{h_s}(1-p_s)^{n_s-h_s}\\
logit(p_s) & = \hat{\eta}_s = \hat{f}_{0}(s)+\phi(s)\boldsymbol{\xi}
\end{aligned}$$

Please note that here, we are assuming $\boldsymbol{Y}_s$ is independent from each other given fPCA score. That is, all correlation across time within the same subject is introduced by underlying eigenfunctions. 

If we write this down for a single score: 

$$E(\xi_k|\boldsymbol{Y})=\frac{\int \xi_k \int p(\boldsymbol{Y}|\boldsymbol{\xi})p(\boldsymbol{\xi})d\boldsymbol{\xi_{(-k)}}d\xi_k}{\int_{\boldsymbol{\xi}}p(\boldsymbol{Y}|\boldsymbol{\xi})p(\boldsymbol{\xi})d\boldsymbol{\xi}}$$


In this case, I ran into the problem of numeric integration being too slow (adaptIntegrate multidimensional). The function value is also very small. Therefore, I turned to MLE instead. 


#### MLE

We utilize the same conditional likelihood of scores:


$$p(\boldsymbol{\xi}|\boldsymbol{Y}) = \frac{p(\boldsymbol{Y}|\boldsymbol{\xi})p(\boldsymbol{\xi})}{\int_{\boldsymbol{\xi}}p(\boldsymbol{Y}|\boldsymbol{\xi})p(\boldsymbol{\xi})d\boldsymbol{\xi}}$$

But now we don't need to calculated the marginal distribution of $\boldsymbol{Y}$ anymore (denominator). So we'll get:

$$\begin{aligned}
p(\boldsymbol{\xi}|\boldsymbol{Y}) & \propto p(\boldsymbol{Y}|\boldsymbol{\xi})p(\boldsymbol{\xi}) \\

l(\boldsymbol{\xi}|\boldsymbol{Y}) & \propto logp (\boldsymbol{Y}|\boldsymbol{\xi})+log p(\boldsymbol{\xi})
\end{aligned}$$

Plug in the conditional log-likelihood of $\boldsymbol{Y}$ and marginal log-likelihood of $\boldsymbol{\xi}$:

$$\begin{aligned}
l(\boldsymbol{\xi}|\boldsymbol{Y}) & \propto \sum_{s=1}^mh_s\eta_s-\sum_{s=1}^mn_slog(1+exp(\eta_s))-\boldsymbol{\xi^T\Gamma^{-1}\xi}/2\\
\frac{dl(\boldsymbol{\xi}|\boldsymbol{Y})}{d\boldsymbol{\xi}}& =\sum_{s=1}^mh_s\phi(s)-\sum_{s=1}^mn_s\frac{exp(\eta_s)}{1+exp(\eta_s)}\phi(s)-\boldsymbol{\xi^T\Gamma^{-1}} = 0
\end{aligned}$$

It is not easy to get a closed-form solution for the score equation above. We can try to get a numeric solution instead. Below are some visualization of the prediction performances

#### Laplace approximation

Posterior mean from Laplace method is the same as posterior mode. In this sense, empirical bayes method and MLE are in fact equivalent. 

https://search.r-project.org/CRAN/refmans/LaplacesDemon/html/LaplaceApproximation.html
https://bookdown.org/rdpeng/advstatcomp/laplace-approximation.html

### In-/Out-of-sample prediction of latent function

```{r, results='hide', message=FALSE}
source(here("Code/OutSampMLE.R"))
source(here("Code/OutSampBayes.R"))
```


```{r, MLE_pred}
# clean MLE out-of-sample prediction results
out_pred_clean <- function(subjID, mid=mid){
  # Prediction
  pred_t195 <- out_samp_dyn_pred(df_new = df %>% filter(id==subjID & sind_inx<=195) %>%
                                  select(-eta_i),
                                fpca_fit = fpca_fit)
  pred_t395 <- out_samp_dyn_pred(df_new = df %>% filter(id==subjID & sind_inx<=395) %>%
                                  select(-eta_i),
                                fpca_fit = fpca_fit)
  pred_t595 <- out_samp_dyn_pred(df_new = df %>% filter(id==subjID & sind_inx<=595) %>%
                                  select(-eta_i),
                                fpca_fit = fpca_fit)
  
  # clean
  df_pred <- data.frame(mid, 
                        pred_t195=pred_t195$eta_pred,
                        pred_t395=pred_t395$eta_pred, 
                        pred_t595=pred_t595$eta_pred)
  df_pred[df_pred$mid<=195, "pred_t195"] <- NA
  df_pred[df_pred$mid<=395, "pred_t395"] <- NA
  df_pred[df_pred$mid<=595, "pred_t595"] <- NA
  
  df_pred <- df_pred %>% pivot_longer(starts_with("pred_t"), 
                                      names_to = "type", values_to = "eta") %>%
    mutate(type = factor(type, levels=c("pred_t195", "pred_t395", "pred_t595"),
                         labels=c("Up to 195", "Up to 395", "Up to 595")))
  
  return(df_pred)
}

```


```{r}
out_pred_subj1 <- out_pred_clean(1, mid) %>% 
  left_join(df_true_eta %>% filter(id==1), by = "mid")
out_pred_subj2 <- out_pred_clean(2, mid) %>% 
  left_join(df_true_eta %>% filter(id==2), by = "mid")
out_pred_subj3 <- out_pred_clean(3, mid) %>% 
  left_join(df_true_eta %>% filter(id==3), by = "mid")
out_pred_subj4 <- out_pred_clean(4, mid) %>% 
  left_join(df_true_eta %>% filter(id==4), by = "mid")
```


```{r, laplace_ped}
# clean MLE out-of-sample prediction results
out_pred_clean_laplace <- function(subjID=1, mid=mid){
  # Prediction
  pred_t195 <- out_pred_laplace(fpca_fit = fpca_fit, df_new = df %>% filter(id==subjID & sind_inx<=195) %>% select(-eta_i))
  pred_t395 <- out_pred_laplace(fpca_fit = fpca_fit, df_new = df %>% filter(id==subjID & sind_inx<=395) %>% select(-eta_i))
  pred_t595 <- out_pred_laplace(fpca_fit = fpca_fit, df_new = df %>% filter(id==subjID & sind_inx<=595) %>% select(-eta_i))
  
  # clean
  df_pred <- data.frame(mid, 
                        pred_t195=pred_t195$eta_pred,
                        pred_t395=pred_t395$eta_pred, 
                        pred_t595=pred_t595$eta_pred)
  df_pred[df_pred$mid<=195, "pred_t195"] <- NA
  df_pred[df_pred$mid<=395, "pred_t395"] <- NA
  df_pred[df_pred$mid<=595, "pred_t595"] <- NA
  
  df_pred <- df_pred %>% pivot_longer(starts_with("pred_t"), 
                                      names_to = "type", values_to = "eta") %>%
    mutate(type = factor(type, levels=c("pred_t195", "pred_t395", "pred_t595"),
                         labels=c("Up to 195", "Up to 395", "Up to 595")))
  
  return(df_pred)
}

```

```{r, results='hide', message=FALSE}
out_pred_lap_subj1 <- out_pred_clean_laplace(1, mid) %>% 
  left_join(df_true_eta %>% filter(id==1), by = "mid")
out_pred_lap_subj2 <- out_pred_clean_laplace(2, mid) %>% 
  left_join(df_true_eta %>% filter(id==2), by = "mid")
out_pred_lap_subj3 <- out_pred_clean_laplace(3, mid) %>% 
  left_join(df_true_eta %>% filter(id==3), by = "mid")
out_pred_lap_subj4 <- out_pred_clean_laplace(4, mid) %>% 
  left_join(df_true_eta %>% filter(id==4), by = "mid")
```


```{r, fig.width=12, fig.height=4}
# compare point prediction with in-sample
# subject 1
grid.arrange(
  out_pred_subj1 %>% 
    group_by("type") %>%
    ggplot()+
    geom_line(aes(x=mid, y=true), col = "black")+
    geom_line(aes(x=mid, y=eta, col= type, group = type), linetype="dashed", na.rm = T,
              show.legend = FALSE)+
    labs(x="Binned time", title = "Subject 1 out-of-sample MLE"),
  
  out_pred_lap_subj1 %>% 
    group_by("type") %>%
    ggplot()+
    geom_line(aes(x=mid, y=true), col = "black")+
    geom_line(aes(x=mid, y=eta, col= type, group = type), linetype="dashed", na.rm = T,
              show.legend = FALSE)+
    labs(x="Binned time", title = "Subject 1 out-of-sample Laplace"),
  
  in_pred_subj1 %>%
    ggplot()+
    geom_line(aes(x=mid, y=true), col="black")+
    geom_line(aes(x=mid, y=eta, col=type, group=type), linetype="dashed", na.rm = T,
              show.legend = FALSE)+
    labs(x="Binned time", title = "Subject 1 in-sample"),
  nrow=1)

# subject 2
grid.arrange(
  out_pred_subj2 %>% 
    group_by("type") %>%
    ggplot()+
    geom_line(aes(x=mid, y=true), col = "black")+
    geom_line(aes(x=mid, y=eta, col= type, group = type), linetype="dashed", na.rm = T,
              show.legend = FALSE)+
    labs(x="Binned time", title = "Subject 2 out-of-sample MLE"),
  
  out_pred_lap_subj2 %>% 
    group_by("type") %>%
    ggplot()+
    geom_line(aes(x=mid, y=true), col = "black")+
    geom_line(aes(x=mid, y=eta, col= type, group = type), linetype="dashed", na.rm = T,
              show.legend = FALSE)+
    labs(x="Binned time", title = "Subject 2 out-of-sample Laplace"),
  
  in_pred_subj2 %>%
    ggplot()+
    geom_line(aes(x=mid, y=true), col="black")+
    geom_line(aes(x=mid, y=eta, col=type, group=type), linetype="dashed", na.rm = T,
              show.legend = FALSE)+
    labs(x="Binned time", title = "Subject 2 in-sample"),
  nrow=1)

# subject 3
grid.arrange(
  out_pred_subj3 %>% 
    group_by("type") %>%
    ggplot()+
    geom_line(aes(x=mid, y=true), col = "black")+
    geom_line(aes(x=mid, y=eta, col= type, group = type), linetype="dashed", na.rm = T,
              show.legend = FALSE)+
    labs(x="Binned time", title = "Subject 3 out-of-sample MLE"),
  
  out_pred_lap_subj3 %>% 
    group_by("type") %>%
    ggplot()+
    geom_line(aes(x=mid, y=true), col = "black")+
    geom_line(aes(x=mid, y=eta, col= type, group = type), linetype="dashed", na.rm = T,
              show.legend = FALSE)+
    labs(x="Binned time", title = "Subject 3 out-of-sample Laplace"),
  
  in_pred_subj3 %>%
    ggplot()+
    geom_line(aes(x=mid, y=true), col="black")+
    geom_line(aes(x=mid, y=eta, col=type, group=type), linetype="dashed", na.rm = T,
              show.legend = FALSE)+
    labs(x="Binned time", title = "Subject 3 in-sample"),
  nrow=1)

# subject 4
grid.arrange(
  out_pred_subj4 %>% 
    group_by("type") %>%
    ggplot()+
    geom_line(aes(x=mid, y=true), col = "black")+
    geom_line(aes(x=mid, y=eta, col= type, group = type), linetype="dashed", na.rm = T,
              show.legend = FALSE)+
    labs(x="Binned time", title = "Subject 4 out-of-sample MLE"),
  
  out_pred_lap_subj4 %>% 
    group_by("type") %>%
    ggplot()+
    geom_line(aes(x=mid, y=true), col = "black")+
    geom_line(aes(x=mid, y=eta, col= type, group = type), linetype="dashed", na.rm = T,
              show.legend = FALSE)+
    labs(x="Binned time", title = "Subject 4 out-of-sample Laplace"),
  
  in_pred_subj4 %>%
    ggplot()+
    geom_line(aes(x=mid, y=true), col="black")+
    geom_line(aes(x=mid, y=eta, col=type, group=type), linetype="dashed", na.rm = T,
              show.legend = FALSE)+
    labs(x="Binned time", title = "Subject 4 in-sample"),
  nrow=1)

```



### In-/Out-of-sample PC scores

<span style="color: red;">Doesn't look biased? </span>
<span style="color: red;">Perhaps because the function value range is so small? </span>

```{r, results='hide', message=FALSE}
# true scores:xi

# scores
in_score <- matrix(NA, nrow=N, ncol=4)
out_score_MLE <- matrix(NA, nrow=N, ncol=4)
out_score_laplace <- matrix(NA, nrow=N, ncol=4)

for(i in 1:N){
  in_score[i, ] <- in_samp_dyn_pred(fpca_fit=fpca_fit, 
                   df_part=df_pred_unique %>% filter(id==i & bin<=595))$score
  out_score_MLE[i,] <- out_samp_dyn_pred(df %>% filter(id==i & sind_inx<=595) %>%
                                  select(-eta_i),
                                  fpca_fit=fpca_fit)$score_out
  out_score_laplace[i,] <- out_pred_laplace(fpca_fit=fpca_fit, 
                                            df_new = df %>% filter(id==i & sind_inx<=595) %>% select(-eta_i))$score_out
}

# bias
in_bias <- in_score-xi
out_bias_MLE <- out_score_MLE-xi
out_bias_laplace <- out_score_laplace-xi
```

```{r, fig.height=4, fig.width=8}
grid.arrange(
  out_bias_MLE %>% data.frame() %>%
    pivot_longer(1:4, names_to = "PC", values_to = "Bias") %>%
    ggplot()+
    geom_boxplot(aes(x=PC, y=Bias))+
    stat_summary(aes(x=PC, y=Bias), fun = mean, geom = "point", col = "red")+
    labs(title = "Out-of-sample MLE"),
  
  out_bias_laplace %>% data.frame() %>%
    pivot_longer(1:4, names_to = "PC", values_to = "Bias") %>%
    ggplot()+
    geom_boxplot(aes(x=PC, y=Bias))+
    stat_summary(aes(x=PC, y=Bias), fun = mean, geom = "point", col = "red")+
    labs(title = "Out-of-sample laplace"),
  
  in_bias %>% data.frame() %>%
    pivot_longer(1:4, names_to = "PC", values_to = "Bias") %>%
    ggplot()+
    geom_boxplot(aes(x=PC, y=Bias))+
    stat_summary(aes(x=PC, y=Bias), fun = mean, geom = "point", col = "red")+
    labs(title = "In-sample"), 
nrow=1)

```

### Prediction interval

With MLE of scores $\hat{\boldsymbol{\xi}}$

$$\boldsymbol{\hat{\eta}}=\boldsymbol{\hat{f_0}}+\boldsymbol{\Phi\hat{\xi}}$$
$$\begin{aligned}
\hat{\boldsymbol{\eta}}-\boldsymbol{\eta}&=\boldsymbol{\hat{f_0}}+\boldsymbol{\Phi\hat{\xi}}-\boldsymbol{\eta}\\
&= \boldsymbol{\hat{f_0}-f_0}+\boldsymbol{\Phi(\hat{\xi}-\xi)}-\boldsymbol{\epsilon}
\end{aligned}$$

$$Var(\hat{\boldsymbol{\eta}}-\boldsymbol{\eta})=Var(\boldsymbol{\hat{f_0}-f_0})+\boldsymbol{\Phi Var(\hat{\xi}-\xi) \Phi^T}+Var(\epsilon)$$

- The three components should be mutually independent? 
- Through some derivation, I think $Var(\boldsymbol{f_0})=\frac{Var(\boldsymbol{Y})}{N} = \frac{1}{N}[\boldsymbol{\Phi\Gamma \Phi^T}+\sigma_{\epsilon}^2\boldsymbol{I}_m]$.
- Variance of $\hat{\boldsymbol{\xi}}$ will depend on the method used for its estimation. But isn't the estimated done conditioning on $Var(\boldsymbol{\xi}) = \boldsymbol{\Gamma}$?ÃŸ
- $Var(\epsilon) =\hat{\sigma}_{\epsilon}^2$ from FPCA
- Very different approach from in-sample estimation

Additional questions:

Variance of $\boldsymbol{\xi}$ and random error is in fact given from the FPCA procedure. But we need to variance of mean function for the actual prediction interval.

What happens to the variance of mean function? In practice it is estimated by smoothed population mean at each observation time. It looks like in existing derivations, outcomes are usually assumed to be centered.


# Next steps:

- Interval prediction for MLE
- FPCA eigenfunctions: how to extend to the original grid? Interpolation or projection?


