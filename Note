
# WNAR paper

-	Repeat simulation: repeat 10-100 times

-	Set-up:
a.	Different eigenfunctions: periodicity
b.	Outcome: binary
c.	Sample size: start with N=500
d.	Grid density: start with J=1000
e.	Bin width, overlap or not: non-overlap, bin width = 10 (100 bins)

# 24/6/25
1. Stress the target data is "high-dimensional dense repeated measures"

2. Set up a motivation example (like fig.3 in the FCR paper)
- add a section in the manuscript
- also format the paper in statistics in medicine format

3. Develop a prediction interval (SD of scores)

- There are a few ways this can be down. We can estimate the score and calculate a wald interval. But I think we probably shouldn't compare the converage to other methods. 
- I am not sure if we need to add in the variantion of mean function, since the variation of scores is estimated conditional on the mean
- Also, I am not sure if we should compare the CI as well
- Another problem: variance of scores seemed to decrease in dramatice scales as more data is added in. Is that suppose to happen? 
- I should look into the intervals of other methods


4. DynPred package? function? 
- Should I priotize this over prediction interval? 

5. Format in statistics in medicine/biometrics format


# 24/7/9
1. Motivation example: add a panel title and (y) axis labels for observed vs predicted
- How to do the split facet title? 

2. Prediction interval:

1) Try to explore the coverage interval
- For LaplaceApproximation, it seems that either probablity interval or standard deviation change dramatically with the estimation (of Hessian matrix) method used. Therefore, I think I should probably turn to stan. 
- what is the different between standard deviation (SD) and monte carlo standard error (mean_se)?
- Results from stan seemed much more resonable, and stable as I change the algorithm/update observed track
- The standard deviation for sure decreased as more data is collected before it
- 

# 24/7/16

1. How do we report the prediction interval? 
- Coverage probablity curve? (not doable for data application)
- Either hard to compare across methods or had to see visually
- For GOFSR, the prediction interval is very very narrow and pretty much never covers the true latent function. Hw does BAM severely underestimate the standard deviation? 
- For GLMMadaptive, a quaratic time will take more than 10 minutes. Is it worth it to do that? 

2. After using stan, it seems that ISE increased compared to LaplaceDemon, though still better than the other methods?
- What is the cause of lower accuracy? Could it have to do with the warnings on small bulk due to sample size? 

3. Prediction time increased if we want the standard deviation estimates
- about 3 minutes

2. Draft out to the committee
