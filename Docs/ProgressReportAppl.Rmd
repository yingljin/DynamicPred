---
title: "Progress Report on NHANES Data"
author: "Ying Jin"
date: "2023-10-03"
output: 
  html_document:
    self_contained: no
    number_sections: true
    code_folding: hide
    toc: true
    toc_depth: 3
    toc_float: true
    font: 12pt
    keep_tex: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

set.seed(121)

library(here)
library(tidyverse)
library(ggpubr)
library(refund)
library(lme4)
library(ROCR)
library(GLMMadaptive)
library(kableExtra)
library(mvtnorm)
library(mgcv)
library(splines)
library(LaplacesDemon)
theme_set(theme_minimal())
```


```{r}
# load data
df <- read_rds(here("Data/nhanes_bi.rds"))

# code
source(here("Code/GLMM-FPCA.R")) 
# use pred_latent function to estimate latent function 
K <- 4
source(here("Code/OutsampBayes.R"))
```


# Data overview

- 8763 subjects, 1440 measures each
- no missingness

```{r}
N <- length(unique(df$SEQN)) # sample size 8763
J <- max(df$sind) # 1440 measures per subject
```


```{r}
rand_id <- sample(unique(df$SEQN), size = 4) # "toy" sample

df %>% 
  filter(SEQN %in% rand_id) %>%
  ggplot()+
  geom_point(aes(x=sind, y=Z), size = 0.5)+
  facet_wrap(~SEQN)+
  labs(x="Time", y = "Activity", title = "A brief overview of the outcome")
```

# Fit fGFPCA model

## Bin data

- Bin every 10 observations

```{r, message=FALSE}
# bin data
bin_w <- 10 # bin width
n_bin <- J/bin_w # number of bins
brks <- seq(0, J, by = bin_w) # cutoff points
mid <- (brks+bin_w/2)[1:n_bin] # mid points

df$bin <- cut(df$sind, breaks = brks, include.lowest = T, labels = mid)
df$bin <- as.numeric(as.character(df$bin))
# unique(df$bin)

df %>% 
  filter(SEQN %in% rand_id) %>%
  group_by(SEQN, bin) %>%
  summarise(num = sum(Z)) %>%
  ggplot()+
  geom_point(aes(x=bin, y=num), size = 0.5)+
  facet_wrap(~SEQN)+
  labs(x="Time", y = "Activity", title = "Number of active nimutes within each bin")
```
## Data split

- Original sample size took too much time in the debias step of fGFPCA method. 
- In fact, the debais step does not work very well in this case
- 500 subjects for training, 500 for out-of-sample prediction

```{r}
# head(df)
df <- df %>% rename(id=SEQN, Y=Z)

Ntr <- Nte <- 500

train_id <- unique(df$id)[1:500]
test_id <- unique(df$id)[501:1000]

train_df <- df %>% filter(id %in% train_id)
test_df <- df %>% filter(id %in% test_id)
# 1440*500
```


## Local GLMM

- Used glmer with PIRLS (nAGQ=0) for local GLMMs step
- Fit on the training set

```{r, cache=TRUE}
# fit model on the training set
train_bin_lst <- split(train_df, f = train_df$bin)

# local GLMM and estimate latent function
# use PIRLS (nAGQ=0) to avoid near-unidentifiability issues 
t1=Sys.time()
df_est_latent <- lapply(train_bin_lst, function(x){pred_latent(x, n_node = 0)}) 
t2= Sys.time()
t_local_glmm <- t2-t1 
```


```{r}
df_est_latent <- bind_rows(df_est_latent) 
# head(df_est_latent)

# example estimated latent function
rand_id <- sample(train_id, 4)

df_est_latent %>% 
  filter(id %in% rand_id) %>%
  mutate(eta_hat = exp(eta_hat)/(1+exp(eta_hat))) %>%
  ggplot()+
  geom_line(aes(x=sind, y=eta_hat, group = id, col = "estimated"))+
  geom_point(aes(x=sind, y = Y, group = id, col = "outcome"), size = 0.5)+
  facet_wrap(~id, scales = "free")+
  labs(x = "Time", y = "Estimated latent function (probablity scale)")
```

## FPCA

```{r}
uni_eta_hat <- df_est_latent %>% filter(bin==sind)
# head(uni_eta_hat)

mat_est_unique <- matrix(uni_eta_hat$eta_hat,
                         nrow=Ntr, 
                         ncol=n_bin, byrow = F) 
# row index subject, column binned time
# dim(mat_est_unique)

t1 <- Sys.time()
fpca_mod <- fpca.face(mat_est_unique, argvals = mid, var=T)
t2 <- Sys.time()
t_fpca <- t2-t1 # 3.21 seconds to fit fPCA model

# dim(fpca_mod$efunctions) # 27 eigenfunctions
K <- 4
```


```{r PC}
data.frame(t = mid, fpca_mod$efunctions[, 1:4]) %>%
  rename(PC1 = X1, PC2=X2, PC3=X3,PX4=X4) %>%
  pivot_longer(2:5, names_to = "PC") %>%
  ggplot()+
  geom_line(aes(x=t, y=value, col = PC))+
  labs(x="Time", y="", title = "Eigenfunctions on the binned grid")
```



```{r, eval=FALSE}
# plot correlation matrix
heatmap(cov2cor(fpca_mod$VarMats[[1]]), Rowv = NA, Colv = NA, main = "Correlation")
```

## Debias (step 4)

According to the fGFPCA paper, in this step we do two things: 

1. Project the eigenfunctions with B-spline basis back to the original grid
2. Debias the eigenvalues with GLMM (mgcv::bam, mgcm::gamm or gamm4:gamm) to be used for Laplace approximation

```{r projection}
# Re-evaluation
## grid extension
p <- 3 # order of b splines 
knots <- 35 # number of knots (same from FPCA model)
knots_values <- seq(-p, knots + p, length = knots + 1 + 2 *p)/knots
knots_values <- knots_values * (max(mid) - min(mid)) + min(mid)

B <- spline.des(knots = knots_values, x = mid, ord = p + 1,
                outer.ok = TRUE)$design  # evaluate B-splines on binned grid
Bnew <- spline.des(knots = knots_values, x = 1:J, ord = p + 1,
                   outer.ok = TRUE)$design  # evaluate B-splines on original grid


df_phi <- matrix(NA, J, K) 
for(k in 1:K){
  lm_mod <- lm(fpca_mod$efunctions[,k] ~ B-1)
  df_phi[,k] <- Bnew %*% coef(lm_mod)
}# project binned eigenfunctions onto the original grid
```




```{r}
df_pc1 <- data.frame(t=1:J,  df_phi) %>%
  rename(PC1=X1, PC2=X2, PC3=X3, PC4=X4) %>%
  pivot_longer(2:5, names_to = "PC") 

df_pc2 <- data.frame(t = mid, fpca_mod$efunctions[, 1:4]) %>%
  rename(PC1=X1, PC2=X2, PC3=X3, PC4=X4) %>%
  pivot_longer(2:5, names_to = "PC") 

left_join(df_pc1, df_pc2, by = c("t", "PC")) %>%
  ggplot()+
  geom_line(aes(x=t, y=value.x, col = PC), linewidth = 0.5)+
  geom_point(aes(x=t, y=value.y, col = PC), na.rm = T, alpha = 0.5, size = 0.5)+
  labs(x="Time", y="", title = "Re-evaluate eigenfunctions on the original grid")

```




```{r revaluation}
df_phi <- data.frame(sind = 1:J, df_phi)
colnames(df_phi) <- c("sind", paste0("phi", 1:K))
train_df <- train_df %>% 
  left_join(df_phi, by = "sind")
train_df$id <- as.factor(train_df$id)
# class(train_df$id)
# train_df$id2 <- factor(train_df$id, levels = train_id, labels = 1:length(train_id))
# train_df$id2 <- as.numeric(train_df$id2)
# range(unique(train_df$id2))

t1 <- Sys.time()
debias_glmm <- bam(Y ~ s(sind, bs="cc")+
                     s(id, by=phi1, bs="re")+
                     s(id, by=phi2, bs="re")+
                     s(id, by=phi3, bs="re")+
                     s(id, by=phi4, bs="re"), 
                   family = binomial, 
                   data=train_df, 
                   method = "fREML",
                   discrete = TRUE)
t2 <- Sys.time()
t2-t1
```


```{r}
# mean
new_mu <- predict(debias_glmm, type = "terms")[1:J, 1] # extract re-evaluated mean

plot(mid, fpca_mod$mu, pch=16, xlab="bin", ylab="PC4", col = "blue",
     cex = 0.5)
lines(1:J, new_mu)

# eigenvalues
fpca_mod$evalues[1:K] # original
lambda_scaled <- 1/debias_glmm$sp[2:5] # extract re-evaluated lambda
fpca_mod$evalues[1:K]/lambda_scaled # ratio
```

```{r rescale}
## rescale
new_phi <- df_phi %>% select(starts_with("phi"))*sqrt(n_bin)
new_phi <- as.matrix(new_phi)

lambda_scaled <- lambda_scaled/n_bin
```

# Out-of-sample prediction


$$
l_u=\sum_{t_{m_s}<T_u}log(h(Y_u^s))+\hat{\eta}_u(t_{m_s})T(Y_u^s)-log(A[\hat{\eta}_u(t_{m_s})])
$$


where $\hat{\eta}_u(t_{m_s}) = \hat{\mu}_0(t_{m_s})+\sum_{k=1}^K \xi_{uk}\hat{\phi}(t_{m_s})$. 

From FPCA model (or the debias step to be added), we obtain $\hat{\mu}_0$, $\hat{\phi}_k$, as well as the variance estimates of $\xi_k$: $\hat{\lambda}_k$, and variance of residual process $\hat{\sigma^2}$.

With all these estimates, we wanna use Laplace approximation to find the $\xi_{uk}$ that maximizes $l_u$. 


I am using a package called **LaplacesDemon**, following this example https://cran.r-project.org/web/packages/LaplacesDemon/vignettes/LaplacesDemonTutorial.pdf

```{r, class.source = "fold-show"}
library(LaplacesDemon)
```


First we write out the model: 

- Prior distribution: $\xi_{uk} \sim N(0, \hat{\lambda}_k)$
- Posterior distribution: 


\begin{aligned}
l(\mathbf{Y_u}|\mathbf{\xi}_u) &= \sum_{t_{m_s}<T_u}l(Y_u^s|\mathbf{\xi}_u) \\

Y_u^s|\mathbf{\xi}_u & \sim Binomial(n_s, p_s) \\

g^{-1}(p_s) = \hat{\eta}(t_{m_s}) &= \hat{\mu}_0(t_{m_s})+\sum_{k=1}^K \xi_{uk}\hat{\phi}(t_{m_s})
\end{aligned}


And $n_s$ is the number of observations in bin s. 

Bayes theroem:

$$
l(\mathbf{\xi}_u|\mathbf{Y}_u) \propto l(\mathbf{Y}_u|\mathbf{\xi}_u)+l(\mathbf{\xi}_u)
$$


- Then we feed in data for one observation

```{r}
window <- seq(0, J, by = 8*60) # midnight,  8am, 4pm, midnight

test_df <- rename(test_df, t = sind)

converge_state_m <- matrix(NA, nrow = length(test_id), 2)
pred_list_m <- list()

t1 <- Sys.time()
for(i in seq_along(test_id)){
  df_i <- test_df %>% filter(id==test_id[i])
  # per max obs time
  for(tmax in window[2:3]){
    df_it <- df_i %>% filter(t <= tmax)
    max_rid <- nrow(df_it)
    
    # into a list
    MyData <- list(K=K, 
                   X=new_phi[1:max_rid, ], 
                   mon.names=mon.names,
                   parm.names=parm.names, 
                   pos.xi=pos.xi, 
                   y=df_it$Y, 
                   tao=diag(lambda_scaled), f0=new_mu[1:max_rid])
    
    
    # fit laplace approximation
    Fit <- LaplaceApproximation(Model, 
                                parm = rep(0, K), 
                                Data=MyData, 
                                Method = "NM", 
                                Iterations = 1000,
                                CovEst = "Identity")
    converge_state_m[i, which(window[2:3]==tmax)] <- Fit$Converged
    score <- Fit$Summary1[, "Mode"]
    
    # prediction
    eta_pred_out <- new_mu+new_phi%*%score
    df_i[ , paste0("pred", tmax)] <- eta_pred_out[,1]
  }
  
  pred_list_m[[i]] <- df_i
}
t2 <- Sys.time()
t_pred <- t2-t1 # About 3.5 minutes

# mean(converge_state_m) # convergence rate 100%
```

```{r}
# check results
df_pred <- bind_rows(pred_list_m)
df_pred$pred480[df_pred$t<480] <- NA
df_pred$pred960[df_pred$t<960] <- NA
```

# GLMMadaptvie

- when fitting model with **random intercept + slope** on the same full training set (even after scaling covariates): Error: vector memory exhausted (limit reached?).
- when fitting model with **only random intercept** on the same full training set: Error regarding a large coefficient value. One fix is to re-scale covariates. So I re-scaled minute index by dividing them by J, so the range is within (0, 1], and tried again. This fitting procedure took 20.72 minutes to finish


```{r}
t1 <- Sys.time()
adglmm_mod <- mixed_model(Y ~ t, random = ~ t | id, 
                          data = train_df %>% mutate(t=sind/J),
                      family = binomial())
t2 <- Sys.time()
t_est_adglmm <- t2-t1 # model fitting took 20.82 mins
summary(adglmm_mod)
```


```{r}
test_df2 <- test_df %>% mutate(t=t/J)
pred_list_m2 <- list(
  predict(adglmm_mod, newdata = test_df2 %>% filter(t <= 480/J),
          newdata2 =  test_df2 %>% filter(t>480/J), 
          type = "subject_specific", type_pred = "link",
          se.fit = FALSE, return_newdata = TRUE)$newdata2,
  
  predict(adglmm_mod, newdata = test_df2 %>% filter(t <= 960/J),
          newdata2 =  test_df2 %>% filter(t>960/J), 
          type = "subject_specific", type_pred = "link",
          se.fit = FALSE, return_newdata = TRUE)$newdata2
)
```


```{r}
# check results
pred_list_m2 <- lapply(pred_list_m2, function(x){x %>% select(id, t, pred)})
df_pred2 <- test_df2 %>% 
  left_join(pred_list_m2[[1]], by = c("id", "t")) %>% 
  rename(pred480 = pred) %>% 
  left_join(pred_list_m2[[2]], by = c("id", "t")) %>% 
  rename(pred960 = pred) %>%
  mutate(t=t*J)
```

# Results

```{r}
rand_id_test <- sample(unique(test_id), 4)
ggarrange(
  df_pred %>% 
    filter(id %in% rand_id_test) %>%
    mutate_at(vars(starts_with("pred")), function(x)exp(x)/(1+exp(x))) %>% 
    ggplot()+
    geom_line(aes(x=t, y = pred480, col = "8am"), linetype = "dashed", na.rm = T)+
    geom_line(aes(x=t, y = pred960, col = "4pm"),linetype = "dashed", na.rm = T)+
    geom_point(aes(x=t, y = Y, col = "Outcome"), size = 0.2)+
    facet_wrap(~id)+
    labs(x = "Time", y = "fGFPCA"),
  
  df_pred2 %>% 
    filter(id %in% rand_id_test) %>%
    mutate_at(vars(starts_with("pred")), function(x)exp(x)/(1+exp(x))) %>% 
    ggplot()+
    geom_line(aes(x=t, y = pred480, col = "8am"), linetype = "dashed", na.rm = T)+
    geom_line(aes(x=t, y = pred960, col = "4pm"),linetype = "dashed", na.rm = T)+
    geom_point(aes(x=t, y = Y, col = "Outcome"), size = 0.2)+
    facet_wrap(~id)+
    labs(x = "Time", y = "GLMMadaptive"),
  nrow = 1, common.legend = T)

```

```{r auc_func}
## a function to calculate AUC
get_auc <- function(y, pred){
  if(sum(is.na(y))>0 | sum(is.na(pred))>0){
    auc <- NA
  }
  else{
    this_perf <- performance(prediction(pred, y), measure = "auc")
    auc <- this_perf@y.values[[1]]
  }
  return(auc)
}
```

```{r auc_fGFPCA_nhanes}
auc_mat1 <- df_pred %>%
  mutate(window = cut(t, breaks = window, include.lowest = T)) %>% 
  select(Y, starts_with("pred"), window) %>%
  group_by(window) %>%
  summarise(auc1 = get_auc(Y, pred480),
            auc2 = get_auc(Y, pred960)) %>%
  filter(window != "[0,480]") %>% 
  select(starts_with("auc"))

colnames(auc_mat1) <- c("0-8am", "0am-4pm")
```

```{r auc_GLMMadaptive_nhanes}
auc_mat2 <- df_pred2 %>%
  mutate(window = cut(t, breaks = window, include.lowest = T)) %>% 
  select(Y, starts_with("pred"), window) %>%
  group_by(window) %>%
  summarise(auc1 = get_auc(Y, pred480),
            auc2 = get_auc(Y, pred960)) %>%
  filter(window != "[0,480]") %>% 
  select(starts_with("auc"))

colnames(auc_mat2) <- c("0-8am", "0am-4pm")
```

```{r}
data.frame(Window = c("8am-4pm", "4am-12pm"),
           auc_mat1, auc_mat2, check.names = F) %>%
  kable(digits = 3, table.attr = "style = \"color: black;\"", caption = "Area Under the ROC curve") %>%
  kable_styling(full_width = F) %>% 
  add_header_above(c(" "=1, "fGFPCA" = 2, "GLMMadaptive" = 2)) %>%
  add_header_above(c(" "= 1, "Observed track" = 4))
```


```{r}
nhanes_pred_fgfpca <- df_pred
nhanes_pred_adglmm <- df_pred2
save(nhanes_pred_fgfpca, nhanes_pred_adglmm,
     file = here("Data/ApplOutput.RData"))
```