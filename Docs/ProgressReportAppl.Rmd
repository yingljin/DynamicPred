---
title: "Progress Report on NHANES Data"
author: "Ying Jin"
date: "2023-10-03"
output: 
  html_document:
    self_contained: no
    number_sections: true
    code_folding: hide
    toc: true
    toc_depth: 3
    toc_float: true
    font: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

set.seed(516)

library(here)
library(tidyverse)
library(ggpubr)
library(refund)
library(lme4)
library(ROCR)
library(GLMMadaptive)
library(kableExtra)
library(mvtnorm)
library(mgcv)
library(splines)
theme_set(theme_minimal())
```


```{r}
# load data
df <- read_rds(here("Data/nhanes_bi.rds"))

# code
source(here("Code/GLMM-FPCA.R")) 
# use pred_latent function to estimate latent function 
source(here("Code/OutSampMLE.R"))
# source(here("Code/OutsampBayes.R"))
```


# Data overview

- 8763 subjects, 1440 measures each
- no missingness

```{r}
N <- length(unique(df$SEQN)) # sample size 8763
J <- max(df$sind) # 1440 measures per subject
```


```{r}
rand_id <- sample(unique(df$SEQN), size = 4) # "toy" sample

df %>% 
  filter(SEQN %in% rand_id) %>%
  ggplot()+
  geom_point(aes(x=sind, y=Z), size = 0.5)+
  facet_wrap(~SEQN)+
  labs(x="Time", y = "Activity", title = "A brief overview of the outcome")
```

# Fit fGFPCA model

## Bin data

- Bin every 10 observations

```{r, message=FALSE}
# bin data
bin_w <- 10 # bin width
n_bin <- J/bin_w # number of bins
brks <- seq(0, J, by = bin_w) # cutoff points
mid <- (brks+bin_w/2)[1:n_bin] # mid points

df$bin <- cut(df$sind, breaks = brks, include.lowest = T, labels = mid)
df$bin <- as.numeric(as.character(df$bin))
# unique(df$bin)

df %>% 
  filter(SEQN %in% rand_id) %>%
  group_by(SEQN, bin) %>%
  summarise(num = sum(Z)) %>%
  ggplot()+
  geom_point(aes(x=bin, y=num), size = 0.5)+
  facet_wrap(~SEQN)+
  labs(x="Time", y = "Activity", title = "Number of active nimutes within each bin")
```
## Data split

- 80% (7010) subjects for training, 20% (1753) for out-of-sample prediction

```{r}
# head(df)
df <- df %>% rename(id=SEQN, Y=Z)

train_id <- sample(unique(df$id), size = N*0.8)
test_id <- setdiff(unique(df$id), train_id)

train_df <- df %>% filter(id %in% train_id)
test_df <- df %>% filter(id %in% test_id)
```


## Local GLMM

- Used glmer with PIRLS (nAGQ=0) for local GLMMs step
- Fit on the training set

```{r, cache=TRUE}
# fit model on the training set
train_bin_lst <- split(train_df, f = train_df$bin)

# local GLMM and estimate latent function
# use PIRLS (nAGQ=0) to avoid near-unidentifiability issues 
t1=Sys.time()
df_est_latent <- lapply(train_bin_lst, function(x){pred_latent(x, n_node = 0)}) 
t2= Sys.time()
t_local_glmm <- t2-t1 
```


```{r}
df_est_latent <- bind_rows(df_est_latent) 
# head(df_est_latent)

# example estimated latent function
rand_id <- sample(train_id, 4)

df_est_latent %>% 
  filter(id %in% rand_id) %>%
  mutate(eta_hat = exp(eta_hat)/(1+exp(eta_hat))) %>%
  ggplot()+
  geom_line(aes(x=sind, y=eta_hat, group = id, col = "estimated"))+
  geom_point(aes(x=sind, y = Y, group = id, col = "outcome"), size = 0.5)+
  facet_wrap(~id, scales = "free")+
  labs(x = "Time", y = "Estimated latent function (probablity scale)")
```

## FPCA

```{r}
uni_eta_hat <- df_est_latent %>% filter(bin==sind)
# head(uni_eta_hat)

mat_est_unique <- matrix(uni_eta_hat$eta_hat,
                         nrow=length(train_id), 
                         ncol=n_bin, byrow = F) 
# row index subject, column binned time
# dim(mat_est_unique)

t1 <- Sys.time()
fpca_mod <- fpca.face(mat_est_unique, argvals = mid, var=T)
t2 <- Sys.time()
t_fpca <- t2-t1 # 3.21 seconds to fit fPCA model

# dim(fpca_mod$efunctions) # 27 eigenfunctions
K <- 4
```

```{r}
plot(mid, fpca_mod$mu, type = "l", xlab = "bin", ylab = "mean")
```


```{r, fig.height=8, fig.width=8}
par(mfrow=c(2,2))

plot(mid, fpca_mod$efunctions[, 1], type="l", xlab="bin", ylab="PC1")
plot(mid, fpca_mod$efunctions[, 2], type="l", xlab="bin", ylab="PC2")
plot(mid, fpca_mod$efunctions[, 3], type="l", xlab="bin", ylab="PC3")
plot(mid, fpca_mod$efunctions[, 4], type="l", xlab="bin", ylab="PC4")
```

```{r}
# plot correlation matrix
heatmap(cov2cor(fpca_mod$VarMats[[1]]), Rowv = NA, Colv = NA, main = "Correlation")
```

## Debias (step 4)

According to the fGFPCA paper, in this step we do two things: 

1. Project the eigenfunctions with B-spline basis back to the original grid
2. Debias the eigenvalues with GLMM (mgcv::bam, mgcm::gamm or gamm4:gamm) to be used for Laplace approximation

Becuase I would like to focus this report on subjects with failed Laplace approximation, I will not do these two things for now. 

# Out-of-sample prediction

follow up on the last report, I'll add some details here.

Let's set: 

- $s$: bin index
- $t_{m_s}$: midpoint time of bin s
- $\mathscr{T}_s$: all observation points in bin s
- $Y_i^s$ all observations in bin s from subject i. $Y_i^S := \{Y_i(t_j), t_j \in \mathscr{T}_s\}$

Let's assume we have a new subject $u$ with maximum observation time $T_u$. Then the log-likelihood of this new subject would be:

$$
l_u=\sum_{t_{m_s}<T_u}log(h(Y_u^s))+\hat{\eta}_u(t_{m_s})T(Y_u^s)-log(A[\hat{\eta}_u(t_{m_s})])
$$


where $\hat{\eta}_u(t_{m_s}) = \hat{\mu}_0(t_{m_s})+\sum_{k=1}^K \xi_{uk}\hat{\phi}(t_{m_s})$. 

From FPCA model (or the debias step to be added), we obtain $\hat{\mu}_0$, $\hat{\phi}_k$, as well as the variance estimates of $\xi_k$: $\hat{\lambda}_k$, and variance of residual process $\hat{\sigma^2}$.

With all these estimates, we wanna use Laplace approximation to find the $\xi_{uk}$ that maximizes $l_u$. 


I am using a package called **LaplacesDemon**, following this example https://cran.r-project.org/web/packages/LaplacesDemon/vignettes/LaplacesDemonTutorial.pdf

```{r, class.source = "fold-show"}
library(LaplacesDemon)
```


First we write out the model: 

- Prior distribution: $\xi_{uk} \sim N(0, \hat{\lambda}_k)$
- Posterior distribution: 


\begin{aligned}
l(\mathbf{Y_u}|\mathbf{\xi}_u) &= \sum_{t_{m_s}<T_u}l(Y_u^s|\mathbf{\xi}_u) \\

Y_u^s|\mathbf{\xi}_u & \sim Binomial(n_s, p_s) \\

g^{-1}(p_s) = \hat{\eta}(t_{m_s}) &= \hat{\mu}_0(t_{m_s})+\sum_{k=1}^K \xi_{uk}\hat{\phi}(t_{m_s})
\end{aligned}


And $n_s$ is the number of observations in bin s. 

Bayes theroem:

$$
l(\mathbf{\xi}_u|\mathbf{Y}_u) \propto l(\mathbf{Y}_u|\mathbf{\xi}_u)+l(\mathbf{\xi}_u)
$$


## One subject example

Now I am going to use one subject as an example to go over the approximation procedure:
```{r, class.source = "fold-show"}
## model
Model <- function(parm, Data){
  xi <- parm[Data$pos.xi]
  
  # log-prior
  xi.prior <- dmvnorm(xi, mean = rep(0, Data$J), sigma=Data$tao, log = TRUE)
  
  # log-posterior likelihood
  eta <- Data$f0+Data$X %*% xi
  p <- exp(eta)/(1+exp(eta))
  LL <- sum(dbinom(x=Data$y, size = Data$n, prob=p, log = TRUE)) # log likelihood of Y|xi
  LP <- LL+sum(xi.prior) # unnormalized joint log likelihood of (Y, xi)
  
  # output
  Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP,
                   yhat=Data$y, parm=parm)
  return(Modelout)
}
```


- Then we feed in data for one observation

```{r, class.source = "fold-show"}
rand_id <- test_id[15]
df_new <- df %>% filter(id == rand_id & sind <= 540) # observation until 9am

# put data into correct format
ns <- as.vector(table(df_new$bin)) # number of observations
hs <- df_new %>% group_by(bin) %>% summarize_at("Y", sum) %>% select(Y) %>% unlist()# number of success
nf <- ns-hs # number of failure
max_bin <- length(unique(df_new$bin)) # assume new skipped bins  
df_new2 <- data.frame(bin = unique(df_new$bin), ns, hs, nf)  
  
# extract estimates from FPCA model to be used in prior distribution
tao <- diag(fpca_mod$evalues[1:K]) # variance of xi

# extract estimates from FPCA model to be used in posterior distribution
f0 <- fpca_mod$mu[1:max_bin]
N <- nrow(df_new2) # "sample size" which is in fact number of observed bins in this case for a new subject
n <- df_new2$ns # number of experiements at each bin
y <- df_new2$hs # outcome, number of 1
X <- fpca_mod$efunctions[1:max_bin, 1:K] # eigen/PC functions
J <- K # number of parameters/scores
mon.names <- "LP"

# parameter names
name_lst <- as.list(rep(0, J))
names(name_lst) <- paste("xi", 1:J, sep = "")
parm.names <- as.parm.names(name_lst) # names of parameters to estimate
pos.xi <- grep("xi", parm.names)

# data
MyData <- list(J=J, X=X, mon.names=mon.names,
                 parm.names=parm.names, pos.xi=pos.xi, y=y, n=n, tao=tao, f0=f0)
```


```{r, class.source = "fold-show"}
# fit laplace approximation
# initial values generated by GIV function in the same package
Fit <- LaplaceApproximation(Model, Data=MyData)
```

- Summarizing output

```{r, class.source = "fold-show", fig.height=8, fig.width=8}
# summarizing output
print(Fit)
plot(Fit, MyData)
```

```{r}
# prediction
eta_pred_out <- fpca_mod$mu+fpca_mod$efunctions[, 1:K]%*%Fit$Summary1[, "Mode"]

data.frame(bin = mid, eta_pred = eta_pred_out) %>% 
  right_join(df %>% filter(id==rand_id) %>%  select(Y, sind, bin), by = "bin") %>%
  ggplot()+
  geom_point(aes(x=sind, y = Y))+
  geom_line(aes(x=sind, y = exp(eta_pred)/(1+exp(eta_pred))))+
  labs(title = rand_id)
  
```



## Full data application output

### fGFPCA


```{r}
# results generated from DataApplNHANES.R
load(here("Data/ApplOutput_fGFPCA.RData"))
```


```{r message=FALSE, warning=FALSE}
df_test_full <- df %>% 
  filter(id %in% test_id) %>%
  # filter(!id %in% skip_id) %>%
  left_join(df_test %>% select(id, bin, pred_t540, pred_t780, pred_t1020), by = c("id", "bin"))

rand_id2 <- sample(unique(df_test$id), 4)
# without interpolation, assume constant latent function value in each bin
df_test_full%>%
  filter(id %in% rand_id2) %>%
  mutate_at(vars(pred_t540, pred_t780, pred_t1020), function(x)exp(x)/(1+exp(x))) %>%
  ggplot()+
    geom_line(aes(x=bin, y = pred_t540, col = "9am"))+
    geom_line(aes(x=bin, y = pred_t780, col = "1pm"))+
    geom_line(aes(x=bin, y = pred_t1020, col = "5pm"))+
    geom_point(aes(x=bin, y = Y, col = "Outcome"), size = 0.2)+
    facet_wrap(~id)+
    labs(x = "Time", y = "Estimated latent function (probablity scale)",
         title = "Constant latent function within bin")
```


<!-- Calculate AUC -->


```{r}
# break by prediction window
lst_auc <- df_test_full %>% 
  mutate(window = cut(sind, breaks = c(0, 540, 780, 1020, 1260, 1440), 
                      labels = c("0-9am", "9am-1pm", "1pm-5pm", "5pm-9pm","9pm-12pm"), 
                      include.lowest = T))
lst_auc <- split(lst_auc, f=lst_auc$window)
```


```{r}
# no interpolation
# up to 9am
auc_t540 <- lst_auc[2:4] %>% 
  lapply(function(x)performance(prediction(x$pred_t540, x$Y), measure = "auc"))
auc_t540 <- lapply(auc_t540, function(x){x@y.values[[1]]}) %>% unlist()

# up to 1pm
auc_t780 <- lst_auc[3:4] %>% 
  lapply(function(x)performance(prediction(x$pred_t780, x$Y), measure = "auc"))
auc_t780 <- lapply(auc_t780, function(x){x@y.values[[1]]}) %>% unlist()

# up to 5pm
auc_t1020 <- lst_auc[4] %>% 
  lapply(function(x)performance(prediction(x$pred_t1020, x$Y), measure = "auc"))
auc_t1020 <- lapply(auc_t1020, function(x){x@y.values[[1]]}) %>% unlist()
```




## GLMMadaptvie

- when fitting model with **random intercept + slope** on the same full training set (even after scaling covariates): Error: vector memory exhausted (limit reached?).
- when fitting model with **only random intercept** on the same full training set: Error regarding a large coefficient value. One fix is to re-scale covariates. So I re-scaled minute index by dividing them by J, so the range is within (0, 1], and tried again. This fitting procedure took 20.72 minutes to finish



```{r}
load(here("Data/ApplOutput_GLMMadaptive.RData"))
```

- `r round(t_est_adglmm, 2)` minutes on model fitting and `r round(t_pred_adglmm, 2)` minutes on out-of-sample prediction. 

- Below is the model fit on the training set

```{r}
summary(adglmm_mod)
```



```{r}
df_test_full2 <- df_test_full %>%
  filter(id %in% rand_id2)%>%
  select(id, sind, Y)

df_test_full2$pred_t540 <- df_test_full2$pred_t780 <- df_test_full2$pred_t1020 <- NA 
for(i in rand_id2){
  df_test_full2[df_test_full2$id==i & df_test_full2$sind>540, "pred_t540"]<- adglmm_pred_t540$newdata2 %>% filter(id ==i) %>% select(pred)
  
  df_test_full2[df_test_full2$id==i & df_test_full2$sind>780, "pred_t780"]<- adglmm_pred_t780$newdata2 %>% filter(id ==i) %>% select(pred)
  
  df_test_full2[df_test_full2$id==i & df_test_full2$sind>1020, "pred_t1020"]<- adglmm_pred_t1020$newdata2 %>% filter(id ==i) %>% select(pred)
}

# colSums(is.na(df_test_full2))
```



- Below is an example of predicted probablity on four random subjects

```{r, warning=FALSE}
df_test_full2 %>% 
  mutate_at(vars(pred_t540, pred_t780, pred_t1020), 
            function(x)exp(x)/(1+exp(x))) %>%
  ggplot()+
    geom_line(aes(x=sind, y = pred_t540, col = "9am"))+
    geom_line(aes(x=sind, y = pred_t780, col = "1pm"))+
    geom_line(aes(x=sind, y = pred_t1020, col = "5pm"))+
    geom_point(aes(x=sind, y = Y, col = "Outcome"), size = 0.2)+
    facet_wrap(~id)+
    labs(x = "Time", y = "Estimated latent function (probablity scale)")
```

<!-- Calculate AUC  -->



```{r}
# up to 540
J <- 1440
auc_t540_adglmm <- adglmm_pred_t540$newdata2 %>%
  # filter(!id %in% skip_id) %>%
  mutate(sind=sind*J) %>%
  mutate(window = cut(sind, breaks = c(540, 780, 1020, 1260, 1440), 
                      labels = c("9am-1pm", "1pm-5pm", "5pm-9pm","9pm-12pm"), 
                      include.lowest = T))
auc_t540_adglmm <- split(auc_t540_adglmm, f=auc_t540_adglmm$window)
auc_t540_adglmm <- auc_t540_adglmm[1:3] %>% 
  lapply(function(x)performance(prediction(x$pred, x$Y), measure = "auc"))
auc_t540_adglmm <- lapply(auc_t540_adglmm, function(x){x@y.values[[1]]}) %>% unlist()

# up to 780
auc_t780_adglmm <- adglmm_pred_t780$newdata2 %>%
  # filter(!id %in% skip_id) %>%
  mutate(sind=sind*J) %>%
  mutate(window = cut(sind, breaks = c(780, 1020, 1260, 1440), 
                      labels = c("1pm-5pm", "5pm-9pm","9pm-12pm"), 
                      include.lowest = T))
auc_t780_adglmm <- split(auc_t780_adglmm, f=auc_t780_adglmm$window)
auc_t780_adglmm <- auc_t780_adglmm[1:2] %>% 
  lapply(function(x)performance(prediction(x$pred, x$Y), measure = "auc"))
auc_t780_adglmm <- lapply(auc_t780_adglmm, function(x){x@y.values[[1]]}) %>% unlist()

# up to 1020
auc_t1020_adglmm <- adglmm_pred_t1020$newdata2 %>%
  # filter(!id %in% skip_id) %>%
  mutate(sind=sind*J) %>%
  mutate(window = cut(sind, breaks = c(1020, 1260, 1440), 
                      labels = c("5pm-9pm","9pm-12pm"), 
                      include.lowest = T))
auc_t1020_adglmm <- split(auc_t1020_adglmm, f=auc_t1020_adglmm$window)
auc_t1020_adglmm <- auc_t1020_adglmm[1] %>% 
  lapply(function(x)performance(prediction(x$pred, x$Y), measure = "auc"))
auc_t1020_adglmm <- lapply(auc_t1020_adglmm, function(x){x@y.values[[1]]}) %>% unlist()


```

### AUC

- To use the same sample for comparison, I'd exclude the subjects with failed Laplace approximation from the test set for both methods

```{r}
options(knitr.kable.NA = '')
tb_auc <- data.frame(auc_t540, 
           c(NA, auc_t780), 
           c(NA, NA, auc_t1020), 
        
           auc_t540_adglmm,
           c(NA, auc_t780_adglmm), 
           c(NA, NA, auc_t1020_adglmm))

colnames(tb_auc) <- rep(c("9am", "1pm", "5pm"), 2)
```


```{r}
# time
t_fit <- c(t_local_glmm+t_fpca, t_est_adglmm)
units(t_fit) <- "mins"

t_test <- c(t_pred, t_pred_adglmm)
# units(t_pred)

t_total <- t_fit+t_test


idx1 <- idx2 <- idx3 <- c(1, 3, 3)
names(idx1) <- c("Total time", as.character(round(t_total, 2)))
names(idx2) <- c("Time on prediction", as.character(round(t_test, 2)))
names(idx3) <- c("Time on model fitting", as.character(round(t_fit, 2)))

t_fit <- as.character(round(t_fit, 2))
t_test <- as.character(round(t_test, 2))
t_total <- as.character(round(t_total, 2))

```



```{r}
tb_auc %>%
  kable(digit = 4, table.attr = "style = \"color: black;\"") %>%
  kable_styling(full_width = F) %>% 
  add_header_above(header=idx1) %>%
  add_header_above(header=idx2) %>%
  add_header_above(header=idx3) %>%
  add_header_above(c(" "=1, "fGFPCA"=3, "GLMMadaptive"=3))
```

- fGFPCA does not always have higher AUC, but definitely most of the time. 
- In fact there are two scenarios where GLMMadaptive outperforms fGFPCA slightly: 1) given 9am to predict 1-5pm; and 2) given 1pm and predict 5-9pm. Both of them have a 4-hour interval apart from the maximum observation time and and prediction window. Could it have anything to do the cyclic trend in the data? Is 4h the length of a cycle? I tried to look at it but really couldn't see anything...there are too many points lumped together! 


```{r}
df %>% filter(id %in% rand_id2) %>%
  ggplot()+
  geom_point(aes(x=sind, y=Y), size = 0.2)+
  geom_vline(xintercept = c(540, 780, 1020, 1260))+
  scale_x_continuous(breaks = c(540, 780, 1020, 1260),
                     labels = c("9am", "1pm", "5pm", "9pm"))+
  facet_wrap(~id)
```



## Failed Laplace approximation

- This problem seems to be solved after changing the optimizer? (Method = "BFGS")
- The new optimizer also took less time
- The old one was SPG (Spectral projected gradient), which is a non-monotone line search. 
- The new one is BFGS (quasi-newton, the same in glmmTMB)


## Discussion

- What would be the reference method to compare our performance to? Original GLMMadaptvie perhaps is feasible on the entire dataset. But what should we use if we wanna do sub-sample comparison? Should we even do that for this data application (or simulation alone)? 
<!-- - There are still three participants with numeric problem! Shall we extend the observation track even more? I suspect that may cause still the same issue just on different subjects.  -->
<!-- - Is interpolation really necessary? It does not improve perfor- Should I calculate subject-wise AUC and average them? But some subjects are all zero within the prediction window, making AUC for this single subjects impossible to calculate...

