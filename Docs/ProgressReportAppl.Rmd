---
title: "Progress Report on NHANES Data"
author: "Ying Jin"
date: "2023-10-03"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# NHANES data

## Data overview

- 8763 subjects, 1440 measures each
- no missingness

```{r}
N <- length(unique(df$SEQN)) # sample size 8763
J <- max(df$sind) # 1440 measures per subject
```


```{r}
rand_id <- sample(unique(df$SEQN), size = 4) # "toy" sample

df %>% 
  filter(SEQN %in% rand_id) %>%
  ggplot()+
  geom_point(aes(x=sind, y=Z), size = 0.5)+
  facet_wrap(~SEQN)+
  labs(x="Time", y = "Activity", title = "A brief overview of the outcome")
```



## Out-of-sample prediction

follow up on the last report, I'll add some details here.

Let's set: 

- $s$: bin index
- $t_{m_s}$: midpoint time of bin s
- $\mathscr{T}_s$: all observation points in bin s
- $Y_i^s$ all observations in bin s from subject i. $Y_i^S := \{Y_i(t_j), t_j \in \mathscr{T}_s\}$

Let's assume we have a new subject $u$ with maximum observation time $T_u$. Then the log-likelihood of this new subject would be:

$$
l_u=\sum_{t_{m_s}<T_u}log(h(Y_u^s))+\hat{\eta}_u(t_{m_s})T(Y_u^s)-log(A[\hat{\eta}_u(t_{m_s})])
$$


where $\hat{\eta}_u(t_{m_s}) = \hat{\mu}_0(t_{m_s})+\sum_{k=1}^K \xi_{uk}\hat{\phi}(t_{m_s})$. 

From FPCA model (or the debias step to be added), we obtain $\hat{\mu}_0$, $\hat{\phi}_k$, as well as the variance estimates of $\xi_k$: $\hat{\lambda}_k$, and variance of residual process $\hat{\sigma^2}$.

With all these estimates, we wanna use Laplace approximation to find the $\xi_{uk}$ that maximizes $l_u$. 


I am using a package called **LaplacesDemon**, following this example https://cran.r-project.org/web/packages/LaplacesDemon/vignettes/LaplacesDemonTutorial.pdf

```{r, class.source = "fold-show"}
library(LaplacesDemon)
```


Now I am going to use one subject as an example to go over the approximation procedure:

First we write out the model: 

- Prior distribution: $\xi_{uk} \sim N(0, \hat{\lambda}_k)$
- Posterior distribution: 


\begin{aligned}
l(\mathbf{Y_u}|\mathbf{\xi}_u) &= \sum_{t_{m_s}<T_u}l(Y_u^s|\mathbf{\xi}_u) \\

Y_u^s|\mathbf{\xi}_u & \sim Binomial(n_s, p_s) \\

g^{-1}(p_s) = \hat{\eta}(t_{m_s}) &= \hat{\mu}_0(t_{m_s})+\sum_{k=1}^K \xi_{uk}\hat{\phi}(t_{m_s})
\end{aligned}



And $n_s$ is the number of observations in bin s. 

Bayes theroem:

$$
l(\mathbf{\xi}_u|\mathbf{Y}_u) \propto l(\mathbf{Y}_u|\mathbf{\xi}_u)+l(\mathbf{\xi}_u)
$$


```{r, class.source = "fold-show"}
## model
Model <- function(parm, Data){
  xi <- parm[Data$pos.xi]
  
  # log-prior
  xi.prior <- dmvnorm(xi, mean = rep(0, Data$J), sigma=Data$tao, log = TRUE)
  
  # log-posterior likelihood
  eta <- Data$f0+Data$X %*% xi
  p <- exp(eta)/(1+exp(eta))
  LL <- sum(dbinom(x=Data$y, size = Data$n, prob=p, log = TRUE)) # log likelihood of Y|xi
  LP <- LL+sum(xi.prior) # unnormalized joint log likelihood of (Y, xi)
  
  # output
  Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP,
                   yhat=Data$y, parm=parm)
  return(Modelout)
}
```


- Then we feed in data for one observation

```{r, class.source = "fold-show"}
rand_id <- test_id[15]
df_new <- df %>% filter(id == rand_id & sind <= 540) # observation until 9am

# put data into correct format
ns <- as.vector(table(df_new$bin)) # number of observations
hs <- df_new %>% group_by(bin) %>% summarize_at("Y", sum) %>% select(Y) %>% unlist()# number of success
nf <- ns-hs # number of failure
max_bin <- length(unique(df_new$bin)) # assume new skipped bins  
df_new2 <- data.frame(bin = unique(df_new$bin), ns, hs, nf)  
  
# extract estimates from FPCA model to be used in prior distribution
tao <- diag(fpca_mod$evalues[1:K]) # variance of xi

# extract estimates from FPCA model to be used in posterior distribution
f0 <- fpca_mod$mu[1:max_bin]
N <- nrow(df_new2) # "sample size" which is in fact number of observed bins in this case for a new subject
n <- df_new2$ns # number of experiements at each bin
y <- df_new2$hs # outcome, number of 1
X <- fpca_mod$efunctions[1:max_bin, 1:K] # eigen/PC functions
J <- K # number of parameters/scores
mon.names <- "LP"

# parameter names
name_lst <- as.list(rep(0, J))
names(name_lst) <- paste("xi", 1:J, sep = "")
parm.names <- as.parm.names(name_lst) # names of parameters to estimate
pos.xi <- grep("xi", parm.names)

# data
MyData <- list(J=J, X=X, mon.names=mon.names,
                 parm.names=parm.names, pos.xi=pos.xi, y=y, n=n, tao=tao, f0=f0)
```


```{r, class.source = "fold-show"}
# fit laplace approximation
# initial values generated by GIV function in the same package
Fit <- LaplaceApproximation(Model, Data=MyData)
```

- Summarizing output

```{r, class.source = "fold-show", fig.height=8, fig.width=8}
# summarizing output
print(Fit)
plot(Fit, MyData)
```

```{r}
# prediction
eta_pred_out <- fpca_mod$mu+fpca_mod$efunctions[, 1:K]%*%Fit$Summary1[, "Mode"]

data.frame(bin = mid, eta_pred = eta_pred_out) %>% 
  right_join(df %>% filter(id==rand_id) %>%  select(Y, sind, bin), by = "bin") %>%
  ggplot()+
  geom_point(aes(x=sind, y = Y))+
  geom_line(aes(x=sind, y = exp(eta_pred)/(1+exp(eta_pred))))+
  labs(title = rand_id)
  
```



# Full data application output

## fGFPCA

- Still need to project eigenfunctions to the full grid

```{r}
# results generated from DataApplNHANES.R
load(here("Data/ApplOutput_fGFPCA.RData"))
```




```{r message=FALSE, warning=FALSE}
df_test_full <- df %>% 
  filter(id %in% test_id) %>%
  filter(!id %in% skip_id) %>%
  left_join(df_test %>% select(id, bin, pred_t540, pred_t780, pred_t1020), by = c("id", "bin"))

rand_id2 <- sample(unique(df_test$id), 4)
# without interpolation, assume constant latent function value in each bin
df_test_full%>%
  filter(id %in% rand_id2) %>%
  mutate_at(vars(pred_t540, pred_t780, pred_t1020), function(x)exp(x)/(1+exp(x))) %>%
  ggplot()+
    geom_line(aes(x=bin, y = pred_t540, col = "9am"))+
    geom_line(aes(x=bin, y = pred_t780, col = "1pm"))+
    geom_line(aes(x=bin, y = pred_t1020, col = "5pm"))+
    geom_point(aes(x=bin, y = Y, col = "Outcome"), size = 0.2)+
    facet_wrap(~id)+
    labs(x = "Time", y = "Estimated latent function (probablity scale)",
         title = "Constant latent function within bin")
```

### Numeric problems 

- Three subjects had failed approximation, all happened with the shortest observed track
- Does it have to do with still the length of observed track? Or the arbitrary choices in the binning procedure (bin width). 
- Per last meeting, adding some details on laplace approximation


```{r, fig.height=3, fig.width=9}
df %>% filter(id %in% skip_id) %>% 
  ggplot()+
  geom_point(aes(x=sind, y=Y), size = 0.2)+
  facet_wrap(~id)+
  geom_vline(xintercept = c(540))
```


<!-- Calculate AUC -->


```{r}
# break by prediction window
lst_auc <- df_test_full %>% 
  mutate(window = cut(sind, breaks = c(0, 540, 780, 1020, 1260, 1440), 
                      labels = c("0-9am", "9am-1pm", "1pm-5pm", "5pm-9pm","9pm-12pm"), 
                      include.lowest = T))
lst_auc <- split(lst_auc, f=lst_auc$window)
```


```{r}
# no interpolation
# up to 9am
auc_t540 <- lst_auc[2:4] %>% 
  lapply(function(x)performance(prediction(x$pred_t540, x$Y), measure = "auc"))
auc_t540 <- lapply(auc_t540, function(x){x@y.values[[1]]}) %>% unlist()

# up to 1pm
auc_t780 <- lst_auc[3:4] %>% 
  lapply(function(x)performance(prediction(x$pred_t780, x$Y), measure = "auc"))
auc_t780 <- lapply(auc_t780, function(x){x@y.values[[1]]}) %>% unlist()

# up to 5pm
auc_t1020 <- lst_auc[4] %>% 
  lapply(function(x)performance(prediction(x$pred_t1020, x$Y), measure = "auc"))
auc_t1020 <- lapply(auc_t1020, function(x){x@y.values[[1]]}) %>% unlist()
```




## GLMMadaptvie

- when fitting model with **random intercept + slope** on the same full training set (even after scaling covariates): Error: vector memory exhausted (limit reached?).
- when fitting model with **only random intercept** on the same full training set: Error regarding a large coefficient value. One fix is to re-scale covariates. So I re-scaled minute index by dividing them by J, so the range is within (0, 1], and tried again. This fitting procedure took 20.72 minutes to finish



```{r}
load(here("Data/ApplOutput_GLMMadaptive.RData"))
```

- `r round(t_est_adglmm, 2)` minutes on model fitting and `r round(t_pred_adglmm, 2)` minutes on out-of-sample prediction. 

- Below is the model fit on the training set

```{r}
summary(adglmm_mod)
```



```{r}
df_test_full2 <- df_test_full %>%
  filter(id %in% rand_id2)%>%
  select(id, sind, Y)

df_test_full2$pred_t540 <- df_test_full2$pred_t780 <- df_test_full2$pred_t1020 <- NA 
for(i in rand_id2){
  df_test_full2[df_test_full2$id==i & df_test_full2$sind>540, "pred_t540"]<- adglmm_pred_t540$newdata2 %>% filter(id ==i) %>% select(pred)
  
  df_test_full2[df_test_full2$id==i & df_test_full2$sind>780, "pred_t780"]<- adglmm_pred_t780$newdata2 %>% filter(id ==i) %>% select(pred)
  
  df_test_full2[df_test_full2$id==i & df_test_full2$sind>1020, "pred_t1020"]<- adglmm_pred_t1020$newdata2 %>% filter(id ==i) %>% select(pred)
}

# colSums(is.na(df_test_full2))
```



- Below is an example of predicted probablity on four random subjects

```{r, warning=FALSE}
df_test_full2 %>% 
  mutate_at(vars(pred_t540, pred_t780, pred_t1020), 
            function(x)exp(x)/(1+exp(x))) %>%
  ggplot()+
    geom_line(aes(x=sind, y = pred_t540, col = "9am"))+
    geom_line(aes(x=sind, y = pred_t780, col = "1pm"))+
    geom_line(aes(x=sind, y = pred_t1020, col = "5pm"))+
    geom_point(aes(x=sind, y = Y, col = "Outcome"), size = 0.2)+
    facet_wrap(~id)+
    labs(x = "Time", y = "Estimated latent function (probablity scale)")
```

<!-- Calculate AUC  -->



```{r}
# up to 540
J <- 1440
auc_t540_adglmm <- adglmm_pred_t540$newdata2 %>%
  filter(!id %in% skip_id) %>%
  mutate(sind=sind*J) %>%
  mutate(window = cut(sind, breaks = c(540, 780, 1020, 1260, 1440), 
                      labels = c("9am-1pm", "1pm-5pm", "5pm-9pm","9pm-12pm"), 
                      include.lowest = T))
auc_t540_adglmm <- split(auc_t540_adglmm, f=auc_t540_adglmm$window)
auc_t540_adglmm <- auc_t540_adglmm[1:3] %>% 
  lapply(function(x)performance(prediction(x$pred, x$Y), measure = "auc"))
auc_t540_adglmm <- lapply(auc_t540_adglmm, function(x){x@y.values[[1]]}) %>% unlist()

# up to 780
auc_t780_adglmm <- adglmm_pred_t780$newdata2 %>%
  filter(!id %in% skip_id) %>%
  mutate(sind=sind*J) %>%
  mutate(window = cut(sind, breaks = c(780, 1020, 1260, 1440), 
                      labels = c("1pm-5pm", "5pm-9pm","9pm-12pm"), 
                      include.lowest = T))
auc_t780_adglmm <- split(auc_t780_adglmm, f=auc_t780_adglmm$window)
auc_t780_adglmm <- auc_t780_adglmm[1:2] %>% 
  lapply(function(x)performance(prediction(x$pred, x$Y), measure = "auc"))
auc_t780_adglmm <- lapply(auc_t780_adglmm, function(x){x@y.values[[1]]}) %>% unlist()

# up to 1020
auc_t1020_adglmm <- adglmm_pred_t1020$newdata2 %>%
  filter(!id %in% skip_id) %>%
  mutate(sind=sind*J) %>%
  mutate(window = cut(sind, breaks = c(1020, 1260, 1440), 
                      labels = c("5pm-9pm","9pm-12pm"), 
                      include.lowest = T))
auc_t1020_adglmm <- split(auc_t1020_adglmm, f=auc_t1020_adglmm$window)
auc_t1020_adglmm <- auc_t1020_adglmm[1] %>% 
  lapply(function(x)performance(prediction(x$pred, x$Y), measure = "auc"))
auc_t1020_adglmm <- lapply(auc_t1020_adglmm, function(x){x@y.values[[1]]}) %>% unlist()


```


## Compare fGFPCA and GLMMadaptive

- To use the same sample for comparison, I'd exclude the subjects with failed Laplace approximation from the test set for both methods

```{r}
options(knitr.kable.NA = '')
tb_auc <- data.frame(auc_t540, 
           c(NA, auc_t780), 
           c(NA, NA, auc_t1020), 
        
           auc_t540_adglmm,
           c(NA, auc_t780_adglmm), 
           c(NA, NA, auc_t1020_adglmm))

colnames(tb_auc) <- rep(c("9am", "1pm", "5pm"), 2)
```


```{r}
# time
t_fit <- c(t_local_glmm+t_fpca, t_est_adglmm)
units(t_fit) <- "mins"

t_test <- c(t_pred, t_pred_adglmm)
# units(t_pred)

t_total <- t_fit+t_test


idx1 <- idx2 <- idx3 <- c(1, 3, 3)
names(idx1) <- c("Total time", as.character(round(t_total, 2)))
names(idx2) <- c("Time on prediction", as.character(round(t_test, 2)))
names(idx3) <- c("Time on model fitting", as.character(round(t_fit, 2)))

t_fit <- as.character(round(t_fit, 2))
t_test <- as.character(round(t_test, 2))
t_total <- as.character(round(t_total, 2))

```



```{r}
tb_auc %>%
  kable(digit = 4, table.attr = "style = \"color: black;\"") %>%
  kable_styling(full_width = F) %>% 
  add_header_above(header=idx1) %>%
  add_header_above(header=idx2) %>%
  add_header_above(header=idx3) %>%
  add_header_above(c(" "=1, "fGFPCA"=3, "GLMMadaptive"=3))
```

- fGFPCA does not always have higher AUC, but definitely most of the time. 
- In fact there are two scenarios where GLMMadaptive outperforms fGFPCA slightly: 1) given 9am to predict 1-5pm; and 2) given 1pm and predict 5-9pm. Both of them have a 4-hour interval apart from the maximum observation time and and prediction window. Could it have anything to do the cyclic trend in the data? Is 4h the length of a cycle? I tried to look at it but really couldn't see anything...there are too many points lumped together! 


```{r}
df %>% filter(id %in% rand_id2) %>%
  ggplot()+
  geom_point(aes(x=sind, y=Y), size = 0.2)+
  geom_vline(xintercept = c(540, 780, 1020, 1260))+
  scale_x_continuous(breaks = c(540, 780, 1020, 1260),
                     labels = c("9am", "1pm", "5pm", "9pm"))+
  facet_wrap(~id)
```

## Discussion

- What would be the reference method to compare our performance to? Original GLMMadaptvie perhaps is feasible on the entire dataset. But what should we use if we wanna do sub-sample comparison? Should we even do that for this data application (or simulation alone)? 
- There are still three participants with numeric problem! Shall we extend the observation track even more? I suspect that may cause still the same issue just on different subjects. 
- Is interpolation really necessary? It does not improve performance very much but consumes time. Also, should I try more sophisticated methods for grid extension? 
- Should I calculate subject-wise AUC and average them? But some subjects are all zero within the prediction window, making AUC for this single subjects impossible to calculate...

