---
title: "Problem of Step 4 (debias) of fGFPCA on NHANES data"
author: "Ying Jin"
output: 
  html_document:
    self_contained: yes
    number_sections: true
    code_folding: hide
    toc: true
    toc_depth: 3
    toc_float: true
    font: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

set.seed(130)

library(here)
library(tidyverse)
library(ggpubr)
library(refund)
library(lme4)
library(ROCR)
library(GLMMadaptive)
library(kableExtra)
library(mvtnorm)
library(mgcv)
library(splines)
library(LaplacesDemon)
library(arsenal)
theme_set(theme_minimal())
```


```{r}
# load data
df <- read_rds(here("Data/nhanes_bi.rds"))

# code
source(here("Code/GLMM-FPCA.R")) 
# use pred_latent function to estimate latent function 
K <- 4
source(here("Code/OutsampBayes.R"))
```


# Fit fGFPCA model

```{r data_subset}
# head(df)
df <- df %>% rename(id=SEQN, Y=Z)
N <- length(unique(df$id))
J <- length(unique(df$sind))
```

# Step 1: Bin data

- Bin every 10 observations

```{r, message=FALSE}
# bin data
bin_w <- 10 # bin width
n_bin <- J/bin_w # number of bins
brks <- seq(0, J, by = bin_w) # cutoff points
mid <- (brks+bin_w/2)[1:n_bin] # mid points

df$bin <- cut(df$sind, breaks = brks, include.lowest = T, labels = mid)
df$bin <- as.numeric(as.character(df$bin))
```


# Step 2-4:  Local GLMM, fPCA, splines interpolation and GLMM reevaluation

```{r, cache=TRUE, results='hide'}
# sample size 
Ntr <- 500

# training sample of a certain size
train_id <- sample(unique(df$id), size = Ntr) 
train_df <- df %>% filter(id %in% train_id)

t1=Sys.time()
# local GLMM
train_bin_lst <- split(train_df, f = train_df$bin)
df_est_latent <- lapply(train_bin_lst, function(x){pred_latent(x, n_node = 0)}) 
df_est_latent <- bind_rows(df_est_latent) 

# FPCA
uni_eta_hat <- df_est_latent %>% filter(bin==sind)
mat_est_unique <- matrix(uni_eta_hat$eta_hat, nrow=Ntr, ncol=n_bin, byrow = F) 
fpca_mod <- fpca.face(mat_est_unique, argvals = mid, var=T, npc=K)

# interpolation
p <- 3 # order of b splines 
knots <- 35 # number of knots (same from FPCA model)
knots_values <- seq(-p, knots + p, length = knots + 1 + 2 *p)/knots
knots_values <- knots_values * (max(mid) - min(mid)) + min(mid)

B <- spline.des(knots = knots_values, x = mid, ord = p + 1,
                outer.ok = TRUE)$design  # evaluate B-splines on binned grid
Bnew <- spline.des(knots = knots_values, x = 1:J, ord = p + 1,
                   outer.ok = TRUE)$design  # evaluate B-splines on original grid


df_phi <- matrix(NA, J, K) 
for(k in 1:K){
  lm_mod <- lm(fpca_mod$efunctions[,k] ~ B-1)
  df_phi[,k] <- Bnew %*% coef(lm_mod)
}# project binned eigenfunctions onto the original grid


# debias model
df_phi <- data.frame(sind = 1:J, df_phi)
colnames(df_phi) <- c("sind", paste0("phi", 1:K))
train_df <- train_df %>% 
  left_join(df_phi, by = "sind")
train_df$id <- as.factor(train_df$id)
debias_glmm <- bam(Y ~ s(sind, bs="cr")+
                   s(id, by=phi1, bs="re")+
                   s(id, by=phi2, bs="re")+
                   s(id, by=phi3, bs="re")+
                   s(id, by=phi4, bs="re"), 
                 family = binomial, 
                 data=train_df, 
                 method = "fREML",
                 discrete = TRUE)
debias_model_list[[i]] <- debias_glmm
t2 <- Sys.time()

time_vec[i] <- as.numeric(t2-t1, unit = "secs")
  
```

<!-- - Time spent on model fitting -->

```{r, eval=FALSE}
time_vec
```

<!-- - FPCA eigenvalues -->

```{r, eval=FALSE}
lapply(fpca_model_list, function(x){x$evalues})
lapply(fpca_model_list, function(x){x$pve})
```

- Eigenvalues from debias models

```{r}
lapply(debias_model_list, function(x){1/x$sp[2:5]})
```

- PC functions

```{r}
pc_plot_list <- list()

for(i in  1:length(N_sub)){
  
  df_phi1 <- data.frame(t=mid, fpca_model_list[[i]]$efunctions)
  df_phi2 <- data.frame(t = 1:J, interp_pc_est[[i]])
  
  this_plot <- left_join(df_phi2, df_phi1, by = "t") %>%
    ggplot()+
    geom_point(aes(x=t, y=X1.y, col = "PC1"), na.rm = T, size = 0.5)+
    geom_point(aes(x=t, y=X2.y, col = "PC2"), na.rm = T, size = 0.5)+
    geom_point(aes(x=t, y=X3.y, col = "PC3"), na.rm = T, size = 0.5)+
    geom_point(aes(x=t, y=X4.y, col = "PC4"), na.rm = T, size = 0.5)+
    geom_line(aes(x=t, y=X1.x, col = "PC1"))+
    geom_line(aes(x=t, y=X2.x, col = "PC2"))+
    geom_line(aes(x=t, y=X3.x, col = "PC3"))+
    geom_line(aes(x=t, y=X4.x, col = "PC4"))+
    labs(x="time", y = "", title = paste0("Iter", i))
  
  pc_plot_list[[i]] <- this_plot
  
}

ggarrange(plotlist = pc_plot_list, common.legend = T)
```


The "reverse eigenvalue problem" does not always happen. When training sample size is small (like maybe 200), the problem happened less. So this is definately a numeric problem. But a very small sample also would induce sigularity issues.


Some other thoughts:

1. What about estimation method of smooth parameter? It seems GCV doesn't treat model as mixed effects. REML may be too slow without the "discrete=T" option on the full dataset. 
2. Is is possible to impose monotone constraint on smoothing parameter estimation? 


```{r, class.source='fold-show'}
load(here("Data/Appl_debias_model.RData"))
summary(debias_glmm)

# re-evaluated mean
new_mu <- predict(debias_glmm, type = "terms")[1:J, 1] # extract re-evaluated mean
plot(1:J, new_mu)

# re-evaluated eigenvalues
new_lambda <- 1/debias_glmm$sp[2:5]
new_lambda <- new_lambda/n_bin

# re-evaluated PC functions
df_phi <- debias_glmm$model %>% select(sind, starts_with("phi")) %>% distinct(.) %>%
  mutate_at(vars(starts_with("phi")), function(x){x*sqrt(n_bin)})
df_phi %>% pivot_longer(2:5) %>%
  ggplot()+
  geom_line(aes(x=sind, y=value, col = name))
  
```

```{r gen_data}
# generate data with the estimates from thedebias step


# generate score
xi <- rmvnorm(Ntr, mean = rep(0, K), sigma = diag(new_lambda))
bi <- xi %*% t(as.matrix(df_phi[, 2:5])) # random effect functions
dim(bi)

# latent gaussian function
new_mu <- matrix(new_mu, nrow = 1)
eta_i <- new_mu+bi
dim(new_mu)
  
  this_df <- data.frame(id = factor(rep(1:N, each=J)),
                        t = rep(t, N), 
                        eta_i = as.vector(t(eta_i)),
                        sind = rep(1:J, N))
  
  this_df$Y <- rbinom(N*J, size=1, prob=plogis(this_df$eta_i))
  
  sim_data[[m]] <- this_df
]
```
