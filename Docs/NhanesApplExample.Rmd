---
title: "Problem of Step 4 (debias) of fGFPCA on NHANES data"
author: "Ying Jin"
output: 
  html_document:
    self_contained: yes
    number_sections: true
    code_folding: hide
    toc: true
    toc_depth: 3
    toc_float: true
    font: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

set.seed(227)

library(here)
library(tidyverse)
library(ggpubr)
library(refund)
library(lme4)
library(ROCR)
library(GLMMadaptive)
library(kableExtra)
library(mvtnorm)
library(mgcv)
library(splines)
library(LaplacesDemon)
library(arsenal)
library(RColorBrewer)
theme_set(theme_minimal())
```


```{r}
# load data
df <- read_rds(here("Data/nhanes_bi.rds"))

# code
source(here("Code/GLMM-FPCA.R")) 
# use pred_latent function to estimate latent function 
K <- 4
source(here("Code/OutsampBayes.R"))
```



```{r bin_data}
# head(df)
df <- df %>% rename(id=SEQN, Y=Z)
N <- length(unique(df$id))
Ntr <- 500
J <- length(unique(df$sind))

load(here("Data/Appl_debias_model.RData"))

# parameters used in binning
bin_w <- 10 # bin width
n_bin <- J/bin_w # number of bins
brks <- seq(0, J, by = bin_w) # cutoff points
mid <- (brks+bin_w/2)[1:n_bin] # mid points
```


# Estimates

- Eigenvalues

```{r evals}
evals <- 1/debias_glmm$sp[2:5]/n_bin
```

- PC functions

```{r}
df_phi <- debias_glmm$model %>% select(sind, starts_with("phi")) %>% distinct(.) %>%
  mutate_at(vars(starts_with("phi")), function(x){x*sqrt(n_bin)}) 

df_phi %>% pivot_longer(2:5) %>%
  ggplot(aes(x=sind, y=value))+
  geom_line()+
  facet_wrap(~name)
```

- Mean function

```{r}
mu <- predict(debias_glmm, type = "terms")[1:J, 1]
# plot(1:J, mu, type = "l", main = "Population mean")
plot(1:J, plogis(mu), type = "l", main = "Population mean on the probablity scale")
```

- Deviation from each PC from the population mean

Below are figures for $\hat{f}_0(t)+2\sqrt{\hat{\lambda}}_k\phi_l(k)$, but on the probablity scale

```{r}
df_pc_dev <- df_phi %>%
  mutate(mu = mu) %>%
  mutate(phi1_dev = mu + 2*sqrt(evals[1])*phi1,
         phi2_dev = mu + 2*sqrt(evals[2])*phi2,
         phi3_dev = mu + 2*sqrt(evals[3])*phi3,
         phi4_dev = mu + 2*sqrt(evals[4])*phi4)

df_pc_dev %>%
  pivot_longer(7:10) %>%
  mutate(value = plogis(value), mu = plogis(mu)) %>%
  ggplot()+
  geom_line(aes(x=sind, y=value))+
  geom_line(aes(x=sind, y=mu), col = "red")+
  facet_wrap(~name) 
```

It looks like subjects with high loadings on PC2 would be more active than population average during afternoon/night, and less active in the morning. PC3, on the other hand, seems to indicate more active at night and early morning and less active in midday. They both reflect more active at night time. 


- Individual score

```{r}
data.frame(matrix(debias_glmm$coefficients[-c(1:9)], ncol = 4)) %>%
  pivot_longer(1:4) %>%
  ggplot(aes(x=name, y=value))+
  geom_boxplot()+
  geom_jitter(size = 0.2, alpha = 0.2)
```

Compared to the other three PCs, the distribution of $\hat{\xi}_{i3}$ seems to be shifted lower (positively skewed?). 

# Simulation using re-evaluated estimates

```{r gen_data}
# generate score
xi <- rmvnorm(Ntr, mean = rep(0, K), sigma = diag(evals))
# xi %>% data.frame() %>% pivot_longer(1:4, names_to = "PC", values_to = "score") %>%
#   ggplot()+
#   geom_boxplot(aes(x=PC, y=score))+
#   geom_jitter(aes(x=PC, y=score), size = 0.5)

# individual random effects
bi <- as.matrix(df_phi[, 2:5]) %*% t(xi)

# add in mean 
eta <- apply(bi, 2, function(x){x+mu})
gen_df <- data.frame(id = factor(rep(1:Ntr, each=J)),
                     sind = rep(1:J, Ntr), 
                     eta_i = as.vector(eta))

# binary outcome
gen_df$Y <- rbinom(Ntr*J, size=1, prob=plogis(gen_df$eta_i))

gen_df %>% filter(id %in% 1:4) %>%
  ggplot()+
  geom_point(aes(x=sind, y=Y), size = 0.5)+
  geom_line(aes(x=sind, y=plogis(eta_i)), col = "red")+
  facet_wrap(~id)+
  labs(title = "Generated data")
```

# fGFPCA on generated data 

```{r fGFPCA1}
# bin 
gen_df$bin <- cut(gen_df$sind, breaks = brks, include.lowest = T, labels = mid)
gen_df$bin <- as.numeric(as.character(gen_df$bin))

# local GLMM
train_bin_lst <- split(gen_df, f = gen_df$bin)
df_est_latent <- lapply(train_bin_lst, function(x){pred_latent(x, n_node = 0)}) 
df_est_latent <- bind_rows(df_est_latent) 

df_est_latent %>% filter(id %in% 1:4) %>%
  ggplot()+
  geom_point(aes(x=sind, y=Y), size = 0.5)+
  geom_line(aes(x=sind, y=plogis(eta_i), col = "True"))+
  geom_line(aes(x=sind, y=plogis(eta_hat), col = "Estimate"))+
  facet_wrap(~id)+
  labs(title = "Generated data")
```


```{r fGFPCA2}
# FPCA
uni_eta_hat <- df_est_latent %>% filter(bin==sind)
mat_est_unique <- matrix(uni_eta_hat$eta_hat, nrow=Ntr, ncol=n_bin, byrow = F) 
fpca_mod <- fpca.face(mat_est_unique, argvals = mid, var=T, npc=K)
```


```{r re_eval_PC}
# interpolation
p <- 3 # order of b splines 
knots <- 35 # number of knots (same from FPCA model)
knots_values <- seq(-p, knots + p, length = knots + 1 + 2 *p)/knots
knots_values <- knots_values * (max(mid) - min(mid)) + min(mid)

B <- spline.des(knots = knots_values, x = mid, ord = p + 1,
                outer.ok = TRUE)$design  # evaluate B-splines on binned grid
Bnew <- spline.des(knots = knots_values, x = 1:J, ord = p + 1,
                   outer.ok = TRUE)$design  # evaluate B-splines on original grid

est_phi <- matrix(NA, J, K) 
for(k in 1:K){
  lm_mod <- lm(fpca_mod$efunctions[,k] ~ B-1)
  est_phi[,k] <- Bnew %*% coef(lm_mod)
} 
colnames(est_phi) <- paste0("phi", 1:K)
```


I would like to look at estimated PC functions:

```{r}
theme_set(theme_minimal())
cols <- c(brewer.pal(4, "Set2"), "#000000") # define a color palette 
names(cols) <- c("0.2", "0.4", "0.6", "0.8", "True")

# PC functions after re-evaluation
bind_rows(
  data.frame(sind=1:J, est_phi*sqrt(n_bin), type = "Simulation"),
  data.frame(df_phi, type = "Generation")
) %>%
  rename("PC1" = phi1, "PC2" = phi2, "PC3" = phi3, "PC4" = phi4) %>%
  pivot_longer(2:5) %>%
  ggplot()+
  geom_line(aes(x=sind, y=value, col=type), linewidth=1)+
  facet_wrap(~name)+
  scale_color_brewer(palette = "Set2")+
   scale_x_continuous(breaks = seq(0, 1440, by = 360),
                     labels = c("0am", "6am", "12pm", "6pm", "12am"))+
  labs(x = "Time", y="", color = "")
```

It looks like something went wrong in the estimation of PC3. The estimated line is either identical or off by a sign compared to the true PC functions. That said, PC2 seems a bit off too. 

What about the estimated PC functions from FPCA without re-evaluation? 

```{r}
fpca_phi <- fpca_mod$efunctions*sqrt(n_bin)
colnames(fpca_phi) <- paste0("phi", 1:K)
bind_rows(
  data.frame(df_phi, type = "True"),
  data.frame(sind = mid, fpca_phi, type = "FPCA")
) %>%
  pivot_longer(2:5) %>%
  ggplot()+
  geom_point(aes(x=sind, y=value, col=type),size = 0.5)+
  facet_wrap(~name)
```


Next I'd like to re-evaluate the eigenvalues.

PS from the mgcv [user manual](https://cran.r-project.org/web/packages/mgcv/mgcv.pdf): *Note that discrete=TRUEmay result in re-ordering of variables in tensor product smooths for improved efficiency, and sp must be
supplied in re-ordered order*. Does this also apply to random effects? Does it have anything to do with the reversed eigenvalue functions? (I don't believe this is the case though, based on the experiments. Basciall I fit the model on a subset of 100 subjects setting discrete = F, and the smoothing parameters still flipped). 

```{r}
# debias model
gen_df <- gen_df %>% 
  left_join(df_phi, by = "sind")
gen_df$id <- as.factor(gen_df$id)
debias_glmm2 <- bam(Y ~ s(sind, bs="cr")+
                   s(id, by=phi1, bs="re")+
                   s(id, by=phi2, bs="re")+
                   s(id, by=phi3, bs="re")+
                   s(id, by=phi4, bs="re"),
                 family = binomial,
                 data=gen_df,
                 method = "fREML",
                 discrete = TRUE)
```

- Eigenvalues before and after re-evaluation

```{r, class.source='fold-show'}
# fpca
fpca_mod$evalues/n_bin

# re-evaluated
1/debias_glmm2$sp[2:5]
```

It looks like the flip happened again. 



```{r, eval=FALSE}
new_df <- gen_df %>% filter(id == 1)
est_phi <- predict(debias_glmm2, newdata = new_df, type = "terms")[, 2:5] %>% 
  data.frame()
est_phi %>%
  mutate(sind=1:J) %>%
  pivot_longer(1:4) %>%
  ggplot()+
  geom_line(aes(x=sind, y= value))+
  facet_wrap(~name)
```

```{r eval=FALSE}
coef1 <- coef(debias_glmm2)[c("s(id):phi1.1", "s(id):phi2.1", "s(id):phi3.1", "s(id):phi4.1")]
coef1 <- matrix(coef1, nrow = J, ncol = 4, byrow = T)
(as.matrix(df_phi[, 2:5]) * coef1) %>% data.frame() %>%
  mutate(sind=1:J) %>%
  pivot_longer(1:4) %>%
  ggplot()+
  geom_line(aes(x=sind, y= value))+
  facet_wrap(~name)
  
```

At this point, I think we could say the reversed eigenvalue problem is an artifact of the dataset it self. It looks like FPCA captures the shape of eigenvalues very well, but for some reason, mgcv::bam thinks it is necessary to penalize the second eigenfunctions more than the third function. Why is that? Could it be that PC2 is more wiggly? 


If we compare the eigenvalues used for data generation to the ones estimated by bam, we can see bam actually does a good job. But the order of eigenfunctions flipped, and that is because FPCA re-ordered the eigenfunctions based on eigenvalues. 

```{r, class.source='fold-show'}
# generation
evals
# estimation
1/debias_glmm2$sp[2:5]
```
