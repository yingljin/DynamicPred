---
title: "Reverse eigenvalue problem"
author: "Ying Jin"
output: 
  html_document:
    self_contained: yes
    number_sections: true
    code_folding: hide
    toc: true
    toc_depth: 3
    toc_float: true
    font: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

set.seed(1114)

library(here)
library(tidyverse)
library(ggpubr)
library(refund)
library(lme4)
library(ROCR)
library(GLMMadaptive)
library(kableExtra)
library(mvtnorm)
library(mgcv)
library(splines)
library(LaplacesDemon)
library(arsenal)
library(RColorBrewer)
theme_set(theme_minimal())
```


```{r load_data}
# load data
df <- read_rds(here("Data/nhanes_bi.rds"))
df <- df %>% rename(id=SEQN, Y=Z)

N <- length(unique(df$id))
J <- length(unique(df$sind))
K <- 4 # number of eigenfunctions to use
```


```{r load_code}
source(here("Code/GLMM-FPCA.R")) 
source(here("Code/OutsampBayes.R"))
```


It seems that the reversed eigenvalue problem has to do with 1) the choice of spline basis and 2) sample size. 

# Spline basis

I'll take a subset of 500 subject and try different spline basis. 

```{r data_split}
# or for sanity checks, use a subset of 1000 subjects
Ntry <- 500
train_id <- sample(unique(df$id), size = Ntry)
train_df <- df %>% filter(id %in% train_id)
```


```{r set_up}
# basis to try
bs_vec <- c("cr", "cs", "tp", "bs", "ps")
evals <- matrix(NA, length(bs_vec), K)
```


```{r fGFPCA}
# Step 1: Bin every 10 observations
bin_w <- 10 # bin width
n_bin <- J/bin_w # number of bins
brks <- seq(0, J, by = bin_w) # cutoff points
mid <- (brks+bin_w/2)[1:n_bin] # mid points

train_df$bin <- cut(train_df$sind, breaks = brks, include.lowest = T, labels = mid)
train_df$bin <- as.numeric(as.character(train_df$bin))

# Step 2:  Local GLMM
train_bin_lst <- split(train_df, f = train_df$bin)
df_est_latent <- lapply(train_bin_lst, function(x){pred_latent(x, n_node = 0)}) 
df_est_latent <- bind_rows(df_est_latent) 

# FPCA
uni_eta_hat <- df_est_latent %>% filter(bin==sind)
mat_est_unique <- matrix(uni_eta_hat$eta_hat,
                         nrow=length(train_id), 
                         ncol=n_bin, byrow = F) 
fpca_mod <- fpca.face(mat_est_unique, argvals = mid, var=T, npc=K) # keep 4 PCs

# Re-evaluation
## grid extension
p <- 3 # order of b splines 
knots <- 35 # number of knots (same from FPCA model)
knots_values <- seq(-p, knots + p, length = knots + 1 + 2 *p)/knots
knots_values <- knots_values * (max(mid) - min(mid)) + min(mid)

B <- spline.des(knots = knots_values, x = mid, ord = p + 1,
                outer.ok = TRUE)$design  # evaluate B-splines on binned grid
Bnew <- spline.des(knots = knots_values, x = 1:J, ord = p + 1,
                   outer.ok = TRUE)$design  # evaluate B-splines on original grid

df_phi <- matrix(NA, J, K) 
for(k in 1:K){
  lm_mod <- lm(fpca_mod$efunctions[,k] ~ B-1)
  df_phi[,k] <- Bnew %*% coef(lm_mod)
}# project binned eigenfunctions onto the original grid

## debias
## let's try scale the eigenfunctions before re-evaluation
df_phi <- data.frame(sind = 1:J, df_phi)
colnames(df_phi) <- c("sind", paste0("phi", 1:K))
# df_phi <- df_phi %>% mutate(
#   phi1 = sqrt(n_bin)*phi1,
#   phi2 = sqrt(n_bin)*phi2,
#   phi3 = sqrt(n_bin)*phi3,
#   phi4 = sqrt(n_bin)*phi4
# )
train_df <- train_df %>% 
  left_join(df_phi, by = "sind")
train_df$id <- as.factor(train_df$id)

```


```{r try_splines, results='hide'}
pb <- txtProgressBar(min=0, max=length(bs_vec), style = 3)

t1 <- Sys.time()
for(i in seq_along(bs_vec)){
  
this_glmm <- bam(Y ~ s(sind, bs = bs_vec[i])+
                     s(id, by=phi1, bs="re")+
                     s(id, by=phi2, bs="re")+
                     s(id, by=phi3, bs="re")+
                     s(id, by=phi4, bs="re"),
                   family = binomial,
                   data=train_df,
                   method = "fREML",
                   discrete = TRUE)
evals[i, ] <- 1/this_glmm$sp[2:5]/n_bin
setTxtProgressBar(pb, i)

}

close(pb)
t2 <- Sys.time()
# t2-t1 # about 5 minutes
```



```{r}
data.frame(bs_vec, evals)
```

It seems the flip happened for both cubic spline basis sets, but not the other ones. Also, when sample size gets larger, the flip happens almost certainly for all spline basis. 

When I increase the number of PC from 4 to 5, the second and third eigenvalues still flipped almost all the time.

```{r, eval=FALSE}
bind_rows(
data.frame(sind=1:J, mu = predict(this_glmm, type="terms")[1:J, 1]) %>% mutate(type = "BAM"),
data.frame(sind=mid, mu = fpca_mod$mu) %>% mutate(type = "FPCA")
) %>%
  ggplot()+
  geom_line(aes(x=sind, y=mu, col=type))+
  labs(title="Population mean estimate")

```


# Simulation using estimates from NHANES data

## Estimates use for simualtion

For now I will use estimates from the model fit on the whole NHANES dataset, even though the spline basis is not proper (bs = "cc"). 

```{r load_est}
load(here("Data/Appl_debias_model.RData"))

# eigenvalues
true_evals <- 1/debias_glmm$sp[2:5]/n_bin

# mean 
true_mu <- predict(debias_glmm, type = "terms")[, 1]
true_mu <- matrix(true_mu, nrow = J)[, 1]
# plot(1:J, true_mu)

# eigenfunctions
df_phi <- debias_glmm$model %>% select(sind, starts_with("phi")) %>% distinct(.) %>% 
  mutate_at(vars(starts_with("phi")), function(x){x*sqrt(n_bin)})
```

Below are some visualizations of these estimates

```{r plot_pc_func}
df_phi %>%
  pivot_longer(starts_with("phi")) %>%
  ggplot()+
  geom_line(aes(x=sind, y=value))+
  facet_wrap(~name)+
  labs(title = "Eigenfunctions")
```


```{r plot_pc_dev_from_mean}
df_pc_dev <- bind_rows(
  data.frame(sind = 1:J, mu=true_mu, pc = "PC1",
             high = true_mu + 2*sqrt(true_evals[1])*df_phi$phi1,
             low = true_mu - 2*sqrt(true_evals[1])*df_phi$phi1),
  data.frame(sind = 1:J, mu=true_mu, pc = "PC2",
             high = true_mu + 2*sqrt(true_evals[2])*df_phi$phi2,
             low = true_mu - 2*sqrt(true_evals[2])*df_phi$phi2),
  data.frame(sind = 1:J, mu=true_mu, pc = "PC3",
             high = true_mu + 2*sqrt(true_evals[3])*df_phi$phi3,
             low = true_mu - 2*sqrt(true_evals[3])*df_phi$phi3),
  data.frame(sind = 1:J, mu=true_mu, pc = "PC4",
             high = true_mu + 2*sqrt(true_evals[4])*df_phi$phi4,
             low = true_mu - 2*sqrt(true_evals[4])*df_phi$phi4)
)

# plot
df_pc_dev %>%
  ggplot()+
  geom_ribbon(aes(x=sind, y=mu, ymin=low, ymax=high, alpha = 0.2))+
  geom_line(aes(x=sind, y=mu))+
  facet_wrap(~pc)+
  labs(title = "Deviation of each PC from population mean")
```

```{r plot_score}
data.frame(matrix(debias_glmm$coefficients[-c(1:9)], ncol = 4)) %>%
  pivot_longer(1:4) %>%
  ggplot(aes(x=name, y=value))+
  geom_boxplot()+
  geom_jitter(size = 0.2, alpha = 0.2)+
  stat_summary(fun=mean, geom="point", color="red")+
  labs(title = "Individual scores")
```

## Data generation

```{r gen_data_unbound}
# interpolate FPCA mean
# interp_mu_fpca <- approx(mid, fpca_mod$mu, xout=1:J, rule=2)
# true_mu <- interp_mu_fpca$y

# increase sample size?
Ntry <- 500
# generate score
xi <- rmvnorm(Ntry, mean = rep(0, K), sigma = diag(true_evals))

# individual random effects
bi <- as.matrix(df_phi[, 2:5]) %*% t(xi)

# add in unbounded mean
eta <- apply(bi, 2, function(x){x+true_mu})
gen_df <- data.frame(id = factor(rep(1:Ntry, each=J)),
                     sind = rep(1:J, Ntry),
                     eta_i = as.vector(eta))

# binary outcome
gen_df$Y <- rbinom(Ntry*J, size=1, prob=plogis(gen_df$eta_i))
```


```{r plot_data}
# comparing generated data to the actual data
bind_rows(
    gen_df %>% 
      group_by(sind) %>% 
      summarize(active = mean(Y)) %>%
      mutate(type="Simulation"),
      
    df %>% 
      group_by(sind) %>% 
      summarize(active = mean(Y)) %>%
      mutate(type="NHANES")
) %>%
  ggplot()+
      geom_line(aes(x=sind, y=active, col = type))+
      labs(title="Proportion of active subjects by minute")
    
```

This is the first discrepancy I have found - It looks like the simulated data is more active and more variable than the real data. Doesn't that mean the algorithm overestimates the population activity and variability? Since the mean estimates will surely change in Step 4 (debias), would the overestimation be caused by that? 

## fGFPCA

```{r exp_gen_data}
# bin data
# using the same bin length 
gen_df$bin <- cut(gen_df$sind, breaks = brks, include.lowest = T, labels = mid)
gen_df$bin <- as.numeric(as.character(gen_df$bin))

# Step 2:  Local GLMM
gen_bin_lst <- split(gen_df, f = gen_df$bin)
df_est_latent <- lapply(gen_bin_lst, function(x){pred_latent(x, n_node = 0)}) 
df_est_latent <- bind_rows(df_est_latent)
## Figure
# df_est_latent %>% filter(id %in% 1:4) %>%
#   ggplot()+
#   geom_point(aes(x=sind, y=Y), size = 0.5)+
#   geom_line(aes(x=sind, y=plogis(eta_i), col = "true"))+
#   geom_line(aes(x=sind, y=plogis(eta_hat), col = "estimate"))+
#   facet_wrap(~id)

# FPCA
uni_eta_hat <- df_est_latent %>% filter(bin==sind)
mat_est_unique <- matrix(uni_eta_hat$eta_hat,
                        nrow=Ntry, 
                        ncol=n_bin, byrow = F) 
# dim(mat_est_unique)
fpca_mod <- fpca.face(mat_est_unique, argvals = mid, var=T, npc=K) # keep 4 PCs
```



```{r}
# Re-evaluation
est_phi <- matrix(NA, J, K) 
p <- 3 # order of b splines 
knots <- 35 # number of knots (same from FPCA model)
knots_values <- seq(-p, knots + p, length = knots + 1 + 2 *p)/knots
knots_values <- knots_values * (max(mid) - min(mid)) + min(mid)

B <- spline.des(knots = knots_values, x = mid, ord = p + 1,
                outer.ok = TRUE)$design  # evaluate B-splines on binned grid
Bnew <- spline.des(knots = knots_values, x = 1:J, ord = p + 1,
                   outer.ok = TRUE)$design
for(k in 1:K){
  lm_mod <- lm(fpca_mod$efunctions[,k] ~ B-1)
  est_phi[,k] <- Bnew %*% coef(lm_mod)
}# project binned eigenfunctions onto the original grid

## debias
est_phi <- data.frame(sind = 1:J, est_phi)
colnames(est_phi) <- c("sind", paste0("phi", 1:K))
gen_df <- gen_df %>% left_join(est_phi, by = "sind")
gen_df$id <- as.factor(gen_df$id)
### model fit with B-splines basis
t1 <- Sys.time()
this_glmm <- bam(Y ~ s(sind, bs = "cr")+
                   s(id, by=phi1, bs="re")+
                   s(id, by=phi2, bs="re")+
                   s(id, by=phi3, bs="re")+
                   s(id, by=phi4, bs="re"),
                 family = binomial,
                 data=gen_df,
                 method = "fREML",
                 discrete = TRUE)
t2 <- Sys.time()
```

Below I compared the estimated eigenvalues from simulated data to the true eigenvalues. As we see, the reverse did not happen on the simulated data. 

```{r new_evals, class.source='fold-show'}
# estimated eigenvalues
1/this_glmm$sp[2:5]/n_bin  

# true eigenvalues
true_evals
```
Below is the comparision between true and esimtated eigenfunctions. 

```{r new_PC}
bind_rows(df_phi %>% mutate(type = "True"),
          est_phi %>% mutate(type = "Estimate",
                             phi1 = phi1*sqrt(n_bin),
                             phi2 = phi2*sqrt(n_bin),
                             phi3 = phi3*sqrt(n_bin),
                             phi4 = phi4*sqrt(n_bin))) %>%
  pivot_longer(2:5) %>%
  ggplot()+
  geom_line(aes(x=sind, y=value, col = type))+
  facet_wrap(~name)
```

And below is the comparision between true mean, mean estimated from FPCA, and re-evaluated mean from the BAM model in Step 4. The re-evaluation seem to drive the mean function closer to the truth. But in that case, why would generated data much more active compared to the NHANES data? 


```{r compare_mean}
bind_rows(
  data.frame(sind=1:J, mu=true_mu) %>% mutate(type = "True"),
  data.frame(sind=1:J, mu=predict(this_glmm, type = "terms")[1:J, 1]) %>% mutate(type = "BAM"),
  data.frame(sind=mid, mu=fpca_mod$mu) %>% mutate(type="FPCA")
) %>%
  ggplot()+
      geom_line(aes(x=sind, y=mu, col = type))+
      labs(title="Mean estimates")


```

# Simulation with the FPCA mean

I would like to experiments if I could simulated data whose activity level and variation is closer to NHANES, using the interpolated FPCA mean and un-evaluated eigenvalues. 

```{r load_fpca_mu}
# load FPCA model from NHANES
load(here("Data/Appl_fpca_model.RData"))

# compare means on NHANES data from FPCA and debias model
bind_rows(
  data.frame(sind=1:J, mu = true_mu, type = "BAM"),
  data.frame(sind=mid, mu = fpca_mod$mu, type = "FPCA")
) %>% 
  ggplot()+
  geom_line(aes(x=sind, y=mu, col = type))+
  labs(title = "Mean estimates on the NHANES dataset")


```

## Data generation

```{r gen_data_fpca_mu}
# interpolate FPCA mean
interp_mu_fpca <- approx(mid, fpca_mod$mu, xout=1:J, rule=2)
true_mu <- interp_mu_fpca$y

# FPCA eigenvalues
true_evals <- fpca_mod$evalues/n_bin

# maybe increase sample size?
# Ntry <- 1000

# generate score
xi <- rmvnorm(Ntry, mean = rep(0, K), sigma = diag(true_evals))

# individual random effects
bi <- as.matrix(df_phi[, 2:5]) %*% t(xi)

# add in un-evaluated mean
eta <- apply(bi, 2, function(x){x+true_mu})
gen_df <- data.frame(id = factor(rep(1:Ntry, each=J)),
                     sind = rep(1:J, Ntry),
                     eta_i = as.vector(eta))

# binary outcome
gen_df$Y <- rbinom(Ntry*J, size=1, prob=plogis(gen_df$eta_i))
```


```{r plot_data_fpca_mu}
# comparing generated data to the actual data
bind_rows(
    gen_df %>% 
      group_by(sind) %>% 
      summarize(active = mean(Y)) %>%
      mutate(type="Simulation"),
      
    df %>% 
      group_by(sind) %>% 
      summarize(active = mean(Y)) %>%
      mutate(type="NHANES")
) %>%
  ggplot()+
      geom_line(aes(x=sind, y=active, col = type))+
      labs(title="Proportion of active subjects by minute")
    
```

Looks like by using the FPCA mean, I was able to lower the activity level of generated data down to the level of NHANES data. 

## fGFPCA

```{r fgfpca_fpca_mean}
# bin data
# using the same bin length 
gen_df$bin <- cut(gen_df$sind, breaks = brks, include.lowest = T, labels = mid)
gen_df$bin <- as.numeric(as.character(gen_df$bin))

# Step 2:  Local GLMM
gen_bin_lst <- split(gen_df, f = gen_df$bin)
df_est_latent <- lapply(gen_bin_lst, function(x){pred_latent(x, n_node = 0)}) 
df_est_latent <- bind_rows(df_est_latent)
## Figure
# df_est_latent %>% filter(id %in% 1:4) %>%
#   ggplot()+
#   geom_point(aes(x=sind, y=Y), size = 0.5)+
#   geom_line(aes(x=sind, y=plogis(eta_i), col = "true"))+
#   geom_line(aes(x=sind, y=plogis(eta_hat), col = "estimate"))+
#   facet_wrap(~id)

# FPCA
uni_eta_hat <- df_est_latent %>% filter(bin==sind)
mat_est_unique <- matrix(uni_eta_hat$eta_hat,
                        nrow=Ntry, 
                        ncol=n_bin, byrow = F) 
# dim(mat_est_unique)
fpca_mod_sim <- fpca.face(mat_est_unique, argvals = mid, var=T, npc=K) # keep 4 PCs
```



```{r}
# Re-evaluation
est_phi <- matrix(NA, J, K) 
p <- 3 # order of b splines 
knots <- 35 # number of knots (same from FPCA model)
knots_values <- seq(-p, knots + p, length = knots + 1 + 2 *p)/knots
knots_values <- knots_values * (max(mid) - min(mid)) + min(mid)

B <- spline.des(knots = knots_values, x = mid, ord = p + 1,
                outer.ok = TRUE)$design  # evaluate B-splines on binned grid

Bnew <- spline.des(knots = knots_values, x = 1:J, ord = p + 1,
                   outer.ok = TRUE)$design
for(k in 1:K){
  lm_mod <- lm(fpca_mod_sim$efunctions[,k] ~ B-1)
  est_phi[,k] <- Bnew %*% coef(lm_mod)
}# project binned eigenfunctions onto the original grid

## debias
est_phi <- data.frame(sind = 1:J, est_phi)
colnames(est_phi) <- c("sind", paste0("phi", 1:K))
gen_df <- gen_df %>% left_join(est_phi, by = "sind")
gen_df$id <- as.factor(gen_df$id)
### model fit with B-splines basis
t1 <- Sys.time()
this_glmm <- bam(Y ~ s(sind, bs = "cr")+
                   s(id, by=phi1, bs="re")+
                   s(id, by=phi2, bs="re")+
                   s(id, by=phi3, bs="re")+
                   s(id, by=phi4, bs="re"),
                 family = binomial,
                 data=gen_df,
                 method = "fREML",
                 discrete = TRUE)
t2 <- Sys.time()
```


Estimated eigenvalues

```{r}
1/this_glmm$sp[2:5]/sqrt(n_bin)
```

Using the FPCA estimates, I was able to lower the activity but not variation. I was also not able to recreate the reversed eigenvalue problem. Also, singularity issue happened in the binnin step when using the FPCA estimates to generate data. 
