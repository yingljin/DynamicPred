---
title: "Manuscript progress report"
author: "Ying Jin"
date: "`r Sys.Date()`"
output:
  html_document:
    self_contained: no
    number_sections: yes
    code_folding: hide
    toc: yes
    toc_depth: 3
    toc_float: yes
    font: 12pt
    keep_tex: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
    keep_tex: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(knitr.kable.NA = "")

set.seed(1114)


library(here)
library(tidyverse)
library(ggpubr)
library(refund)
library(lme4)
library(ROCR)
library(GLMMadaptive)
library(kableExtra)
library(knitr)
library(mvtnorm)
library(mgcv)
library(splines)
library(boot)
library(RColorBrewer)


# plot settings
theme_set(theme_minimal())
cols <- c(brewer.pal(4, "Set2"), "#000000") # define a color palette 
names(cols) <- c("0.2", "0.4", "0.6", "0.8", "True")
# display.brewer.all()
# display.brewer.pal(4, "Set2")
# brewer.pal(5, "Set2")

# code 
source(here("Code/PredEval.R"))

## prediction window 
window <- seq(0, 1, by = 0.2)
M <- 5

```


# Method

## Assumptions

- For each subject i in the population, a generalized outcome $Y_i(t)$ is generated along a variable t (for example, time), where $t \in (0, T)$. 
- The outcome, at any specific t, follows an exponential family distribution characterized by a (latent) continuous function $\eta_i(t)$: 

$$g[E(Y_i(t))] = \eta_i(t) = \beta_0(t)+b_i(t)$$

$$p(Y_i(t)) = h(Y_i(t))exp\{\eta_i(t)T[Y_i(t)]-A(\eta_i(t))\}$$

- The continuous latent function consists of a population-level fixed process and an individual-level random process

$$\eta_i(t) = \beta_0(t)+b_i(t)$$

## Observed data

In practice we would observe the discrete realization of $\{Y_i(t), t\}$ along a dense grid. For simplicity, we assume the observation grid is regular (same across sample). When we have J observations points in $(0, T]$, then for the jth observation point, we denote the corresponding value of t as $t_j$, and the corresponding outcome at this point $Y_i(t_j)$. 


## fGFPCA Algorithm

### Bin data: 

Choose a proper bin width $w$ considering model complexity and identifiability. For now let's say the bins are equal-length and non-overlapping. 

- Bin index $s = 1...S$
- Index of bin midpoints $m_s$
- Value of t corresponding to bin midpoints $t_{m_s}$
- Bin endpoints: $(t_{m_s}-\frac{w}{2}, t_{m_s}+\frac{w}{2}]$



### Local GLMMs

At the every bin, we fit a local intercept-only model:

$$g[E(Y_i(t_j))] =\eta_i(t_{m_s})= \beta_0(t_{m_s})+b_i(t_{m_s})$$
where $t_j \in (t_{m_s}-\frac{w}{2}, t_{m_s}+\frac{w}{2}]$.

Here we are basically saying that the value of latent function is constant within the same bin, which clearly is a misspecification of the true latent process.

From the model above. we will be able to estimate a $\hat{\eta_i}(t_{m_s})$ on the binned grid for every individual in the training sample. 

### FPCA 

Here, we fit a FPCA model on the $\hat{\eta_i}(t_{m_s})$ obtained from step 2:

$$\hat{\eta}_i(t_{m_s}) = f_0(t_{m_s})+\sum_{k=1}^K\xi_{ik}\phi_{k}(t_{m_s})+\epsilon_i(t_{m_s})$$

where $\xi_{ik}$ independently follows normal distribution $N(0, \lambda_k)$, and $\epsilon_i(t_{m_s})$ at each point follows $N(0, \sigma_2)$.

From this model, we will be able to obtain the following estimates which are shared across population: 

- Population mean $\hat{f_0}(t_{m_s})$
- Basis functions $\hat{\mathbf{\Phi}} = \{\hat{\phi}_1(t_{m_s}), ...,\hat{\phi}_K(t_{m_s}))\}$
- Estimates of variance of scores $\hat{\lambda}_1...\hat{\lambda}_K$

### Projection and Debias

The mean and basis functions are evaluated on the binned grid. To extend it to the original measurement grid data was collected on, we project the estimated eigenfunctions $\hat{\mathbf{\Phi}}$ back use spline basis. Now we have extend the $\hat{\phi}_k(t_{m_s})$ to the original grid $\hat{\phi}_k(t_j)$

Because of the misspecification of local GLMMs, the estimated eigenfunctions and eigenvalues are also biased by a constant multiplicative effect. Therefore, we use a GLMM to re-evaluate the mean function, eigenfunctions and eigenvalues. 

## Out-of-sample prediction 

Now, let's assume we have a new subject $u$ with $J_u$ observations ($J_u < J$). Then the log-likelihood of this new subject would be:


$$l_u=\sum_{t_j<t_{J_u}}log(h(Y_u(t_j)))+\hat{\eta}_u(t_j)T(Y_u(t_j))-log(A[\hat{\eta}_u(t_j)])$$

where $\hat{\eta}_u(t_j) = \hat{f}_0(t_j)+\sum_{k=1}^K \xi_{uk}\hat{\phi}(t_j)$. 

With estimates for the population-level parameters from fGFPCA algorithms above, we can estimate $\xi_{uk}$ by maximization of $l_u$. Direct maximization some times does not have closed form solution. Numeric maximization methods seem not very stable as well. So I have decided to used a Bayes approach (Laplace Approximation):

- Prior distribution: $\xi_{uk} \sim N(0, \hat{\lambda}_k)$
- Posterior distribution: the likelihood of $l_u =l(Y_u(t_j)|\mathbf{\xi}_u)$

Laplace Approximation would get the posterior mode of $\xi_{uk}$ through quadratic approximation.

# Reference method

## GLMMadaptive

- For large datasets, we can fit a model with random intercept and slope for time. It is doable on 500 datasets, but obviously too simple for the data generation scheme. We would expect it to perform terribly.

$$g(E(Y_i(t))) = \beta_0+\beta_1t+b_{i0}+b_{i1}t$$

- For small datasets, we would like to fit fGFPCA and GLMMadaptive on a dataset with smaller sample size and/or smaller measurement density. For the GLMMadaptive model, we would set it up with spline basis functions so that its flexibility is comparable with fGFPCA model, such as: 

$$g(E(Y_i(t))) = \sum_{k=1}^4\zeta_{k}B_k(t)+\sum_{l=1}^4\xi_{il}\phi_l(t)$$

## Generalized functional-on-scalar regression

We would like to use a second reference method for predictive performance comparision. In this model, predictions on a specific interval are all made using the last few observations in the observed window. For example, if we are given observations on [0, 0.2] (j = 1...200), we may use L observations taken right before t=0.2 ($t_j: j=200, ... 200-(L-1)$) as time-fixed covariate to predict any future time. 

Let's write out the model expression (with questionable notation). If the observations if up to $t_m$, then: 

\[\begin{aligned}
g(E[Y_i(t)]) &= \beta_0(t) +\sum_{l=1}^L \beta_l (t)Y_i(t_l)\\
l & = m,...m-(j-1)\\
t &> t_m 
\end{aligned}\]

This is a simple function-on-scalar model with no random effect, meaning all subject with the same last observed outcome would have the same estimated/predicted latent track. This is clearly anti-intuitive. But adding random effects would make out-of-sample prediction impossible. 

Under both frameworks, for each dataset we need to fit four models (similar to the GLMMadaptive method): 

- Given 0-0.2, predict 0.2-1
- Given 0-0.4, predict 0.4-1
- Given 0-0.6, predict 0.6-1
- Given 0-0.8, predict 0.8-1

But prediction performance will be reported by equal-length time window. 


# Larger-scale simulation

## Simulation set up

Here we simulate binary data from cyclic latent process:


\[\begin{aligned}
Y_i(t) & \sim Bernoulli(\frac{exp(\eta_i(t))}{1+exp(\eta_i(t))}) \\
\eta_i(t) &= f_0(t)+ \xi_{i1}\sqrt{2}sin(2\pi t)+\xi_{i2}\sqrt{2}cos(2\pi t)+\xi_{i3}\sqrt{2}sin(4\pi t)+\xi_{i4}\sqrt{2}cos(4\pi t)
\end{aligned}\]

where:

- $t$ is 1000 equal-spaced observations points on $[0, 1]$ (J = 1000).
- $f_0(t)=0$
- $\xi_k \sim N(0, \lambda_k)$, and $\lambda_k = 1, 0.5, 0.25, 0.125$ for k = 1, 2, 3, 4 respectively. 
- Sample size $N = 500$
- In the binning step, we bin every 10 observations
- 500 simulations were implemented



## Figures

```{r load_sim1}
load(here("Data/TrialRun/SimOutput_fGFPCA.RData"))
load(here("Data/TrialRun/SimOutput_GLMMadaptive.RData"))
load(here("Data/TrialRun/SimOutput_GFOSR_L1.RData"))
load(here("Data/TrialRun/SimOutput_GFOSR_L5.RData"))

rand_id <- sample(501:600, size = 4)
```


```{r DataFormat}
pred_list_fGFPCA <- lapply(pred_list_fGFPCA, function(x){x %>% select(-bin)})
name_vec <- colnames(pred_list_fGFPCA[[1]])
pred_list_fGFPCA <- lapply(pred_list_fGFPCA, 
                           function(x){
                      x[x$t<=0.2, c("pred0.2", "pred0.2_lb", "pred0.2_ub")] <- NA
                      x[x$t<=0.4, c("pred0.4", "pred0.4_lb", "pred0.4_ub")] <- NA
                      x[x$t<=0.6, c("pred0.6", "pred0.6_lb", "pred0.6_ub")] <- NA
                      x[x$t<=0.8, c("pred0.8", "pred0.8_lb", "pred0.8_ub")] <- NA
                      return(x)    
                       })



pred_list_GLMMad <- pred_list_GLMMad[1:5]
pred_list_GLMMad <- lapply(pred_list_GLMMad, function(x){
  colnames(x) <- name_vec
  return(x)
})

pred_list_gfofr <- lapply(pred_list_gfofr, function(x){
  x <- x %>% select(-window)
  colnames(x) <- name_vec
  return(x)
})

pred_list_gfofr_l5 <- lapply(pred_list_gfofr_l5, function(x){
  x <- x %>% select(-window)
  colnames(x) <- name_vec
  return(x)
})



```


```{r large_sim_plot, fig.height=8, fig.width=8}
bind_rows(
  pred_list_fGFPCA[[1]] %>% filter(id %in% rand_id) %>% 
    mutate(method="fGFPCA"),
  pred_list_GLMMad[[1]] %>% filter(id %in% rand_id) %>%
    mutate(method = "GLMMadaptive"),
  pred_list_gfofr[[1]] %>% filter(id %in% rand_id) %>%
    mutate(method = "GFOSR (L=1)"),
  pred_list_gfofr_l5[[1]] %>% filter(id %in% rand_id) %>%
    mutate(method = "GFOSR (L=5)") 
) %>% 
  mutate_at(vars(eta_i, starts_with("pred")), 
                .funs = function(x){exp(x)/(1+exp(x))}) %>%
  mutate(method=factor(method, 
         levels = c("fGFPCA", "GFOSR (L=5)", "GFOSR (L=1)","GLMMadaptive"))) %>%
  ggplot()+
  geom_point(aes(x=t, y=Y), size = 0.2)+
  geom_line(aes(x=t, y=eta_i, col = "True"))+
  geom_line(aes(x=t, y=pred0.2, col = "0.2"), na.rm = T)+
  geom_ribbon(aes(x=t, ymin=pred0.2_lb, ymax = pred0.2_ub,
                  col = "0.2", fill = "0.2", alpha = 0.1),
              linetype="dashed")+
  geom_line(aes(x=t, y=pred0.4, col = "0.4"), na.rm = T)+
  geom_ribbon(aes(x=t, ymin=pred0.4_lb, ymax = pred0.4_ub,
                  col = "0.4", fill = "0.4", alpha = 0.1),
              linetype="dashed")+
  geom_line(aes(x=t, y=pred0.6, col = "0.6"), na.rm = T)+
  geom_ribbon(aes(x=t, ymin=pred0.6_lb, ymax = pred0.6_ub,
                  col = "0.6", fill = "0.6", alpha = 0.1),
              linetype="dashed")+
  geom_line(aes(x=t, y=pred0.8, col = "0.8"), na.rm = T)+
  geom_ribbon(aes(x=t, ymin=pred0.8_lb, ymax = pred0.8_ub,
                  col = "0.8", fill = "0.8", alpha = 0.1),
              linetype="dashed")+
  guides(alpha = "none", col="none")+
  facet_grid(rows = vars(id), cols = vars(method))+
  labs(col = "Maximum observation time", x = "Time", y="")+
  scale_x_continuous(breaks = seq(0, 1, by = 0.2))+
  theme(legend.position = "bottom")+
  scale_color_manual(values = cols)
```

## Preformance

Please note that here, the GLMM has a **square** time for both fixed effect and random effect.

### Time (minutes)

```{r}
data.frame(Method = c("fGFPCA", "GFOSR (L=5)", "GFOSR (L=1)","GLMMadaptive"), 
           Time = c(mean(time_fGFPCA),
                    mean(time_gfofr_l5),
                    mean(time_gfofr),
                    mean(time_GLMMad))) %>%
  kable(digits = 3, booktabs=T,
        table.attr="style=\"color:black;\"") %>%
  kable_styling(full_width = F)
```

### ISE

```{r}
cbind(calc_ISE(pred_list_fGFPCA, window),
      calc_ISE(pred_list_gfofr, window),
      calc_ISE(pred_list_gfofr_l5, window),
      calc_ISE(pred_list_GLMMad, window)) %>%
  data.frame(window = c("[0, 0.2]", "(0,2, 0.4]", "(0.4, 0.6]", 
                    "(0.6, 0.8]","(0.8, 1]"),
             ., 
             check.names = F) %>%
  kable(digits = 3, booktabs=T,
        table.attr="style=\"color:black;\"") %>%
  kable_styling(full_width = F) %>% 
  add_header_above(c(" " = 1, "fGFPCA" = 4, "GFOSR (L=5)" = 4, 
                     "GFOSR (L=1)" = 4, "GLMMadaptive" = 4)) %>%
  add_header_above(c(" " = 1, "Maximum observation time" = 16)) %>%
  landscape()
```


It looks like the predictive performance can be seriously worse if the prediction window is far from the observation window.


### AUC

```{r}
cbind(calc_AUC(pred_list_fGFPCA, window),
      calc_AUC(pred_list_gfofr, window),
      calc_AUC(pred_list_gfofr_l5, window),
      calc_AUC(pred_list_GLMMad, window)) %>%
  data.frame(window = c("[0, 0.2]", "(0,2, 0.4]", "(0.4, 0.6]", 
                    "(0.6, 0.8]","(0.8, 1]"),
             ., 
             check.names = F) %>%
  kable(digits = 3, booktabs=T,
        table.attr="style=\"color:black;\"") %>%
  kable_styling(full_width = F) %>% 
  add_header_above(c(" " = 1, "fGFPCA" = 4, "GFOSR (L=5)" = 4, 
                     "GFOSR (L=1)" = 4, "GLMMadaptive" = 4)) %>%
  add_header_above(c(" " = 1, "Maximum observation time" = 16)) %>%
  landscape()
```

It seems that the GFOSR models, while are the fastest, also performs the worst in AUC.


### Prediction interval


Not sure how to display. Maybe start with if the prediction interval covers the truth? That is, coverage rate at each time point, averaged over all simulation and subjects? 

```{r, fig.height=4, fig.width=16}
bind_rows(calc_predint_cover(pred_list_fGFPCA) %>% mutate(method = "fGFPCA"),
          calc_predint_cover(pred_list_gfofr_l5) %>% mutate(method = "GFOSR (L=5)"),
          calc_predint_cover(pred_list_gfofr) %>% mutate(method = "GFOSR (L=1)"),
          calc_predint_cover(pred_list_GLMMad) %>% mutate(method = "GLMMadaptive")) %>%
  ggplot()+
  geom_line(aes(x=t, y=cover0.2, col = "0.2"), na.rm = T)+
  geom_line(aes(x=t, y=cover0.4, col = "0.4"), na.rm = T)+
  geom_line(aes(x=t, y=cover0.6, col = "0.6"), na.rm = T)+
  geom_line(aes(x=t, y=cover0.8, col = "0.8"), na.rm = T)+
  facet_grid(cols = vars(method))+
  labs(col = "Maximum observation time", x = "Time", y="Coverage rate")+
  scale_x_continuous(breaks = seq(0, 1, by = 0.2))+
  theme(legend.position = "bottom")+
  scale_color_manual(values = cols)
```

