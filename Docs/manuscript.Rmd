---
title: "Manuscript progress report"
author: "Ying Jin"
date: "`r Sys.Date()`"
output: 
  html_document:
    self_contained: yes
    number_sections: true
    code_folding: hide
    toc: true
    toc_depth: 3
    toc_float: true
    font: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
options(knitr.kable.NA = "")

set.seed(1120)


library(here)
library(tidyverse)
library(ggpubr)
library(refund)
library(lme4)
library(ROCR)
library(GLMMadaptive)
library(kableExtra)
library(knitr)
library(mvtnorm)
library(mgcv)
library(splines)
theme_set(theme_minimal())

load(here("Data/SimOutput_fGFPCA.RData"))
load(here("Data/SimOutput_GLMMadaptive.RData"))

## prediction window 
window <- seq(0, 1, by = 0.2)
M <- length(pred_list_all)
```


# Method

## Assumptions

- For each subject i in the population, a generalized outcome $Y_i(t)$ is generated along a variable t (for example, time), where $t \in (0, T)$. 
- The outcome, at any specific t, follows an exponential family distribution characterized by a (latent) continuous function $\eta_i(t)$: 

$$g[E(Y_i(t))] = \eta_i(t) = \beta_0(t)+b_i(t)$$

$$p(Y_i(t)) = h(Y_i(t))\frac{exp(\eta_i(t)T[Y_i(t)])}{A(\eta_i(t))}$$

- The continuous latent function consists of a population-level fixed process and an individual-level random process

$$\eta_i(t) = \beta_0(t)+b_i(t)$$

## Observed data

In practice we would observe the discrete realization of $\{Y_i(t), t\}$ along a dense grid. For simplicity, we assume the observation grid is regular (same across sample). When we have J observations points in $(0, T]$, then for the jth observation point, we denote the corresponding value of t as $t_j$, and the corresponding outcome at this point $Y_i(t_j)$. 


## fGFPCA Algorithm

### Bin data: 

Choose a proper bin width $w$ considering model complexity and identifiability. For now let's say the bins are equal-length and non-overlapping. 

- Bin index $s = 1...S$
- Index of bin midpoints $m_s$
- Value of t corresponding to bin midpoints $t_{m_s}$
- Bin endpoints: $(t_{m_s}-\frac{w}{2}, t_{m_s}+\frac{w}{2}]$

```{r, eval=FALSE}
grid_exp <- data.frame(t = seq(0, 1, by = 0.01), y = 0)
ggplot(grid_exp, aes(x=t, y=y))+
  geom_point(size = 0.5)
```

### Local GLMMs

At the every bin, we fit a local intercept-only model:

$$g[E(Y_i(t_j))] =\eta_i(t_{m_s})= \beta_0(t_{m_s})+b_i(t_{m_s})$$
where $t_j \in (t_{m_s}-\frac{w}{2}, t_{m_s}+\frac{w}{2}]$.

Here we are basically saying that the value of latent function is constant within the same bin, which clearly is a misspecification of the true latent process.

From the model above. we will be able to estimate a $\hat{\eta_i}(t_{m_s})$ on the binned grid for every individual in the training sample. 

### FPCA 

Here, we fit a FPCA model on the $\hat{\eta_i}(t_{m_s})$ obtained from step 2:

$$\hat{\eta}_i(t_{m_s}) = f_0(t_{m_s})+\sum_{k=1}^K\xi_{ik}\phi_{k}(t_{m_s})+\epsilon_i(t_{m_s})$$

where $\xi_{ik}$ independently follows normal distribution $N(0, \lambda_k)$, and $\epsilon_i(t_{m_s})$ at each point follows $N(0, \sigma_2)$.

From this model, we will be able to obtain the following estimates which are shared across population: 

- Population mean $\hat{f_0}(t_{m_s})$
- Basis functions $\hat{\mathbf{\Phi}} = \{\hat{\phi}_1(t_{m_s}), ...,\hat{\phi}_K(t_{m_s}))\}$
- Estimates of variance of scores $\hat{\lambda}_1...\hat{\lambda}_K$

### Projection and Debias

The mean and basis functions are evaluated on the binned grid. To extend it to the original measurement grid data was collected on, we project the estimated eigenfunctions $\hat{\mathbf{\Phi}}$ back use spline basis. Now we have extend the $\hat{\phi}_k(t_{m_s})$ to the original grid $\hat{\phi}_k(t_j)$

Because of the misspecification of local GLMMs, the estimated eigenfunctions and eigenvalues are also biased by a constant multiplicative effect. Therefore, we use a GLMM to re-evaluate the mean function, eigenfunctions and eigenvalues. 

## Out-of-sample prediction 

Now, let's assume we have a new subject $u$ with $J_u$ observations ($J_u < J$). Then the log-likelihood of this new subject would be:


$$l_u=\sum_{t_j<t_{J_u}}log(h(Y_u(t_j)))+\hat{\eta}_u(t_j)T(Y_u(t_j))-log(A[\hat{\eta}_u(t_j)])$$

where $\hat{\eta}_u(t_j) = \hat{f}_0(t_j)+\sum_{k=1}^K \xi_{uk}\hat{\phi}(t_j)$. 

With estimates for the population-level parameters from fGFPCA algorithms above, we can estimate $\xi_{uk}$ by maximization of $l_u$. Direct maximization some times does not have closed form solution. Numeric maximization methods seem not very stable as well. So I have decided to used a Bayes approach (Laplace Approximation):

- Prior distribution: $\xi_{uk} \sim N(0, \hat{\lambda}_k)$
- Posterior distribution: the likelihood of $l_u =l(Y_u(t_j)|\mathbf{\xi}_u)$

Laplace Approximation would get the posterior mode of $\xi_{uk}$ through quadratic approximation.


# Larger-scale simulation

## Simulation set up

Here we simulate binary data from cyclic latent process:


\[\begin{aligned}
Y_i(t) & \sim Bernoulli(\frac{exp(\eta_i(t))}{1+exp(\eta_i(t))}) \\
\eta_i(t) &= f_0(t)+ \xi_{i1}\sqrt{2}sin(2\pi t)+\xi_{i2}\sqrt{2}cos(2\pi t)+\xi_{i3}\sqrt{2}sin(4\pi t)+\xi_{i4}\sqrt{2}cos(4\pi t)
\end{aligned}\]

where:

- $t$ is 1000 equal-spaced observations points on $[0, 1]$ (J = 1000).
- $f_0(t)=0$
- $\xi_k \sim N(0, \lambda_k)$, and $\lambda_k = 1, 0.5, 0.25, 0.125$ for k = 1, 2, 3, 4 respectively. 
- Sample size $N = 500$
- In the binning step, we bin every 10 observations
- 500 simulations were implemented

## Figure


```{r fig_pred}
rand_id <- sample(501:600, size = 4)
# range(pred_list_all[[1]]$pred0.2, na.rm = T)
fig_fgfpca <- pred_list_all[[1]] %>%
  filter(id %in% rand_id) %>%
  mutate_at(vars(eta_i, pred0.2, pred0.4, pred0.6, pred0.8), function(x){exp(x)/(1+exp(x))}) %>%
  ggplot()+
  geom_point(aes(x=t, y=Y), size = 0.2)+
  geom_line(aes(x=t, y=eta_i, col = "True"))+
  geom_line(aes(x=t, y=pred0.2, col = "0.2"), linetype="dashed", na.rm=T)+
  geom_line(aes(x=t, y=pred0.4, col = "0.4"), linetype="dashed", na.rm=T)+
  geom_line(aes(x=t, y=pred0.6, col = "0.6"), linetype="dashed", na.rm=T)+
  geom_line(aes(x=t, y=pred0.8, col = "0.8"), linetype="dashed", na.rm=T)+
  facet_wrap(~id)+
  labs(title = "fGFPCA", col = "Maximum observation time")

fig_adglmm <- pred_list_ref[[1]] %>%
  filter(id %in% rand_id) %>%
  mutate_at(vars(eta_i, pred0.2, pred0.4, pred0.6, pred0.8), function(x){exp(x)/(1+exp(x))}) %>%
  ggplot()+
  geom_point(aes(x=t, y=Y), size = 0.2)+
  geom_line(aes(x=t, y=eta_i, col = "True"))+
  geom_line(aes(x=t, y=pred0.2, col = "0.2"), linetype="dashed", na.rm=T)+
  geom_line(aes(x=t, y=pred0.4, col = "0.4"), linetype="dashed", na.rm=T)+
  geom_line(aes(x=t, y=pred0.6, col = "0.6"), linetype="dashed", na.rm=T)+
  geom_line(aes(x=t, y=pred0.8, col = "0.8"), linetype="dashed", na.rm=T)+
  facet_wrap(~id)+
  labs(title = "GLMMadaptive", col = "Maximum observation time")

```

```{r}
ggarrange(fig_fgfpca, fig_adglmm,
          nrow = 1, common.legend = T)
```

## ISE

```{r}
## ISE container 
ise_mat <- ise_mat_ref <- array(NA, 
                                dim = c(length(window)-2, length(window)-2, M))
# dims: prediction window, max obs time, simulation iter

```

```{r ise_fGFPCA}
## calculation
for(m in 1:M){
  this_df <- pred_list_all[[m]]
  ise_tb <- pred_list_all[[m]] %>%
    mutate(err1 = (pred0.2-eta_i)^2,
           err2 = (pred0.4-eta_i)^2,
           err3 = (pred0.6-eta_i)^2,
           err4 = (pred0.8-eta_i)^2) %>%
    select(id, t, starts_with("err")) %>% 
    mutate(window = cut(t, breaks = window, include.lowest = T)) %>% 
    group_by(window, id) %>% 
    summarise_at(vars(err1, err2, err3, err4), sum) %>% 
    group_by(window) %>% 
    summarize_at(vars(err1, err2, err3, err4), mean) %>%
    filter(window != "[0,0.2]") %>% 
    select(starts_with("err"))
  ise_mat[, ,m] <- as.matrix(ise_tb)
  
  
}

mean_ise <- apply(ise_mat, c(1, 2), mean)

# mean_ise <- data.frame(mean_ise) %>%
#   mutate(Window = c("(0.2, 0.4]", "(0.4, 0.6]", "(0.6, 0.8]", "(0.8, 1.0]"),
#          .before = 1)
colnames(mean_ise) <- c("0.2", "0.4", "0.6", "0.8")
```

```{r ise_GLMMadaptive}
## calculation
for(m in 1:M){
  this_df <- pred_list_ref[[m]]
  ise_tb <- pred_list_ref[[m]] %>%
    mutate(err1 = (pred0.2-eta_i)^2,
           err2 = (pred0.4-eta_i)^2,
           err3 = (pred0.6-eta_i)^2,
           err4 = (pred0.8-eta_i)^2) %>%
    select(id, t, starts_with("err")) %>% 
    mutate(window = cut(t, breaks = window, include.lowest = T)) %>% 
    group_by(window, id) %>% 
    summarise_at(vars(err1, err2, err3, err4), sum) %>% 
    group_by(window) %>% 
    summarize_at(vars(err1, err2, err3, err4), mean) %>%
    filter(window != "[0,0.2]") %>% 
    select(starts_with("err"))
  ise_mat_ref[, ,m] <- as.matrix(ise_tb)
  
  
}

mean_ise_ref <- apply(ise_mat_ref, c(1, 2), mean)
# mean_ise_ref <- data.frame(mean_ise_ref) %>% 
#   mutate(Window = c("(0.2, 0.4]", "(0.4, 0.6]", "(0.6, 0.8]", "(0.8, 1.0]"),
#          .before = 1)
colnames(mean_ise_ref) <- c("0.2", "0.4", "0.6", "0.8")
```


```{r}
data.frame(Window = c("(0.2, 0.4]", "(0.4, 0.6]", "(0.6, 0.8]", "(0.8, 1.0]"),
           mean_ise, mean_ise_ref, check.names = F) %>%
  kable(digits = 3, table.attr = "style = \"color: black;\"", caption = "Integrated squared error") %>%
  kable_styling(full_width = F) %>% 
  add_header_above(c(" "=1, "fGFPCA" = 4, "GLMMadaptive" = 4)) %>%
  add_header_above(c(" "= 1, "Maximum observation time" = 8))
```


## AUC

```{r auc_func}
## a function to calculate AUC
get_auc <- function(y, pred){
  if(sum(is.na(y))>0 | sum(is.na(pred))>0){
    auc <- NA
  }
  else{
    this_perf <- performance(prediction(pred, y), measure = "auc")
    auc <- this_perf@y.values[[1]]
  }
  return(auc)
}

```


```{r}
## auc container 
auc_mat <- array(NA, dim = c(length(window)-2, length(window)-2, M))
auc_mat_ref <- array(NA, dim = c(length(window)-2, length(window)-2, M))
```


```{r auc_fGFPCA}
for(m in 1:M){
  this_df <- pred_list_all[[m]]
  auc_tb <- this_df %>% 
    mutate(window = cut(t, breaks = window, include.lowest = T)) %>% 
    select(Y, starts_with("pred"), window) %>%
    group_by(window) %>%
    summarise(auc1 = get_auc(Y, pred0.2),
              auc2 = get_auc(Y, pred0.4),
              auc3 = get_auc(Y, pred0.6),
              auc4 = get_auc(Y, pred0.8)) %>%
    filter(window != "[0,0.2]") %>% 
    select(starts_with("auc"))
  auc_mat[, ,m] <- as.matrix(auc_tb)

}


mean_auc <- apply(auc_mat, c(1, 2), mean)
# mean_auc <- data.frame(mean_auc) %>% 
#   mutate(Window = c("(0.2, 0.4]", "(0.4, 0.6]", "(0.6, 0.8]", "(0.8, 1.0]"),
#          .before = 1)
colnames(mean_auc) <- c("0.2", "0.4", "0.6", "0.8")
```

```{r auc_GLMMadaptive}
for(m in 1:M){
  this_df <- pred_list_ref[[m]]
  auc_tb <- this_df %>% 
    mutate(window = cut(t, breaks = window, include.lowest = T)) %>% 
    select(Y, starts_with("pred"), window) %>%
    group_by(window) %>%
    summarise(auc1 = get_auc(Y, pred0.2),
              auc2 = get_auc(Y, pred0.4),
              auc3 = get_auc(Y, pred0.6),
              auc4 = get_auc(Y, pred0.8)) %>%
    filter(window != "[0,0.2]") %>% 
    select(starts_with("auc"))
  auc_mat_ref[, ,m] <- as.matrix(auc_tb)

}


mean_auc_ref <- apply(auc_mat_ref, c(1, 2), mean)
# mean_auc <- data.frame(mean_auc) %>% 
#   mutate(Window = c("(0.2, 0.4]", "(0.4, 0.6]", "(0.6, 0.8]", "(0.8, 1.0]"),
#          .before = 1)
colnames(mean_auc_ref) <- c("0.2", "0.4", "0.6", "0.8")
```

```{r}
data.frame(Window = c("(0.2, 0.4]", "(0.4, 0.6]", "(0.6, 0.8]", "(0.8, 1.0]"),
           mean_auc, mean_auc_ref, check.names = F) %>%
  kable(digits = 3, table.attr = "style = \"color: black;\"", caption = "Area under the ROC curve") %>%
  kable_styling(full_width = F) %>% 
  add_header_above(c(" "=1, "fGFPCA" = 4, "GLMMadaptive" = 4)) %>%
  add_header_above(c(" "= 1, "Maximum observation time" = 8))
```


```{r time}
data.frame(
  "Method" = c("fGFPCA", "GLMMadaptive"),
  "Fit" = c(mean(fit_time)/60, mean(fit_time_ref)),
           "Prediction"= c(mean(pred_time), mean(pred_time_ref)/60)) %>%
  kable(digits = 3, table.attr = "style = \"color: black;\"", caption = "Computation time (minutes)") %>%
  kable_styling(full_width = F) 
```


I think we could say that while the total time spend on model fitting + prediction are similar between two methods, fGFPCA achieved much better flexibility and much better predictive performance of prediction under every scenario. 


<!-- # Small-scale simulation  -->

<!-- # NHANES data application -->