---
title: "Manuscript progress report"
author: "Ying Jin"
date: "`r Sys.Date()`"
output:
  html_document:
    self_contained: no
    number_sections: yes
    code_folding: hide
    toc: yes
    toc_depth: 3
    toc_float: yes
    font: 12pt
    keep_tex: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
    keep_tex: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(knitr.kable.NA = "")

set.seed(1120)


library(here)
library(tidyverse)
library(ggpubr)
library(refund)
library(lme4)
library(ROCR)
library(GLMMadaptive)
library(kableExtra)
library(knitr)
library(mvtnorm)
library(mgcv)
library(splines)
library(boot)
theme_set(theme_minimal())

load(here("Data/SimOutput_fGFPCA.RData"))
load(here("Data/SimOutput_GLMMadaptive.RData"))

## prediction window 
window <- seq(0, 1, by = 0.2)
M <- length(pred_list_all)
```


# Method

## Assumptions

- For each subject i in the population, a generalized outcome $Y_i(t)$ is generated along a variable t (for example, time), where $t \in (0, T)$. 
- The outcome, at any specific t, follows an exponential family distribution characterized by a (latent) continuous function $\eta_i(t)$: 

$$g[E(Y_i(t))] = \eta_i(t) = \beta_0(t)+b_i(t)$$

$$p(Y_i(t)) = h(Y_i(t))exp\{\eta_i(t)T[Y_i(t)]-A(\eta_i(t))\}$$

- The continuous latent function consists of a population-level fixed process and an individual-level random process

$$\eta_i(t) = \beta_0(t)+b_i(t)$$

## Observed data

In practice we would observe the discrete realization of $\{Y_i(t), t\}$ along a dense grid. For simplicity, we assume the observation grid is regular (same across sample). When we have J observations points in $(0, T]$, then for the jth observation point, we denote the corresponding value of t as $t_j$, and the corresponding outcome at this point $Y_i(t_j)$. 


## fGFPCA Algorithm

### Bin data: 

Choose a proper bin width $w$ considering model complexity and identifiability. For now let's say the bins are equal-length and non-overlapping. 

- Bin index $s = 1...S$
- Index of bin midpoints $m_s$
- Value of t corresponding to bin midpoints $t_{m_s}$
- Bin endpoints: $(t_{m_s}-\frac{w}{2}, t_{m_s}+\frac{w}{2}]$

```{r, eval=FALSE}
grid_exp <- data.frame(t = seq(0, 1, by = 0.01), y = 0)
ggplot(grid_exp, aes(x=t, y=y))+
  geom_point(size = 0.5)
```

### Local GLMMs

At the every bin, we fit a local intercept-only model:

$$g[E(Y_i(t_j))] =\eta_i(t_{m_s})= \beta_0(t_{m_s})+b_i(t_{m_s})$$
where $t_j \in (t_{m_s}-\frac{w}{2}, t_{m_s}+\frac{w}{2}]$.

Here we are basically saying that the value of latent function is constant within the same bin, which clearly is a misspecification of the true latent process.

From the model above. we will be able to estimate a $\hat{\eta_i}(t_{m_s})$ on the binned grid for every individual in the training sample. 

### FPCA 

Here, we fit a FPCA model on the $\hat{\eta_i}(t_{m_s})$ obtained from step 2:

$$\hat{\eta}_i(t_{m_s}) = f_0(t_{m_s})+\sum_{k=1}^K\xi_{ik}\phi_{k}(t_{m_s})+\epsilon_i(t_{m_s})$$

where $\xi_{ik}$ independently follows normal distribution $N(0, \lambda_k)$, and $\epsilon_i(t_{m_s})$ at each point follows $N(0, \sigma_2)$.

From this model, we will be able to obtain the following estimates which are shared across population: 

- Population mean $\hat{f_0}(t_{m_s})$
- Basis functions $\hat{\mathbf{\Phi}} = \{\hat{\phi}_1(t_{m_s}), ...,\hat{\phi}_K(t_{m_s}))\}$
- Estimates of variance of scores $\hat{\lambda}_1...\hat{\lambda}_K$

### Projection and Debias

The mean and basis functions are evaluated on the binned grid. To extend it to the original measurement grid data was collected on, we project the estimated eigenfunctions $\hat{\mathbf{\Phi}}$ back use spline basis. Now we have extend the $\hat{\phi}_k(t_{m_s})$ to the original grid $\hat{\phi}_k(t_j)$

Because of the misspecification of local GLMMs, the estimated eigenfunctions and eigenvalues are also biased by a constant multiplicative effect. Therefore, we use a GLMM to re-evaluate the mean function, eigenfunctions and eigenvalues. 

## Out-of-sample prediction 

Now, let's assume we have a new subject $u$ with $J_u$ observations ($J_u < J$). Then the log-likelihood of this new subject would be:


$$l_u=\sum_{t_j<t_{J_u}}log(h(Y_u(t_j)))+\hat{\eta}_u(t_j)T(Y_u(t_j))-log(A[\hat{\eta}_u(t_j)])$$

where $\hat{\eta}_u(t_j) = \hat{f}_0(t_j)+\sum_{k=1}^K \xi_{uk}\hat{\phi}(t_j)$. 

With estimates for the population-level parameters from fGFPCA algorithms above, we can estimate $\xi_{uk}$ by maximization of $l_u$. Direct maximization some times does not have closed form solution. Numeric maximization methods seem not very stable as well. So I have decided to used a Bayes approach (Laplace Approximation):

- Prior distribution: $\xi_{uk} \sim N(0, \hat{\lambda}_k)$
- Posterior distribution: the likelihood of $l_u =l(Y_u(t_j)|\mathbf{\xi}_u)$

Laplace Approximation would get the posterior mode of $\xi_{uk}$ through quadratic approximation.


# Larger-scale simulation

## Simulation set up

Here we simulate binary data from cyclic latent process:


\[\begin{aligned}
Y_i(t) & \sim Bernoulli(\frac{exp(\eta_i(t))}{1+exp(\eta_i(t))}) \\
\eta_i(t) &= f_0(t)+ \xi_{i1}\sqrt{2}sin(2\pi t)+\xi_{i2}\sqrt{2}cos(2\pi t)+\xi_{i3}\sqrt{2}sin(4\pi t)+\xi_{i4}\sqrt{2}cos(4\pi t)
\end{aligned}\]

where:

- $t$ is 1000 equal-spaced observations points on $[0, 1]$ (J = 1000).
- $f_0(t)=0$
- $\xi_k \sim N(0, \lambda_k)$, and $\lambda_k = 1, 0.5, 0.25, 0.125$ for k = 1, 2, 3, 4 respectively. 
- Sample size $N = 500$
- In the binning step, we bin every 10 observations
- 500 simulations were implemented


```{r fig_sim_data}
rand_id <- sample(501:600, size = 4)
pred_list_all[[1]] %>% filter(id %in% rand_id) %>%
  mutate_at(vars(eta_i), function(x){exp(x)/(1+exp(x))}) %>%
  ggplot()+
  geom_point(aes(x=t, y=Y), size = 0.2)+
  geom_line(aes(x=t, y=eta_i, col = "True"), show.legend = F)+
  facet_wrap(~id)+
  labs(title = "Simulated data")+
  scale_x_continuous(breaks = seq(0, 1, by = 0.2))
```


## Reference method


- GLMMadaptive
- Here we can fit a model with random intercept and slope for time. It is doable on 500 datasets, but obviously too simple for the data generation scheme. We would expect it to perform terribly.

$$g(E(Y_i(t))) = \beta_0+\beta_1t+b_{i0}+b_{i1}t$$

## Figure


```{r}
# range(pred_list_all[[1]]$pred0.2, na.rm = T)
fig_fgfpca <- pred_list_all[[1]] %>%
  filter(id %in% rand_id) %>%
  mutate_at(vars(eta_i, pred0.2, pred0.4, pred0.6, pred0.8), function(x){exp(x)/(1+exp(x))}) %>%
  ggplot()+
  geom_point(aes(x=t, y=Y), size = 0.2)+
  geom_line(aes(x=t, y=eta_i, col = "True"))+
  geom_line(aes(x=t, y=pred0.2, col = "0.2"), linetype="dashed", na.rm=T)+
  geom_line(aes(x=t, y=pred0.4, col = "0.4"), linetype="dashed", na.rm=T)+
  geom_line(aes(x=t, y=pred0.6, col = "0.6"), linetype="dashed", na.rm=T)+
  geom_line(aes(x=t, y=pred0.8, col = "0.8"), linetype="dashed", na.rm=T)+
  facet_wrap(~id)+
  labs(title = "fGFPCA", col = "Maximum observation time")+
  scale_x_continuous(breaks = seq(0, 1, by = 0.2))+
  theme(axis.text = element_text(size = 5))

fig_adglmm <- pred_list_ref[[1]] %>%
  filter(id %in% rand_id) %>%
  mutate_at(vars(eta_i, pred0.2, pred0.4, pred0.6, pred0.8), function(x){exp(x)/(1+exp(x))}) %>%
  ggplot()+
  geom_point(aes(x=t, y=Y), size = 0.2)+
  geom_line(aes(x=t, y=eta_i, col = "True"))+
  geom_line(aes(x=t, y=pred0.2, col = "0.2"), linetype="dashed", na.rm=T)+
  geom_line(aes(x=t, y=pred0.4, col = "0.4"), linetype="dashed", na.rm=T)+
  geom_line(aes(x=t, y=pred0.6, col = "0.6"), linetype="dashed", na.rm=T)+
  geom_line(aes(x=t, y=pred0.8, col = "0.8"), linetype="dashed", na.rm=T)+
  facet_wrap(~id)+
  labs(title = "GLMMadaptive", col = "Maximum observation time")+
  scale_x_continuous(breaks = seq(0, 1, by = 0.2))+
  theme(axis.text = element_text(size = 5))

```

```{r fig_sim, fig.height=4, fig.width=8}
ggarrange(fig_fgfpca, fig_adglmm,
          nrow = 1, common.legend = T)
```

## ISE

```{r}
## ISE container 
ise_mat <- ise_mat_ref <- array(NA, 
                                dim = c(length(window)-2, length(window)-2, M))
# dims: prediction window, max obs time, simulation iter

```

```{r ise_fGFPCA}
## calculation
for(m in 1:M){
  this_df <- pred_list_all[[m]]
  ise_tb <- pred_list_all[[m]] %>%
    mutate(err1 = (pred0.2-eta_i)^2,
           err2 = (pred0.4-eta_i)^2,
           err3 = (pred0.6-eta_i)^2,
           err4 = (pred0.8-eta_i)^2) %>%
    select(id, t, starts_with("err")) %>% 
    mutate(window = cut(t, breaks = window, include.lowest = T)) %>% 
    group_by(window, id) %>% 
    summarise_at(vars(err1, err2, err3, err4), sum) %>% 
    group_by(window) %>% 
    summarize_at(vars(err1, err2, err3, err4), mean) %>%
    filter(window != "[0,0.2]") %>% 
    select(starts_with("err"))
  ise_mat[, ,m] <- as.matrix(ise_tb)
  
  
}

mean_ise <- apply(ise_mat, c(1, 2), mean)

# mean_ise <- data.frame(mean_ise) %>%
#   mutate(Window = c("(0.2, 0.4]", "(0.4, 0.6]", "(0.6, 0.8]", "(0.8, 1.0]"),
#          .before = 1)
colnames(mean_ise) <- c("0.2", "0.4", "0.6", "0.8")
```

```{r ise_GLMMadaptive}
## calculation
for(m in 1:M){
  this_df <- pred_list_ref[[m]]
  ise_tb <- pred_list_ref[[m]] %>%
    mutate(err1 = (pred0.2-eta_i)^2,
           err2 = (pred0.4-eta_i)^2,
           err3 = (pred0.6-eta_i)^2,
           err4 = (pred0.8-eta_i)^2) %>%
    select(id, t, starts_with("err")) %>% 
    mutate(window = cut(t, breaks = window, include.lowest = T)) %>% 
    group_by(window, id) %>% 
    summarise_at(vars(err1, err2, err3, err4), sum) %>% 
    group_by(window) %>% 
    summarize_at(vars(err1, err2, err3, err4), mean) %>%
    filter(window != "[0,0.2]") %>% 
    select(starts_with("err"))
  ise_mat_ref[, ,m] <- as.matrix(ise_tb)
  
  
}

mean_ise_ref <- apply(ise_mat_ref, c(1, 2), mean)
# mean_ise_ref <- data.frame(mean_ise_ref) %>% 
#   mutate(Window = c("(0.2, 0.4]", "(0.4, 0.6]", "(0.6, 0.8]", "(0.8, 1.0]"),
#          .before = 1)
colnames(mean_ise_ref) <- c("0.2", "0.4", "0.6", "0.8")
```


```{r}
data.frame(Window = c("(0.2, 0.4]", "(0.4, 0.6]", "(0.6, 0.8]", "(0.8, 1.0]"),
           mean_ise, mean_ise_ref, check.names = F) %>%
  kable(digits = 3, caption = "Integrated squared error", booktabs=T) %>%
  kable_styling(full_width = F) %>% 
  add_header_above(c(" "=1, "fGFPCA" = 4, "GLMMadaptive" = 4)) %>%
  add_header_above(c(" "= 1, "Maximum observation time" = 8))
```


## AUC

```{r auc_func}
## a function to calculate AUC
get_auc <- function(y, pred){
  if(sum(is.na(y))>0 | sum(is.na(pred))>0){
    auc <- NA
  }
  else{
    this_perf <- performance(prediction(pred, y), measure = "auc")
    auc <- this_perf@y.values[[1]]
  }
  return(auc)
}

```


```{r}
## auc container 
auc_mat <- array(NA, dim = c(length(window)-2, length(window)-2, M))
auc_mat_ref <- array(NA, dim = c(length(window)-2, length(window)-2, M))
```


```{r auc_fGFPCA}
for(m in 1:M){
  this_df <- pred_list_all[[m]]
  auc_tb <- this_df %>% 
    mutate(window = cut(t, breaks = window, include.lowest = T)) %>% 
    select(Y, starts_with("pred"), window) %>%
    group_by(window) %>%
    summarise(auc1 = get_auc(Y, pred0.2),
              auc2 = get_auc(Y, pred0.4),
              auc3 = get_auc(Y, pred0.6),
              auc4 = get_auc(Y, pred0.8)) %>%
    filter(window != "[0,0.2]") %>% 
    select(starts_with("auc"))
  auc_mat[, ,m] <- as.matrix(auc_tb)

}


mean_auc <- apply(auc_mat, c(1, 2), mean)
# mean_auc <- data.frame(mean_auc) %>% 
#   mutate(Window = c("(0.2, 0.4]", "(0.4, 0.6]", "(0.6, 0.8]", "(0.8, 1.0]"),
#          .before = 1)
colnames(mean_auc) <- c("0.2", "0.4", "0.6", "0.8")
```

```{r auc_GLMMadaptive}
for(m in 1:M){
  this_df <- pred_list_ref[[m]]
  auc_tb <- this_df %>% 
    mutate(window = cut(t, breaks = window, include.lowest = T)) %>% 
    select(Y, starts_with("pred"), window) %>%
    group_by(window) %>%
    summarise(auc1 = get_auc(Y, pred0.2),
              auc2 = get_auc(Y, pred0.4),
              auc3 = get_auc(Y, pred0.6),
              auc4 = get_auc(Y, pred0.8)) %>%
    filter(window != "[0,0.2]") %>% 
    select(starts_with("auc"))
  auc_mat_ref[, ,m] <- as.matrix(auc_tb)

}


mean_auc_ref <- apply(auc_mat_ref, c(1, 2), mean)
# mean_auc <- data.frame(mean_auc) %>% 
#   mutate(Window = c("(0.2, 0.4]", "(0.4, 0.6]", "(0.6, 0.8]", "(0.8, 1.0]"),
#          .before = 1)
colnames(mean_auc_ref) <- c("0.2", "0.4", "0.6", "0.8")
```

```{r}
data.frame(Window = c("(0.2, 0.4]", "(0.4, 0.6]", "(0.6, 0.8]", "(0.8, 1.0]"),
           mean_auc, mean_auc_ref, check.names = F) %>%
  kable(digits = 3, caption = "Area under the ROC curve", booktabs = T) %>%
  kable_styling(full_width = F) %>% 
  add_header_above(c(" "=1, "fGFPCA" = 4, "GLMMadaptive" = 4)) %>%
  add_header_above(c(" "= 1, "Maximum observation time" = 8))
```


```{r time}
data.frame(
  "Method" = c("fGFPCA", "GLMMadaptive"),
  "Fit" = c(mean(fit_time)/60, mean(fit_time_ref)),
           "Prediction"= c(mean(pred_time), mean(pred_time_ref)/60)) %>%
  kable(digits = 3, caption = "Computation time (minutes)", booktabs = T) %>%
  kable_styling(full_width = F) 
```


I think we could say that while the total time spend on model fitting + prediction are similar between two methods, fGFPCA achieved much better flexibility and much better predictive performance of prediction under every scenario. 


# Small-scale simulation

Here we would like to fit fGFPCA and GLMMadaptive on a dataset with smaller sample size and/or smaller measurement density. For the GLMMadaptive model, we would set it up with spline basis functions so that its flexibility is comparable with fGFPCA model, such as: 

$$g(E(Y_i(t))) = \sum_{k=1}^4\zeta_{k}B_k(t)+\sum_{l=1}^4\xi_{il}\phi_l(t)$$

I have used 100 subjects for training and testing, and repeated 100 times. When fitting GLMMadpative, I reduce the number of measurements to 1/10 by taking one every 10 observations. The prediction is on the original grid. 


```{r load_subset_sim_output}
load(here("Data/SubSimOutput_GLMMadaptive.RData"))
load(here("Data/SubSimOutput_fGFPCA.RData"))
```

## Figure


```{r fig_pred_subset}
rand_id2 <- sample(unique(pred_subset_fGFPCA[[1]]$id), 4)

fig_fgfpca_subset <- pred_subset_fGFPCA[[1]] %>%
  filter(id %in% rand_id2) %>%
  mutate_at(vars(eta_i, starts_with("pred")), function(x){exp(x)/(1+exp(x))}) %>%
  ggplot()+
  geom_point(aes(x=t, y=Y), size = 0.2)+
  geom_line(aes(x=t, y=eta_i, col = "True"))+
  geom_line(aes(x=t, y=pred0.2, col = "0.2"), linetype="dashed", na.rm=T)+
  geom_line(aes(x=t, y=pred0.4, col = "0.4"), linetype="dashed", na.rm=T)+
  geom_line(aes(x=t, y=pred0.6, col = "0.6"), linetype="dashed", na.rm=T)+
  geom_line(aes(x=t, y=pred0.8, col = "0.8"), linetype="dashed", na.rm=T)+
  facet_wrap(~id)+
  labs(title = "fGFPCA", col = "Maximum observation time")+
  scale_x_continuous(breaks = seq(0, 1, by = 0.2))+
  theme(axis.text = element_text(size = 5))

fig_adglmm_subset <- pred_subset_adglmm[[1]] %>% 
  filter(id %in% rand_id2) %>%
  mutate_at(vars(eta_i, starts_with("pred")), function(x){exp(x)/(1+exp(x))}) %>%
  ggplot()+
  geom_point(aes(x=t, y=Y), size = 0.2)+
  geom_line(aes(x=t, y=eta_i, col = "True"))+
  geom_line(aes(x=t, y=pred0.2, col = "0.2"), linetype="dashed", na.rm=T)+
  geom_line(aes(x=t, y=pred0.4, col = "0.4"), linetype="dashed", na.rm=T)+
  geom_line(aes(x=t, y=pred0.6, col = "0.6"), linetype="dashed", na.rm=T)+
  geom_line(aes(x=t, y=pred0.8, col = "0.8"), linetype="dashed", na.rm=T)+
  facet_wrap(~id)+
  labs(title = "GLMMadaptive", col = "Maximum observation time")+
  scale_x_continuous(breaks = seq(0, 1, by = 0.2))+
  theme(axis.text = element_text(size = 5))
```

```{r fit_sim_small, fig.height=4, fig.width=8}
ggarrange(fig_fgfpca_subset, fig_adglmm_subset,
          nrow = 1, common.legend = T)
```

## ISE


```{r}
# M2 <- length(pred_subset_fGFPCA)
## ISE container 
ise_mat_subset <- array(NA, dim = c(length(window)-2, length(window)-2, length(pred_subset_fGFPCA)))

pred_subset_adglmm <- pred_subset_adglmm[num_probs==0]
ise_mat_subset_ref <- array(NA, dim = c(length(window)-2, length(window)-2, length(pred_subset_adglmm)))
 
```

```{r ise_fGFPCA_subset}
for(m in 1:length(pred_subset_fGFPCA)){
  this_df <- pred_subset_fGFPCA[[m]]
  ise_tb_m <- this_df %>%
    mutate(err1 = (pred0.2-eta_i)^2,
           err2 = (pred0.4-eta_i)^2,
           err3 = (pred0.6-eta_i)^2,
           err4 = (pred0.8-eta_i)^2) %>%
    select(id, t, starts_with("err")) %>% 
    mutate(window = cut(t, breaks = window, include.lowest = T)) %>% 
    group_by(window, id) %>% 
    summarise_at(vars(err1, err2, err3, err4), sum) %>% 
    group_by(window) %>% 
    summarize_at(vars(err1, err2, err3, err4), mean) %>%
    filter(window != "[0,0.2]") %>% 
    select(starts_with("err")) %>% as.matrix()
  ise_mat_subset[,,m] <- ise_tb_m
}

ise_mat_subset <- apply(ise_mat_subset, c(1, 2), mean)

colnames(ise_mat_subset) <- c("0.2", "0.4", "0.6", "0.8")
```

```{r ise_GLMMadaptive_subset}
for(m in 1:length(pred_subset_adglmm)){
  this_df <- pred_subset_adglmm[[m]]
  ise_tb_m <- this_df %>%
    mutate(err1 = (pred0.2-eta_i)^2,
           err2 = (pred0.4-eta_i)^2,
           err3 = (pred0.6-eta_i)^2,
           err4 = (pred0.8-eta_i)^2) %>%
    select(id, t, starts_with("err")) %>% 
    mutate(window = cut(t, breaks = window, include.lowest = T)) %>% 
    group_by(window, id) %>% 
    summarise_at(vars(err1, err2, err3, err4), sum) %>% 
    group_by(window) %>% 
    summarize_at(vars(err1, err2, err3, err4), mean) %>%
    filter(window != "[0,0.2]") %>% 
    select(starts_with("err")) %>% as.matrix()
  ise_mat_subset_ref[,,m] <- ise_tb_m
}

ise_mat_subset_ref <- apply(ise_mat_subset_ref, c(1, 2), mean)


colnames(ise_mat_subset_ref) <- c("0.2", "0.4", "0.6", "0.8")
```


```{r}
data.frame(Window = c("(0.2,0.4]", "(0.4,0.6]", "(0.6,0.8]", "(0.8, 1.0]"),
           ise_mat_subset, ise_mat_subset_ref, check.names = F) %>%
  kable(digits = 3, caption = "Integrated squared error", booktabs = T) %>%
  kable_styling(full_width = F) %>% 
  add_header_above(c(" "=1, "fGFPCA" = 4, "GLMMadaptive" = 4)) %>%
  add_header_above(c(" "= 1, "Observation track" = 8))
```


## AUC

```{r}
## auc container 
auc_mat_subset <- array(NA, dim = c(length(window)-2, length(window)-2, 
                                    length(pred_subset_fGFPCA)))
auc_mat_subset_ref <- array(NA, dim = c(length(window)-2, length(window)-2, 
                                        length(pred_subset_adglmm)))
```


```{r auc_fGFPCA_subset}
for(m in 1:length(pred_subset_fGFPCA)){
  this_df <- pred_subset_fGFPCA[[m]]
  
  auc_tb_m <- this_df %>%
  mutate(window = cut(t, breaks = window, include.lowest = T)) %>% 
  select(Y, starts_with("pred"), window) %>%
  group_by(window) %>%
  summarise(auc1 = get_auc(Y, pred0.2),
            auc2 = get_auc(Y, pred0.4),
            auc3 = get_auc(Y, pred0.6),
            auc4 = get_auc(Y, pred0.8)) %>%
  filter(window != "[0,0.2]") %>% 
  select(starts_with("auc")) %>% as.matrix()
  
  auc_mat_subset[, , m] <- auc_tb_m
  
}


auc_mat_subset <- apply(auc_mat_subset, c(1,2), mean)
colnames(auc_mat_subset) <- c("0.2", "0.4", "0.6", "0.8")
```

```{r auc_GLMMadaptive_subset}
for(m in 1:length(pred_subset_adglmm)){
  this_df <- pred_subset_adglmm[[m]]
  
  auc_tb_m <- this_df %>%
  mutate(window = cut(t, breaks = window, include.lowest = T)) %>% 
  select(Y, starts_with("pred"), window) %>%
  group_by(window) %>%
  summarise(auc1 = get_auc(Y, pred0.2),
            auc2 = get_auc(Y, pred0.4),
            auc3 = get_auc(Y, pred0.6),
            auc4 = get_auc(Y, pred0.8)) %>%
  filter(window != "[0,0.2]") %>% 
  select(starts_with("auc")) %>% as.matrix()
  
  auc_mat_subset_ref[, , m] <- auc_tb_m
  
}

auc_mat_subset_ref <- apply(auc_mat_subset_ref, c(1,2), mean)
colnames(auc_mat_subset_ref) <- c("0.2", "0.4", "0.6", "0.8")
```

```{r}
data.frame(Window = c("(0.2,0.4]", "(0.4,0.6]", "(0.6,0.8]", "(0.8, 1.0]"),
           auc_mat_subset, auc_mat_subset_ref, check.names = F) %>%
  kable(digits = 3, caption = "Area under the ROC curve", booktabs = T) %>%
  kable_styling(full_width = F) %>% 
  add_header_above(c(" "=1, "fGFPCA" = 4, "GLMMadaptive" = 4)) %>%
  add_header_above(c(" "= 1, "Observation track" = 8))
```

Among 500 hundred iterations, `r sum(num_probs)` did not converge for the GLMMadaptive model. No numeric issue for fGFPCA. 


```{r}
data.frame(
  "Method" = c("fGFPCA", "GLMMadaptive"),
  "Fit" = c(mean(fit_time_subset_fGFPCA)/60, mean(fit_time_subset_adglmm, na.rm=T)),
           "Prediction"= c(mean(pred_time_subset_fGFPCA), mean(pred_time_subset_adglmm, na.rm=T)/60)) %>%
  kable(digits = 3, caption = "Computation time (minutes)", booktabs = T) %>%
  kable_styling(full_width = F) 
```


# NHANES data application

We take 500 subjects for training, 500 for out-of-sample prediction. When using the full sample The debiase step (mgcv::bam) in fGFPCA took a really a long time, and GLMM adaptive only works for intercept-only model. In the debias step, re-evaluated eigenvalues also lost their decreasing nature. 

In addition to reduce sample size, maybe we could just drop the debias step and only interpolate the eigenfunctions? 


```{r load_nhanes}
df_nhanes <- read_rds(here("Data/nhanes_bi.rds"))
load(here("Data/ApplOutput.RData"))
```

```{r nhance_example}
df_nhanes %>% select(SEQN, Z, sind) %>% 
  filter(SEQN %in% unique(nhanes_pred_adglmm$id) | SEQN %in% unique(nhanes_pred_fgfpca$id)) %>%
  mutate(Z = factor(Z, levels = 0:1, labels = c("Inactive", "Active"))) %>%
  mutate(SEQN = factor(SEQN)) %>%
  ggplot()+
  geom_tile(aes(x=sind, y = SEQN, fill = Z))+
  labs(x="Time", y="Subject", title = "Overview of NHANES binary activity indicator")+
  theme(axis.text.y = element_blank())+
  scale_x_continuous(breaks = seq(0, 1440, by = 480),
                     labels = c("Midnight", "8am", "4pm", "Midnight"))
```

## AUC

```{r auc_fGFPCA_nhanes}
window3 <- c(0, 480, 960, 1440)

auc_mat_nhanes <- nhanes_pred_fgfpca %>%
  mutate(window = cut(t, breaks = window3, include.lowest = T)) %>% 
  select(Y, starts_with("pred"), window) %>%
  group_by(window) %>%
  summarise(auc1 = get_auc(Y, pred480),
            auc2 = get_auc(Y, pred960)) %>%
  filter(window != "[0,480]") %>% 
  select(starts_with("auc"))

colnames(auc_mat_nhanes) <- c("8am", "4pm")
```

```{r auc_GLMMadaptive_nhanes}
auc_mat_nhanes2 <- nhanes_pred_adglmm %>%
  mutate(window = cut(t, breaks = window3, include.lowest = T)) %>% 
  select(Y, starts_with("pred"), window) %>%
  group_by(window) %>%
  summarise(auc1 = get_auc(Y, pred480),
            auc2 = get_auc(Y, pred960)) %>%
  filter(window != "[0,480]") %>% 
  select(starts_with("auc"))

colnames(auc_mat_nhanes2) <- c("8am", "4pm")
```

```{r}
data.frame(Window = c("8am-4pm", "4am-midnight"),
           auc_mat_nhanes, auc_mat_nhanes2, check.names = F) %>%
  kable(digits = 3, caption = "Area Under the ROC curve", booktabs = T) %>%
  kable_styling(full_width = F) %>% 
  add_header_above(c(" "=1, "fGFPCA" = 2, "GLMMadaptive" = 2)) %>%
  add_header_above(c(" "= 1, "Maximum observation time" = 4))
```

## Figure

```{r pred_nhanes, fig.height=4, fig.width=10}
rand_id3 <- sample(unique(nhanes_pred_fgfpca$id), 4)

ggarrange(
  nhanes_pred_fgfpca %>% 
    filter(id %in% rand_id3) %>%
    mutate_at(vars(starts_with("pred")), function(x)exp(x)/(1+exp(x))) %>% 
    ggplot()+
    geom_line(aes(x=t, y = pred480, col = "8am"), linetype = "dashed", na.rm = T)+
    geom_line(aes(x=t, y = pred960, col = "4pm"),linetype = "dashed", na.rm = T)+
    geom_point(aes(x=t, y = Y, col = "Outcome"), size = 0.2)+
    facet_wrap(~id)+
    labs(x = "Time", y = "fGFPCA")+
  scale_x_continuous(breaks = seq(0, 1440, by = 480), 
                     labels = c("Midnight", "8am", "4pm", "Midnight"))+
  theme(axis.text = element_text(size = 5)),
  
  nhanes_pred_adglmm %>% 
    filter(id %in% rand_id3) %>%
    mutate_at(vars(starts_with("pred")), function(x)exp(x)/(1+exp(x))) %>% 
    ggplot()+
    geom_line(aes(x=t, y = pred480, col = "8am"), linetype = "dashed", na.rm = T)+
    geom_line(aes(x=t, y = pred960, col = "4pm"),linetype = "dashed", na.rm = T)+
    geom_point(aes(x=t, y = Y, col = "Outcome"), size = 0.2)+
    facet_wrap(~id)+
    labs(x = "Time", y = "GLMMadaptive")+
  scale_x_continuous(breaks = seq(0, 1440, by = 480), 
                     labels = c("Midnight", "8am", "4pm", "Midnight"))+
  theme(axis.text = element_text(size = 5)),
  nrow = 1, common.legend = T)
```

# The second reference method:

```{r}
load(here("Data/sim_data.RData"))
df_m <- sim_data[[1]]
```

We would like to use a second reference method for predictive performance comparision. In this model, predictions on a specific interval are all made using the last few observations in the observed window. For example, if we are given observations on [0, 0.2] (j = 1...200), we may use L observations taken right before t=0.2 ($t_j: j=200, ... 200-(L-1)$) as time-fixed covariate to predict any future time. 

Let's write out the model expression (with questionable notation). If the observations if up to $t_m$, then: 

\[\begin{aligned}
g(E[Y_i(t)]) &= \beta_0(t) +\sum_{l=1}^L \beta_l (t)Y_i(t_l)\\
l & = m,...m-(j-1)\\
t &> t_m 
\end{aligned}\]

This is a simple function-on-scalar model with no random effect, meaning all subject with the same last observed outcome would have the same estimated/predicted latent track. This is clearly anti-intuitive. But adding random effects would make out-of-sample prediction impossible. 

Under this framework, for each dataset we need to fit four models (similar to the GLMMadaptive method): 

- Given 0-0.2, predict 0.2-1
- Given 0-0.4, predict 0.4-1
- Given 0-0.6, predict 0.6-1
- Given 0-0.8, predict 0.8-1

Below are some single dataset examples.

## Example of L = 1, given [0, 0.2] to predict (0.2, 1)

First let try to make prediction on an interval with only one historical observation (L = 1):

\[\begin{aligned}
g(E[Y_i(t)]) &= \beta_0(t) +\beta_1(t) Y_i(t_{m})\\
t &> t_m 
\end{aligned}\]

```{r data_format}
L <- 1 # number of observations to use as predictor

# prediction window 
windows <- seq(0, 1, by = 0.2)
df_m$window <- cut(df_m$t, breaks=windows, labels = 1:5,
                       include.lowest = T)

# split data
N_train <- 500
N_test <- 100
train_df <- df_m %>% filter(id %in% 1:N_train)
test_df <- df_m %>% filter(!id %in% 1:N_train)
```


```{r fit_model, class.source='fold-show'}
# format training data
## given [0, 0.2], predictor would be
y_obs_max <- train_df %>% filter(window==1) %>% arrange(desc(t)) %>%
  group_by(id) %>%
  slice(L) %>% select(id, Y) %>% rename(yl = Y)
## to predict (0.2, 1), outcome:
df_pred_tr <- train_df %>% filter(window!=1) %>%
  left_join(y_obs_max, by = "id") %>% 
  mutate_at(vars(Y, yl), as.factor)

# range(df_pred_tr$sind)

# model
t1 <- Sys.time()
fit_gen_fosr <- bam(Y ~ s(t, bs="cc", k=20) + 
                        s(t, bs="cc", k=20, by = yl),
                   family = binomial, data=df_pred_tr, 
                   method = "fREML",
                   discrete = TRUE)
t2 <- Sys.time()
t2-t1

summary(fit_gen_fosr)
```

```{r est_train}
# estimation on the training set
df_pred_tr$eta_est <- predict(fit_gen_fosr, type = "link")

df_pred_tr %>%
  mutate_at(vars(eta_est, eta_i), function(x){exp(x)/(1+exp(x))}) %>%
  mutate_at(vars(Y), function(x){as.numeric(as.character(x))}) %>%
  filter(id %in% 1:4) %>%
  ggplot()+
  geom_point(aes(x=t, y=Y), size = 0.5)+
  geom_line(aes(x=t, y=eta_est, col = "estimate"))+
  geom_line(aes(x=t, y=eta_i, col = "ture"))+
  facet_wrap(~id)+
  labs(title = "In-sample estimation")
```

```{r pred_test}
# format testing data
df_pred_te <- test_df %>% filter(window!=1) %>%
  left_join(test_df %>% 
              filter(window==1) %>% arrange(desc(t)) %>% group_by(id) %>%
              slice(L) %>% select(id, Y) %>% rename(yl = Y),
            by = "id") %>% 
  mutate_at(vars(Y, yl), as.factor)

# predict using the FOSR model fit above
df_pred_te$eta_est <- predict(fit_gen_fosr, newdata = df_pred_te, type = "link")

df_pred_te %>%
  mutate_at(vars(eta_est, eta_i), function(x){exp(x)/(1+exp(x))}) %>%
  mutate_at(vars(Y), function(x){as.numeric(as.character(x))}) %>%
  filter(id %in% rand_id) %>%
  ggplot()+
  geom_point(aes(x=t, y=Y), size = 0.5)+
  geom_line(aes(x=t, y=eta_est, col = "estimate"))+
  geom_line(aes(x=t, y=eta_i, col = "ture"))+
  facet_wrap(~id)+
  labs(title = "Out-of-sample prediction")

```

## Example of prediction on one dataset (L = 1)

```{r}
# table(df_m$window)
train_df$window <- as.numeric(as.character(train_df$window))
test_df$window <- as.numeric(as.character(test_df$window))
test_df2 <- test_df

t1 <- Sys.time()
for(w in 1:4){
  # model fit
  # predictor (training set)
  y_obs_max <- train_df %>% filter(window==w) %>% arrange(desc(t)) %>%
    group_by(id) %>%
    slice(L) %>% select(id, Y) %>% rename(yl = Y)
  # outcome (training set)
  df_pred_tr <- train_df %>% filter(window > w) %>%
    left_join(y_obs_max, by = "id") %>% 
    mutate_at(vars(Y, yl), as.factor)
  # model 
  fit_gen_fosr <- bam(Y ~ s(t, bs="cc", k=20) + 
                        s(t, bs="cc", k=20, by = yl),
                   family = binomial, data=df_pred_tr, 
                   method = "fREML",
                   discrete = TRUE)
  
  # prediction
  # predictor (testing set)
  y_obs_max_te <- test_df %>% 
                filter(window==w) %>% arrange(desc(t)) %>% group_by(id) %>%
                slice(L) %>% select(id, Y) %>% rename(yl = Y)
  # outcome (testing set)
  df_pred_te <- test_df %>% filter(window > w) %>%
    left_join(y_obs_max_te, by = "id") %>% 
    mutate_at(vars(Y, yl), as.factor)

  # predict using the FOSR model fit above
  pred_name <- paste0("pred_w", w)
  test_df[, pred_name] <- NA
  test_df[test_df$window>w, pred_name] <- predict(fit_gen_fosr, newdata = df_pred_te, type = "link")
}
t2 <- Sys.time()
# t2-t1

```



```{r}
test_df %>%
  mutate_at(vars(eta_i, pred_w1, pred_w2, pred_w3, pred_w4), function(x){exp(x)/(1+exp(x))}) %>%
  mutate_at(vars(Y), function(x){as.numeric(as.character(x))}) %>%
  filter(id %in% rand_id) %>%
  ggplot()+
  geom_point(aes(x=t, y=Y), size = 0.5)+
  geom_line(aes(x=t, y=pred_w1, col = "Up to 0.1"), na.rm=T)+
  geom_line(aes(x=t, y=pred_w2, col = "Up to 0.2"), na.rm=T)+
  geom_line(aes(x=t, y=pred_w3, col = "Up to 0.3"), na.rm=T)+
  geom_line(aes(x=t, y=pred_w4, col = "Up to 0.4"), na.rm=T)+
  geom_line(aes(x=t, y=eta_i, col = "ture"))+
  facet_wrap(~id)+
  labs(title = "Out-of-sample prediction")
```



## Example of prediction on one dataset (L = 5)

```{r}
t1 <- Sys.time()
for(w in 1:4){
  # model fit
  # predictor (training set)
  y_obs_max <- train_df %>% filter(window==w) %>% group_by(id) %>%
    slice_max(t, n=5) %>% select(id, Y, sind) %>%
    mutate(name = sind-min(sind)+1) %>%
    pivot_wider(id_cols = id, values_from = Y, names_from = "name", names_prefix = "yl")
  # outcome (training set)
  df_pred_tr <- train_df %>% filter(window > w) %>%
    left_join(y_obs_max, by = "id") %>% 
    mutate_at(vars(Y, starts_with("yl")), as.factor) 
  fit_gen_fosr <- bam(Y ~ s(t, bs="cc", k=20) + 
                        s(t, bs="cc", k=20, by = yl5)+
                        s(t, bs="cc", k=20, by = yl4)+
                        s(t, bs="cc", k=20, by = yl3)+
                        s(t, bs="cc", k=20, by = yl2)+
                        s(t, bs="cc", k=20, by = yl1),
                   family = binomial, data=df_pred_tr, 
                   method = "fREML",
                   discrete = TRUE)
  
  # prediction
  # predictor (testing set)
  y_obs_max_te <- test_df2 %>% filter(window==w) %>% group_by(id) %>%
    slice_max(t, n=5) %>% select(id, Y, sind) %>% 
    mutate(name = sind-min(sind)+1) %>%
    pivot_wider(id_cols = id, values_from = Y, names_from = name, names_prefix = "yl")
  # outcome (testing set)
  df_pred_te <- test_df2 %>% filter(window > w) %>%
    left_join(y_obs_max_te, by = "id") %>% 
    mutate_at(vars(Y, starts_with("yl")), as.factor)

  # predict using the FOSR model fit above
  pred_name <- paste0("pred_w", w)
  test_df2[, pred_name] <- NA
  test_df2[test_df2$window>w, pred_name] <- predict(fit_gen_fosr, newdata = df_pred_te, type = "link")
}
t2 <- Sys.time()
# t2-t1

```



```{r}
test_df2 %>%
  mutate_at(vars(eta_i, pred_w1, pred_w2, pred_w3, pred_w4), function(x){exp(x)/(1+exp(x))}) %>%
  mutate_at(vars(Y), function(x){as.numeric(as.character(x))}) %>%
  filter(id %in% rand_id) %>%
  ggplot()+
  geom_point(aes(x=t, y=Y), size = 0.5)+
  geom_line(aes(x=t, y=pred_w1, col = "Up to 0.1"), na.rm=T)+
  geom_line(aes(x=t, y=pred_w2, col = "Up to 0.2"), na.rm=T)+
  geom_line(aes(x=t, y=pred_w3, col = "Up to 0.3"), na.rm=T)+
  geom_line(aes(x=t, y=pred_w4, col = "Up to 0.4"), na.rm=T)+
  geom_line(aes(x=t, y=eta_i, col = "ture"))+
  facet_wrap(~id)+
  labs(title = "Out-of-sample prediction")
```

This method is very, very fast, takes 10 seconds or so, much faster than both fGFPCA and GLMMadaptive. The performance lies in between them for most cases, sometimes worse than GLMMadaptive.


Do we really want to use the lag-1 model? 

```{r ise_fosr}
tb_ies_l5 <- test_df2 %>%
    mutate(err1 = (pred_w1-eta_i)^2,
           err2 = (pred_w2-eta_i)^2,
           err3 = (pred_w3-eta_i)^2,
           err4 = (pred_w4-eta_i)^2) %>%
    select(id, t, window, starts_with("err")) %>% 
    group_by(window, id) %>% 
    summarise_at(vars(err1, err2, err3, err4), sum) %>% 
    group_by(window) %>% 
    summarize_at(vars(err1, err2, err3, err4), mean) %>%
    filter(window != 1) %>%
  select(starts_with("err"))

tb_ies_l1 <- test_df %>%
    mutate(err1 = (pred_w1-eta_i)^2,
           err2 = (pred_w2-eta_i)^2,
           err3 = (pred_w3-eta_i)^2,
           err4 = (pred_w4-eta_i)^2) %>%
    select(id, t, window, starts_with("err")) %>% 
    group_by(window, id) %>% 
    summarise_at(vars(err1, err2, err3, err4), sum) %>% 
    group_by(window) %>% 
    summarize_at(vars(err1, err2, err3, err4), mean) %>%
    filter(window != 1) %>%
  select(starts_with("err"))

colnames(tb_ies_l1) <- colnames(tb_ies_l5) <- c("0.2", "0.4", "0.6", "0.8")
```


```{r}
data.frame(Window = c("(0.2,0.4]", "(0.4,0.6]", "(0.6,0.8]", "(0.8, 1.0]"),
           tb_ies_l1, tb_ies_l5, check.names = F) %>%
  kable(digits = 3, caption = "Integrated squared error", booktabs = T) %>%
  kable_styling(full_width = F) %>% 
  add_header_above(c(" "=1, "L = 1" = 4, "L = 5" = 4)) %>%
  add_header_above(c(" "= 1, "Observation track" = 8))
```




# Additional figures

<!-- ## fGFPCA algorithm demonstration -->

<!-- ```{r binning, eval=FALSE} -->
<!-- df_nhanes %>% select(SEQN, Z, sind) %>%  -->
<!--   filter(SEQN %in% rand_id3 & sind >= 650 & sind <= 750) %>%  -->
<!--   mutate(SEQN = as.factor(SEQN)) %>%  -->
<!--   ggplot()+ -->
<!--   geom_point(aes(x=sind, y = Z, col = SEQN), size = 0.5)+ -->
<!--   geom_vline(xintercept = seq(650, 750, by = 10), linetype="dashed")+ -->
<!--   scale_y_continuous(breaks = c(0, 1))+ -->
<!--   labs(x="Time", y="", title = "NHANES binary activity indicator") -->
<!-- ``` -->


<!-- ```{r,eval=FALSE} -->
<!-- df_fig <- df_nhanes %>% select(SEQN, Z, sind) %>%  -->
<!--   filter(SEQN %in% rand_id3 & sind >= 650 & sind <= 750) %>%  -->
<!--   mutate(bin = cut(sind, breaks = seq(650, 750, by = 10),  -->
<!--                    labels = seq(655, 745, by = 10), -->
<!--                    include.lowest = T))  -->
<!-- bin_mid <- seq(655, 745, by = 10) -->

<!-- for(i in bin_mid){ -->
<!--   df_bin_i <- df_fig %>% filter(bin==i)  -->
<!--   glm_i <- glmer(Z ~ 1 + (1|SEQN), data = df_bin_i, family = binomial, nAGQ=0) -->
<!--   eta_i <- predict(glm_i, type = "link") -->

<!--   df_fig$eta_hat[df_fig$bin==i] <- eta_i -->
<!-- } -->


<!-- df_fig %>%  -->
<!--   mutate(SEQN = as.factor(SEQN), -->
<!--          eta_hat = exp(eta_hat)/(1+exp(eta_hat)), -->
<!--          bin=as.numeric(as.character(bin))) %>%  -->

<!--   ggplot()+ -->
<!--   geom_point(aes(x=sind, y = Z, col = SEQN), size = 0.5)+ -->
<!--   geom_vline(xintercept = seq(650, 750, by = 10), linetype="dashed")+ -->
<!--   geom_point(aes(x=bin, y=eta_hat, col = SEQN))+ -->
<!--   geom_line(aes(x=bin, y=eta_hat, col = SEQN))+ -->
<!--   scale_y_continuous(breaks = c(0, 1))+ -->
<!--   labs(x="Time", y="", title = "NHANES binary activity indicator") -->

<!-- ``` -->


<!-- ## Example of continuous and binary NHANES -->

<!-- ```{r, eval=FALSE} -->
<!-- df_nhanes_cont <- readRDS(here("Data/NHANES_AC_processed.rds")) -->
<!-- p_cont <- df_nhanes_cont %>%  -->
<!--   filter(SEQN %in% sample(unique(df_nhanes_cont$SEQN), 4) & WEEKDAY==6) %>%  -->
<!--   select(starts_with("min"), SEQN) %>% -->
<!--   pivot_longer(1:1440, names_to = "Minute") %>%  -->
<!--   mutate(Minute = gsub("MIN", "", Minute)) %>% -->
<!--   mutate(Minute = as.numeric(Minute), -->
<!--          SEQN=as.factor(SEQN)) %>% -->
<!--   ggplot(aes(x=Minute, y=value, col=SEQN))+ -->
<!--   geom_point(size = 0.5, alpha=0.5)+ -->
<!--   geom_smooth(se=F, size = 0.5)+ -->
<!--   scale_x_continuous(breaks = seq(0, 1440, by = 480), -->
<!--                      labels = c("Midnight", "8am", "4pm", "Midnight"))+ -->
<!--   theme(axis.text = element_text(size = 5))+ -->
<!--   labs(y="", title = "NHANES Activitiy count (2003-2004)") -->
<!-- ``` -->


<!-- ```{r, eval=FALSE} -->
<!-- # a example of continuous and binary NHANES data -->
<!-- p_bi <- df_nhanes %>% select(SEQN, Z, sind) %>%  -->
<!--   filter(SEQN %in% rand_id3 & Z == 1) %>%  -->
<!--   mutate(SEQN = as.factor(SEQN)) %>%  -->
<!--   ggplot()+ -->
<!--   geom_point(aes(x=sind, y = SEQN), size = 0.5)+ -->
<!--   scale_x_continuous(breaks = seq(0, 1440, by = 480), -->
<!--                      labels = c("Midnight", "8am", "4pm", "Midnight"))+ -->
<!--   theme(axis.text = element_text(size = 5))+ -->
<!--   labs(x="Time", y="Subject", title = "NHANES binary activity indicator (2011-2014)") -->
<!-- ``` -->

<!-- ```{r data_exp, fig.height=4, fig.width=10, eval=FALSE} -->
<!-- ggarrange(p_cont, p_bi, nrow = 1) -->
<!-- ``` -->


<!-- ## Dynamic prediction example -->

<!-- ```{r dynpred_exp,  fig.height=4, fig.width=10, eval=FALSE} -->
<!-- ggarrange( -->
<!--   pred_list_all[[1]] %>%  -->
<!--     filter(id==501) %>% -->
<!--     mutate(eta_i = ifelse(t<=0.2, eta_i, NA)) %>% -->
<!--     ggplot()+ -->
<!--     geom_line(aes(x=t, y=eta_i, col = "Observed"), na.rm = T)+ -->
<!--     geom_line(aes(x=t, y=pred0.2, col = "Predicted"), na.rm = T,  -->
<!--               linetype = "dashed")+ -->
<!--     geom_vline(xintercept = 0.2, linetype = "dashed")+ -->
<!--     labs(x="Time", y="")+ -->
<!--     scale_x_continuous(breaks = seq(0, 1, by = 0.2))+ -->
<!--     theme(axis.text = element_text(size = 5))+ -->
<!--     ylim(-1.2, 1.7), -->

<!--  pred_list_all[[1]] %>%  -->
<!--     filter(id==501) %>% -->
<!--     mutate(eta_i = ifelse(t<=0.4, eta_i, NA)) %>% -->
<!--     ggplot()+ -->
<!--     geom_line(aes(x=t, y=eta_i, col = "Observed"), na.rm = T)+ -->
<!--     geom_line(aes(x=t, y=pred0.4, col = "Predicted"), na.rm = T,  -->
<!--               linetype = "dashed")+ -->
<!--     geom_vline(xintercept = 0.4, linetype = "dashed")+ -->
<!--     labs(x="Time", y="")+ -->
<!--     scale_x_continuous(breaks = seq(0, 1, by = 0.2))+ -->
<!--     theme(axis.text = element_text(size = 5))+ -->
<!--    ylim(-1.2, 1.7), -->
<!--  nrow = 1, common.legend = T -->
<!--   ) -->

<!-- ``` -->