<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Ying Jin" />

<meta name="date" content="2023-09-15" />

<title>Progress Report</title>

<script src="ProgressReport_files/header-attrs-2.21/header-attrs.js"></script>
<script src="ProgressReport_files/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="ProgressReport_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="ProgressReport_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="ProgressReport_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="ProgressReport_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="ProgressReport_files/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="ProgressReport_files/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="ProgressReport_files/tocify-1.9.1/jquery.tocify.js"></script>
<script src="ProgressReport_files/navigation-1.1/tabsets.js"></script>
<script src="ProgressReport_files/navigation-1.1/codefolding.js"></script>
<link href="ProgressReport_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="ProgressReport_files/highlightjs-9.12.0/highlight.js"></script>
<script src="ProgressReport_files/kePrint-0.0.1/kePrint.js"></script>
<link href="ProgressReport_files/lightable-0.0.1/lightable.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div id="header">

<div class="btn-group pull-right float-right">
<button type="button" class="btn btn-default btn-xs btn-secondary btn-sm dropdown-toggle" data-toggle="dropdown" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu dropdown-menu-right" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Progress Report</h1>
<h4 class="author">Ying Jin</h4>
<h4 class="date">2023-09-15</h4>

</div>


<div id="nhanes-data" class="section level1" number="1">
<h1><span class="header-section-number">1</span> NHANES data</h1>
<div id="data-overview" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> Data overview</h2>
<ul>
<li>8763 subjects, 1440 measures each</li>
<li>no missingness</li>
</ul>
<pre class="r"><code>N &lt;- length(unique(df$SEQN)) # sample size 8763
J &lt;- max(df$sind) # 1440 measures per subject</code></pre>
<pre class="r"><code>rand_id &lt;- sample(unique(df$SEQN), size = 4) # &quot;toy&quot; sample

df %&gt;% 
  filter(SEQN %in% rand_id) %&gt;%
  ggplot()+
  geom_point(aes(x=sind, y=Z), size = 0.5)+
  facet_wrap(~SEQN)+
  labs(x=&quot;Time&quot;, y = &quot;Activity&quot;, title = &quot;A brief overview of the outcome&quot;)</code></pre>
<p><img src="ProgressReport_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
</div>
<div id="fgfpca-model-fitting" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> fGFPCA model
fitting</h2>
<pre class="r"><code># code
source(here(&quot;Code/GLMM-FPCA.R&quot;)) 
# use pred_latent function to estimate latent function 
source(here(&quot;Code/OutSampMLE.R&quot;))
# source(here(&quot;Code/OutsampBayes.R&quot;))</code></pre>
<div id="bin-data" class="section level3" number="1.2.1">
<h3><span class="header-section-number">1.2.1</span> Bin Data</h3>
<ul>
<li>Bin every 10 consecutive observations</li>
</ul>
<pre class="r"><code># bin data
bin_w &lt;- 10 # bin width
n_bin &lt;- J/bin_w # number of bins
brks &lt;- seq(0, J, by = bin_w) # cutoff points
mid &lt;- (brks+bin_w/2)[1:n_bin] # mid points


df$bin &lt;- cut(df$sind, breaks = brks, include.lowest = T, labels = mid)
df$bin &lt;- as.numeric(as.character(df$bin))
# head(df)

df %&gt;% 
  filter(SEQN %in% rand_id) %&gt;%
  group_by(SEQN, bin) %&gt;%
  summarise(num = sum(Z)) %&gt;%
  ggplot()+
  geom_point(aes(x=bin, y=num), size = 0.5)+
  facet_wrap(~SEQN)+
  labs(x=&quot;Time&quot;, y = &quot;Activity&quot;, title = &quot;Number of active nimutes within each bin&quot;)</code></pre>
<p><img src="ProgressReport_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code>df &lt;- df %&gt;% rename(id = SEQN, Y=Z)</code></pre>
</div>
<div id="data-split" class="section level3" number="1.2.2">
<h3><span class="header-section-number">1.2.2</span> Data split</h3>
<pre class="r"><code>id_vec &lt;- unique(df$id)
train_id &lt;- sample(id_vec, size = 0.8*N, replace = FALSE) # 7010 subjects for training
test_id &lt;- setdiff(id_vec, train_id) # 1753 subjects for testing</code></pre>
<ul>
<li>80% (7017) subjects for model fitting, 20% (1753) subjects for
out-of-sample prediction</li>
</ul>
</div>
<div id="local-glmm" class="section level3" number="1.2.3">
<h3><span class="header-section-number">1.2.3</span> Local GLMM</h3>
<ul>
<li>Used glmer with PIRLS (nAGQ=0) for local GLMMs step</li>
<li>Fit on the training set</li>
</ul>
<pre class="r"><code># fit model on the training set
df_train &lt;- df %&gt;% filter(id %in% train_id)
train_bin_lst &lt;- split(df_train, f = df_train$bin)

# local GLMM and estimate latent function
# use PIRLS (nAGQ=0) to avoid near-unidentifiability issues 
t1=Sys.time()
df_est_latent &lt;- lapply(train_bin_lst, function(x){pred_latent(x, n_node = 0)}) 
t2= Sys.time()
t_local_glmm &lt;- t2-t1 </code></pre>
<pre class="r"><code>df_est_latent &lt;- bind_rows(df_est_latent) %&gt;% 
  select(-sind, -Y) %&gt;% distinct(.) 
# example estimated latent function
df %&gt;% 
  filter(id %in% rand_id) %&gt;%
  left_join(df_est_latent, by = c(&quot;id&quot;, &quot;bin&quot;)) %&gt;%
  mutate(eta_hat = exp(eta_hat)/(1+exp(eta_hat))) %&gt;%
  ggplot()+
  geom_line(aes(x=sind, y=eta_hat, group = id))+
  geom_point(aes(x=sind, y = Y, group = id), size = 0.5)+
  facet_wrap(~id, scales = &quot;free&quot;)+
  labs(x = &quot;Time&quot;, y = &quot;Estimated latent function (probablity scale)&quot;)</code></pre>
<pre><code>## Warning: Removed 1440 rows containing missing values (`geom_line()`).</code></pre>
<p><img src="ProgressReport_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<pre class="r"><code>df %&gt;% 
  filter(id %in% sample(train_id, size = 4)) %&gt;%
  group_by(id, bin) %&gt;%
  summarise(num = sum(Y)) %&gt;%
  left_join(df_est_latent, by = c(&quot;id&quot;, &quot;bin&quot;)) %&gt;%
  ggplot()+
  geom_point(aes(x=bin, y=num), size = 0.5)+
  geom_line(aes(x=bin, y=eta_hat, group = id))+
  facet_wrap(~id)+
  labs(x=&quot;Time&quot;, y = &quot;Estimated latent function (original scale&quot;)</code></pre>
<p><img src="ProgressReport_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
</div>
<div id="fpca" class="section level3" number="1.2.4">
<h3><span class="header-section-number">1.2.4</span> FPCA</h3>
<pre class="r"><code>mat_est_unique &lt;- matrix(df_est_latent$eta_hat,
                         nrow=length(train_id), 
                         ncol=n_bin, byrow = F) # row index subject, column binned time
#dim(mat_est_unique)

t1 &lt;- Sys.time()
fpca_mod &lt;- fpca.face(mat_est_unique, argvals = mid, var=T)
t2 &lt;- Sys.time()
t_fpca &lt;- t2-t1 # 3.21 seconds to fit fPCA model

# dim(fpca_mod$efunctions) # 27 eigenfunctions total
# fpca_mod$evalues
K &lt;- 4</code></pre>
<pre class="r"><code>plot(mid, fpca_mod$mu, type = &quot;l&quot;, xlab = &quot;bin&quot;, ylab = &quot;mean&quot;)</code></pre>
<p><img src="ProgressReport_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<pre class="r"><code>par(mfrow=c(2,2))

plot(mid, fpca_mod$efunctions[, 1], type=&quot;l&quot;, xlab=&quot;bin&quot;, ylab=&quot;PC1&quot;)
plot(mid, fpca_mod$efunctions[, 2], type=&quot;l&quot;, xlab=&quot;bin&quot;, ylab=&quot;PC2&quot;)
plot(mid, fpca_mod$efunctions[, 3], type=&quot;l&quot;, xlab=&quot;bin&quot;, ylab=&quot;PC3&quot;)
plot(mid, fpca_mod$efunctions[, 4], type=&quot;l&quot;, xlab=&quot;bin&quot;, ylab=&quot;PC4&quot;)</code></pre>
<p><img src="ProgressReport_files/figure-html/unnamed-chunk-11-1.png" width="768" /></p>
<pre class="r"><code># plot correlation matrix
heatmap(cov2cor(fpca_mod$VarMats[[1]]), Rowv = NA, Colv = NA, main = &quot;Correlation&quot;)</code></pre>
<p><img src="ProgressReport_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
</div>
<div id="debias-step-4" class="section level3" number="1.2.5">
<h3><span class="header-section-number">1.2.5</span> Debias (step
4)</h3>
<p>According to the fGFPCA paper, in this step we do two things:</p>
<ol style="list-style-type: decimal">
<li>Interpolate the eigenfunctions with B-spline basis back to the
original grid</li>
<li>Debias the eigenvalues with GLMM (mgcv::bam, mgcm::gamm or
gamm4:gamm) to be used for Laplace approximation</li>
</ol>
<ul>
<li>The bias was caused by the misspecification of latent process
(assume constant effect over smooth function).</li>
<li>What needs re-evaluation are eigenvalues (variance estimates). They
will be used in Laplace Approximation for out-of-sample prediction.</li>
<li>We do not need to debias individual scores because the scores for
training sample will not be used for out-of-sample prediction</li>
<li>How are the eigenvalues biased?</li>
<li>It looks like the FPCA step now estimates mean and eigenfunctions,
while each eigenfunction serves as a random slope. Can we fit such model
with an offset of mean/intercept function?</li>
</ul>
<p>First I’d like to try to fit the debias GLMM model on the full
training set with un-interpolated eigenfunctions. The model failed
because of vector memory exhaustion. Shall I: - Decrease the density of
grid (increase bin width)? - Decrease sample size? - Increase memory
(re-set to 100GB). But still took a lot of time (more than one hour).
The model actually did not finish fitting.</p>
<pre class="r fold-show"><code>df_phi &lt;- data.frame(bin=mid, fpca_mod$efunctions[, 1:4])
colnames(df_phi) &lt;- c(&quot;bin&quot;, paste0(&quot;phi&quot;, 1:4))

df_train &lt;- df_train %&gt;% left_join(df_phi, by = &quot;bin&quot;)
df_train$id &lt;- as.factor(df_train$id)

# usethis::edit_r_environ()
t1 &lt;- Sys.time()
debias_glmm &lt;- bam(Y ~ s(bin, bs=&quot;cr&quot;, k=10)+
                     s(id, by=phi1, bs=&quot;re&quot;)+
                     s(id, by=phi2, bs=&quot;re&quot;)+
                     s(id, by=phi3, bs=&quot;re&quot;)+
                     s(id, by=phi4, bs=&quot;re&quot;), 
                   family = binomial, data=df_train, method = &quot;fREML&quot;)
t2 &lt;- Sys.time()
t_debias &lt;- t2=t1</code></pre>
<p>Then I would like to interpolate eigenfunctions using B-spline basis
functions:</p>
<ul>
<li>Would you interpolate mean and eigenfunctions separately (in my
case, five interpolations)?</li>
<li>I did not find the reeval_efunctions function</li>
</ul>
<pre class="r fold-show"><code># mean
interp_mean &lt;- interpSpline(mid, fpca_mod$mu)
full_mean &lt;- predict(interp_mean, x=1:J)
plot(mid, fpca_mod$mu, ylab = &quot;mean&quot;, pch = 20)
lines(full_mean$x, full_mean$y, col = &quot;blue&quot;)</code></pre>
<p><img src="ProgressReport_files/figure-html/unnamed-chunk-14-1.png" width="480" /></p>
<pre class="r fold-show"><code># PCs
interp_phi1 &lt;- interpSpline(mid, fpca_mod$efunctions[, 1])
full_phi1 &lt;- predict(interp_phi1, x=1:J)
plot(mid, fpca_mod$efunctions[, 1], ylab = &quot;pc1&quot;, pch=20)
lines(full_phi1$x, full_phi1$y, col = &quot;blue&quot;)</code></pre>
<p><img src="ProgressReport_files/figure-html/unnamed-chunk-14-2.png" width="480" /></p>
<pre class="r fold-show"><code>interp_phi2 &lt;- interpSpline(mid, fpca_mod$efunctions[, 2])
full_phi2 &lt;- predict(interp_phi2, x=1:J)
plot(mid, fpca_mod$efunctions[, 2], ylab = &quot;pc2&quot;, pch=20)
lines(full_phi2$x, full_phi2$y, col = &quot;blue&quot;)</code></pre>
<p><img src="ProgressReport_files/figure-html/unnamed-chunk-14-3.png" width="480" /></p>
<pre class="r fold-show"><code>interp_phi3 &lt;- interpSpline(mid, fpca_mod$efunctions[, 3])
full_phi3 &lt;- predict(interp_phi3, x=1:J)
plot(mid, fpca_mod$efunctions[, 3], ylab = &quot;pc3&quot;, pch=20)
lines(full_phi3$x, full_phi3$y, col = &quot;blue&quot;)</code></pre>
<p><img src="ProgressReport_files/figure-html/unnamed-chunk-14-4.png" width="480" /></p>
<pre class="r fold-show"><code>interp_phi4 &lt;- interpSpline(mid, fpca_mod$efunctions[, 4])
full_phi4 &lt;- predict(interp_phi4, x=1:J)
plot(mid, fpca_mod$efunctions[, 4], ylab = &quot;pc4&quot;, pch=20)
lines(full_phi4$x, full_phi4$y, col = &quot;blue&quot;)</code></pre>
<p><img src="ProgressReport_files/figure-html/unnamed-chunk-14-5.png" width="480" /></p>
<p>Perhaps I should try with the simulation data.</p>
</div>
</div>
<div id="out-of-sample-prediction" class="section level2" number="1.3">
<h2><span class="header-section-number">1.3</span> Out-of-sample
prediction</h2>
<p>follow up on the last report, I’ll add some details here.</p>
<p>Let’s set:</p>
<ul>
<li><span class="math inline">\(s\)</span>: bin index</li>
<li><span class="math inline">\(t_{m_s}\)</span>: midpoint time of bin
s</li>
<li><span class="math inline">\(\mathscr{T}_s\)</span>: all observation
points in bin s</li>
<li><span class="math inline">\(Y_i^s\)</span> all observations in bin s
from subject i. <span class="math inline">\(Y_i^S := \{Y_i(t_j), t_j \in
\mathscr{T}_s\}\)</span></li>
</ul>
<p>Let’s assume we have a new subject <span
class="math inline">\(u\)</span> with maximum observation time <span
class="math inline">\(T_u\)</span>. Then the log-likelihood of this new
subject would be:</p>
<p><span class="math display">\[
l_u=\sum_{t_{m_s}&lt;T_u}log(h(Y_u^s))+\hat{\eta}_u(t_{m_s})T(Y_u^s)-log(A[\hat{\eta}_u(t_{m_s})])
\]</span></p>
<p>where <span class="math inline">\(\hat{\eta}_u(t_{m_s}) =
\hat{\mu}_0(t_{m_s})+\sum_{k=1}^K
\xi_{uk}\hat{\phi}(t_{m_s})\)</span>.</p>
<p>From FPCA model (or the debias step to be added), we obtain <span
class="math inline">\(\hat{\mu}_0\)</span>, <span
class="math inline">\(\hat{\phi}_k\)</span>, as well as the variance
estimates of <span class="math inline">\(\xi_k\)</span>: <span
class="math inline">\(\hat{\lambda}_k\)</span>, and variance of residual
process <span class="math inline">\(\hat{\sigma^2}\)</span>.</p>
<p>With all these estimates, we wanna use Laplace approximation to find
the <span class="math inline">\(\xi_{uk}\)</span> that maximizes <span
class="math inline">\(l_u\)</span>.</p>
<p>I am using a package called <strong>LaplacesDemon</strong>, following
this example <a
href="https://cran.r-project.org/web/packages/LaplacesDemon/vignettes/LaplacesDemonTutorial.pdf"
class="uri">https://cran.r-project.org/web/packages/LaplacesDemon/vignettes/LaplacesDemonTutorial.pdf</a></p>
<pre class="r fold-show"><code>library(LaplacesDemon)</code></pre>
<pre><code>## 
## Attaching package: &#39;LaplacesDemon&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:mgcv&#39;:
## 
##     dmvn, rmvn</code></pre>
<pre><code>## The following objects are masked from &#39;package:mvtnorm&#39;:
## 
##     dmvt, rmvt</code></pre>
<pre><code>## The following objects are masked from &#39;package:lubridate&#39;:
## 
##     dst, interval</code></pre>
<pre><code>## The following object is masked from &#39;package:purrr&#39;:
## 
##     partial</code></pre>
<p>Now I am going to use one subject as an example to go over the
approximation procedure:</p>
<p>First we write out the model:</p>
<ul>
<li>Prior distribution: <span class="math inline">\(\xi_{uk} \sim N(0,
\hat{\lambda}_k)\)</span></li>
<li>Posterior distribution:</li>
</ul>
<span class="math display">\[\begin{aligned}
l(\mathbf{Y_u}|\mathbf{\xi}_u) &amp;=
\sum_{t_{m_s}&lt;T_u}l(Y_u^s|\mathbf{\xi}_u) \\

Y_u^s|\mathbf{\xi}_u &amp; \sim Binomial(n_s, p_s) \\

g^{-1}(p_s) = \hat{\eta}(t_{m_s}) &amp;=
\hat{\mu}_0(t_{m_s})+\sum_{k=1}^K \xi_{uk}\hat{\phi}(t_{m_s})
\end{aligned}\]</span>
<p>And <span class="math inline">\(n_s\)</span> is the number of
observations in bin s.</p>
<p>Bayes theroem:</p>
<p><span class="math display">\[
l(\mathbf{\xi}_u|\mathbf{Y}_u) \propto
l(\mathbf{Y}_u|\mathbf{\xi}_u)+l(\mathbf{\xi}_u)
\]</span></p>
<pre class="r fold-show"><code>## model
Model &lt;- function(parm, Data){
  xi &lt;- parm[Data$pos.xi]
  
  # log-prior
  xi.prior &lt;- dmvnorm(xi, mean = rep(0, Data$J), sigma=Data$tao, log = TRUE)
  
  # log-posterior likelihood
  eta &lt;- Data$f0+Data$X %*% xi
  p &lt;- exp(eta)/(1+exp(eta))
  LL &lt;- sum(dbinom(x=Data$y, size = Data$n, prob=p, log = TRUE)) # log likelihood of Y|xi
  LP &lt;- LL+sum(xi.prior) # unnormalized joint log likelihood of (Y, xi)
  
  # output
  Modelout &lt;- list(LP=LP, Dev=-2*LL, Monitor=LP,
                   yhat=Data$y, parm=parm)
  return(Modelout)
}</code></pre>
<ul>
<li>Then we feed in data for one observation</li>
</ul>
<pre class="r fold-show"><code>rand_id &lt;- test_id[15]
df_new &lt;- df %&gt;% filter(id == rand_id &amp; sind &lt;= 540) # observation until 9am

# put data into correct format
ns &lt;- as.vector(table(df_new$bin)) # number of observations
hs &lt;- df_new %&gt;% group_by(bin) %&gt;% summarize_at(&quot;Y&quot;, sum) %&gt;% select(Y) %&gt;% unlist()# number of success
nf &lt;- ns-hs # number of failure
max_bin &lt;- length(unique(df_new$bin)) # assume new skipped bins  
df_new2 &lt;- data.frame(bin = unique(df_new$bin), ns, hs, nf)  
  
# extract estimates from FPCA model to be used in prior distribution
tao &lt;- diag(fpca_mod$evalues[1:K]) # variance of xi

# extract estimates from FPCA model to be used in posterior distribution
f0 &lt;- fpca_mod$mu[1:max_bin]
N &lt;- nrow(df_new2) # &quot;sample size&quot; which is in fact number of observed bins in this case for a new subject
n &lt;- df_new2$ns # number of experiements at each bin
y &lt;- df_new2$hs # outcome, number of 1
X &lt;- fpca_mod$efunctions[1:max_bin, 1:K] # eigen/PC functions
J &lt;- K # number of parameters/scores
mon.names &lt;- &quot;LP&quot;

# parameter names
name_lst &lt;- as.list(rep(0, J))
names(name_lst) &lt;- paste(&quot;xi&quot;, 1:J, sep = &quot;&quot;)
parm.names &lt;- as.parm.names(name_lst) # names of parameters to estimate
pos.xi &lt;- grep(&quot;xi&quot;, parm.names)

# data
MyData &lt;- list(J=J, X=X, mon.names=mon.names,
                 parm.names=parm.names, pos.xi=pos.xi, y=y, n=n, tao=tao, f0=f0)</code></pre>
<pre class="r fold-show"><code># fit laplace approximation
# initial values generated by GIV function in the same package
Fit &lt;- LaplaceApproximation(Model, Data=MyData)</code></pre>
<pre><code>## Initial values were not supplied, and
## have been set to zero prior to LaplaceApproximation().
## Sample Size:  54 
## Laplace Approximation begins...
## Iteration:  10  of  100 ,   LP:  -72.4 
## Iteration:  20  of  100 ,   LP:  -68.7 
## Iteration:  30  of  100 ,   LP:  -68.6 
## Iteration:  40  of  100 ,   LP:  -68.5 
## Iteration:  50  of  100 ,   LP:  -68.5 
## Iteration:  60  of  100 ,   LP:  -68.5 
## Estimating the Covariance Matrix
## Sampling from Posterior with Sampling Importance Resampling
## Creating Summary from Point-Estimates
## Creating Summary from Posterior Samples
## Estimating Log of the Marginal Likelihood
## Laplace Approximation is finished.</code></pre>
<ul>
<li>Summarizing output</li>
</ul>
<pre class="r fold-show"><code># summarizing output
print(Fit)</code></pre>
<pre><code>## 
## Call:
## LaplaceApproximation(Model = Model, Data = MyData)
## 
## Converged: TRUE
## Covariance Matrix: (NOT SHOWN HERE; diagonal shown instead)
##       xi1       xi2       xi3       xi4 
## 43.853857 39.604572  7.058457 10.894559 
## 
## Deviance (Final):  100.8664 
## History: (NOT SHOWN HERE)
## Initial Values:
## [1] 0 0 0 0
## 
## Iterations: 63
## Log(Marginal Likelihood): -36.37464
## Log-Posterior (Final): -68.49917
## Log-Posterior (Initial): -169.8166
## Minutes of run-time: 0
## Monitor: (NOT SHOWN HERE)
## Posterior: (NOT SHOWN HERE)
## Step Size (Final): [1] 1
## Step Size (Initial): 1
## Summary1: (SHOWN BELOW)
## Summary2: (SHOWN BELOW)
## Tolerance (Final): 1.932676e-06
## Tolerance (Stop): 1e-05
## 
## Summary1:
##          Mode       SD        LB        UB
## xi1 31.548354 6.622224 18.303905 44.792803
## xi2 14.247931 6.293216  1.661498 26.834363
## xi3  1.778145 2.656776 -3.535406  7.091696
## xi4 10.146144 3.300691  3.544763 16.747526
## 
## Summary2:
##                Mode       SD       MCSE  ESS         LB    Median         UB
## xi1       31.464763 6.684349 0.21137767 1000  17.976682  31.31260  44.140356
## xi2       13.680542 6.281579 0.19864096 1000   1.438422  13.61012  25.713624
## xi3        1.263562 2.742649 0.08673019 1000  -4.962504   1.35873   6.351043
## xi4       10.152744 3.242481 0.10253627 1000   3.556399  10.19438  16.769759
## Deviance 103.939259 5.946755 0.18805291 1000  93.027527 103.63877 116.187683
## LP       -70.560984 1.501012 0.04746616 1000 -74.430162 -70.21294 -68.724827</code></pre>
<pre class="r fold-show"><code>plot(Fit, MyData)</code></pre>
<p><img src="ProgressReport_files/figure-html/unnamed-chunk-19-1.png" width="768" /><img src="ProgressReport_files/figure-html/unnamed-chunk-19-2.png" width="768" /><img src="ProgressReport_files/figure-html/unnamed-chunk-19-3.png" width="768" /></p>
<pre class="r"><code># prediction
eta_pred_out &lt;- fpca_mod$mu+fpca_mod$efunctions[, 1:K]%*%Fit$Summary1[, &quot;Mode&quot;]

data.frame(bin = mid, eta_pred = eta_pred_out) %&gt;% 
  right_join(df %&gt;% filter(id==rand_id) %&gt;%  select(Y, sind, bin), by = &quot;bin&quot;) %&gt;%
  ggplot()+
  geom_point(aes(x=sind, y = Y))+
  geom_line(aes(x=sind, y = exp(eta_pred)/(1+exp(eta_pred))))+
  labs(title = rand_id)</code></pre>
<p><img src="ProgressReport_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
</div>
</div>
<div id="full-data-application-output" class="section level1"
number="2">
<h1><span class="header-section-number">2</span> Full data application
output</h1>
<div id="fgfpca" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> fGFPCA</h2>
<ul>
<li>Still need to project eigenfunctions to the full grid</li>
</ul>
<pre class="r"><code># results generated from DataApplNHANES.R
load(here(&quot;Data/ApplOutput_fGFPCA.RData&quot;))</code></pre>
<pre class="r"><code>df_test_full &lt;- df %&gt;% 
  filter(id %in% test_id) %&gt;%
  filter(!id %in% skip_id) %&gt;%
  left_join(df_test %&gt;% select(id, bin, pred_t540, pred_t780, pred_t1020), by = c(&quot;id&quot;, &quot;bin&quot;))

rand_id2 &lt;- sample(unique(df_test$id), 4)
# without interpolation, assume constant latent function value in each bin
df_test_full%&gt;%
  filter(id %in% rand_id2) %&gt;%
  mutate_at(vars(pred_t540, pred_t780, pred_t1020), function(x)exp(x)/(1+exp(x))) %&gt;%
  ggplot()+
    geom_line(aes(x=bin, y = pred_t540, col = &quot;9am&quot;))+
    geom_line(aes(x=bin, y = pred_t780, col = &quot;1pm&quot;))+
    geom_line(aes(x=bin, y = pred_t1020, col = &quot;5pm&quot;))+
    geom_point(aes(x=bin, y = Y, col = &quot;Outcome&quot;), size = 0.2)+
    facet_wrap(~id)+
    labs(x = &quot;Time&quot;, y = &quot;Estimated latent function (probablity scale)&quot;,
         title = &quot;Constant latent function within bin&quot;)</code></pre>
<p><img src="ProgressReport_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<div id="numeric-problems" class="section level3" number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> Numeric
problems</h3>
<ul>
<li>Three subjects had failed approximation, all happened with the
shortest observed track</li>
<li>Does it have to do with still the length of observed track? Or the
arbitrary choices in the binning procedure (bin width).</li>
<li>Per last meeting, adding some details on laplace approximation</li>
</ul>
<pre class="r"><code>df %&gt;% filter(id %in% skip_id) %&gt;% 
  ggplot()+
  geom_point(aes(x=sind, y=Y), size = 0.2)+
  facet_wrap(~id)+
  geom_vline(xintercept = c(540))</code></pre>
<p><img src="ProgressReport_files/figure-html/unnamed-chunk-23-1.png" width="864" /></p>
<!-- Calculate AUC -->
<pre class="r"><code># break by prediction window
lst_auc &lt;- df_test_full %&gt;% 
  mutate(window = cut(sind, breaks = c(0, 540, 780, 1020, 1260, 1440), 
                      labels = c(&quot;0-9am&quot;, &quot;9am-1pm&quot;, &quot;1pm-5pm&quot;, &quot;5pm-9pm&quot;,&quot;9pm-12pm&quot;), 
                      include.lowest = T))
lst_auc &lt;- split(lst_auc, f=lst_auc$window)</code></pre>
<pre class="r"><code># no interpolation
# up to 9am
auc_t540 &lt;- lst_auc[2:4] %&gt;% 
  lapply(function(x)performance(prediction(x$pred_t540, x$Y), measure = &quot;auc&quot;))
auc_t540 &lt;- lapply(auc_t540, function(x){x@y.values[[1]]}) %&gt;% unlist()

# up to 1pm
auc_t780 &lt;- lst_auc[3:4] %&gt;% 
  lapply(function(x)performance(prediction(x$pred_t780, x$Y), measure = &quot;auc&quot;))
auc_t780 &lt;- lapply(auc_t780, function(x){x@y.values[[1]]}) %&gt;% unlist()

# up to 5pm
auc_t1020 &lt;- lst_auc[4] %&gt;% 
  lapply(function(x)performance(prediction(x$pred_t1020, x$Y), measure = &quot;auc&quot;))
auc_t1020 &lt;- lapply(auc_t1020, function(x){x@y.values[[1]]}) %&gt;% unlist()</code></pre>
</div>
</div>
<div id="glmmadaptvie" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> GLMMadaptvie</h2>
<ul>
<li>when fitting model with <strong>random intercept + slope</strong> on
the same full training set (even after scaling covariates): Error:
vector memory exhausted (limit reached?).</li>
<li>when fitting model with <strong>only random intercept</strong> on
the same full training set: Error regarding a large coefficient value.
One fix is to re-scale covariates. So I re-scaled minute index by
dividing them by J, so the range is within (0, 1], and tried again. This
fitting procedure took 20.72 minutes to finish</li>
</ul>
<pre class="r"><code>load(here(&quot;Data/ApplOutput_GLMMadaptive.RData&quot;))</code></pre>
<ul>
<li><p>20.82 minutes on model fitting and 11.43 minutes on out-of-sample
prediction.</p></li>
<li><p>Below is the model fit on the training set</p></li>
</ul>
<pre class="r"><code>summary(adglmm_mod)</code></pre>
<pre><code>## 
## Call:
## mixed_model(fixed = Y ~ sind, random = ~1 | id, data = df_train %&gt;% 
##     mutate(sind = sind/J), family = binomial())
## 
## Data Descriptives:
## Number of Observations: 10094400
## Number of Groups: 7010 
## 
## Model:
##  family: binomial
##  link: logit 
## 
## Fit statistics:
##   log.Lik      AIC      BIC
##  -5216266 10432537 10432558
## 
## Random effects covariance matrix:
##               StdDev
## (Intercept) 1.099137
## 
## Fixed effects:
##             Estimate Std.Err   z-value p-value
## (Intercept)  -2.1130  0.0133 -159.3623 &lt; 1e-04
## sind          1.9153  0.0027  700.7869 &lt; 1e-04
## 
## Integration:
## method: adaptive Gauss-Hermite quadrature rule
## quadrature points: 11
## 
## Optimization:
## method: hybrid EM and quasi-Newton
## converged: TRUE</code></pre>
<pre class="r"><code>df_test_full2 &lt;- df_test_full %&gt;%
  filter(id %in% rand_id2)%&gt;%
  select(id, sind, Y)

df_test_full2$pred_t540 &lt;- df_test_full2$pred_t780 &lt;- df_test_full2$pred_t1020 &lt;- NA 
for(i in rand_id2){
  df_test_full2[df_test_full2$id==i &amp; df_test_full2$sind&gt;540, &quot;pred_t540&quot;]&lt;- adglmm_pred_t540$newdata2 %&gt;% filter(id ==i) %&gt;% select(pred)
  
  df_test_full2[df_test_full2$id==i &amp; df_test_full2$sind&gt;780, &quot;pred_t780&quot;]&lt;- adglmm_pred_t780$newdata2 %&gt;% filter(id ==i) %&gt;% select(pred)
  
  df_test_full2[df_test_full2$id==i &amp; df_test_full2$sind&gt;1020, &quot;pred_t1020&quot;]&lt;- adglmm_pred_t1020$newdata2 %&gt;% filter(id ==i) %&gt;% select(pred)
}

# colSums(is.na(df_test_full2))</code></pre>
<ul>
<li>Below is an example of predicted probablity on four random
subjects</li>
</ul>
<pre class="r"><code>df_test_full2 %&gt;% 
  mutate_at(vars(pred_t540, pred_t780, pred_t1020), 
            function(x)exp(x)/(1+exp(x))) %&gt;%
  ggplot()+
    geom_line(aes(x=sind, y = pred_t540, col = &quot;9am&quot;))+
    geom_line(aes(x=sind, y = pred_t780, col = &quot;1pm&quot;))+
    geom_line(aes(x=sind, y = pred_t1020, col = &quot;5pm&quot;))+
    geom_point(aes(x=sind, y = Y, col = &quot;Outcome&quot;), size = 0.2)+
    facet_wrap(~id)+
    labs(x = &quot;Time&quot;, y = &quot;Estimated latent function (probablity scale)&quot;)</code></pre>
<p><img src="ProgressReport_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<!-- Calculate AUC  -->
<pre class="r"><code># up to 540
J &lt;- 1440
auc_t540_adglmm &lt;- adglmm_pred_t540$newdata2 %&gt;%
  filter(!id %in% skip_id) %&gt;%
  mutate(sind=sind*J) %&gt;%
  mutate(window = cut(sind, breaks = c(540, 780, 1020, 1260, 1440), 
                      labels = c(&quot;9am-1pm&quot;, &quot;1pm-5pm&quot;, &quot;5pm-9pm&quot;,&quot;9pm-12pm&quot;), 
                      include.lowest = T))
auc_t540_adglmm &lt;- split(auc_t540_adglmm, f=auc_t540_adglmm$window)
auc_t540_adglmm &lt;- auc_t540_adglmm[1:3] %&gt;% 
  lapply(function(x)performance(prediction(x$pred, x$Y), measure = &quot;auc&quot;))
auc_t540_adglmm &lt;- lapply(auc_t540_adglmm, function(x){x@y.values[[1]]}) %&gt;% unlist()

# up to 780
auc_t780_adglmm &lt;- adglmm_pred_t780$newdata2 %&gt;%
  filter(!id %in% skip_id) %&gt;%
  mutate(sind=sind*J) %&gt;%
  mutate(window = cut(sind, breaks = c(780, 1020, 1260, 1440), 
                      labels = c(&quot;1pm-5pm&quot;, &quot;5pm-9pm&quot;,&quot;9pm-12pm&quot;), 
                      include.lowest = T))
auc_t780_adglmm &lt;- split(auc_t780_adglmm, f=auc_t780_adglmm$window)
auc_t780_adglmm &lt;- auc_t780_adglmm[1:2] %&gt;% 
  lapply(function(x)performance(prediction(x$pred, x$Y), measure = &quot;auc&quot;))
auc_t780_adglmm &lt;- lapply(auc_t780_adglmm, function(x){x@y.values[[1]]}) %&gt;% unlist()

# up to 1020
auc_t1020_adglmm &lt;- adglmm_pred_t1020$newdata2 %&gt;%
  filter(!id %in% skip_id) %&gt;%
  mutate(sind=sind*J) %&gt;%
  mutate(window = cut(sind, breaks = c(1020, 1260, 1440), 
                      labels = c(&quot;5pm-9pm&quot;,&quot;9pm-12pm&quot;), 
                      include.lowest = T))
auc_t1020_adglmm &lt;- split(auc_t1020_adglmm, f=auc_t1020_adglmm$window)
auc_t1020_adglmm &lt;- auc_t1020_adglmm[1] %&gt;% 
  lapply(function(x)performance(prediction(x$pred, x$Y), measure = &quot;auc&quot;))
auc_t1020_adglmm &lt;- lapply(auc_t1020_adglmm, function(x){x@y.values[[1]]}) %&gt;% unlist()</code></pre>
</div>
<div id="compare-fgfpca-and-glmmadaptive" class="section level2"
number="2.3">
<h2><span class="header-section-number">2.3</span> Compare fGFPCA and
GLMMadaptive</h2>
<ul>
<li>To use the same sample for comparison, I’d exclude the subjects with
failed Laplace approximation from the test set for both methods</li>
</ul>
<pre class="r"><code>options(knitr.kable.NA = &#39;&#39;)
tb_auc &lt;- data.frame(auc_t540, 
           c(NA, auc_t780), 
           c(NA, NA, auc_t1020), 
        
           auc_t540_adglmm,
           c(NA, auc_t780_adglmm), 
           c(NA, NA, auc_t1020_adglmm))

colnames(tb_auc) &lt;- rep(c(&quot;9am&quot;, &quot;1pm&quot;, &quot;5pm&quot;), 2)</code></pre>
<pre class="r"><code># time
t_fit &lt;- c(t_local_glmm+t_fpca, t_est_adglmm)
units(t_fit) &lt;- &quot;mins&quot;

t_test &lt;- c(t_pred, t_pred_adglmm)
# units(t_pred)

t_total &lt;- t_fit+t_test


idx1 &lt;- idx2 &lt;- idx3 &lt;- c(1, 3, 3)
names(idx1) &lt;- c(&quot;Total time&quot;, as.character(round(t_total, 2)))
names(idx2) &lt;- c(&quot;Time on prediction&quot;, as.character(round(t_test, 2)))
names(idx3) &lt;- c(&quot;Time on model fitting&quot;, as.character(round(t_fit, 2)))

t_fit &lt;- as.character(round(t_fit, 2))
t_test &lt;- as.character(round(t_test, 2))
t_total &lt;- as.character(round(t_total, 2))</code></pre>
<pre class="r"><code>tb_auc %&gt;%
  kable(digit = 4, table.attr = &quot;style = \&quot;color: black;\&quot;&quot;) %&gt;%
  kable_styling(full_width = F) %&gt;% 
  add_header_above(header=idx1) %&gt;%
  add_header_above(header=idx2) %&gt;%
  add_header_above(header=idx3) %&gt;%
  add_header_above(c(&quot; &quot;=1, &quot;fGFPCA&quot;=3, &quot;GLMMadaptive&quot;=3))</code></pre>
<table style="color: black; width: auto !important; margin-left: auto; margin-right: auto;" class="table">
<thead>
<tr>
<th style="empty-cells: hide;border-bottom:hidden;" colspan="1">
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="3">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
fGFPCA
</div>
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="3">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
GLMMadaptive
</div>
</th>
</tr>
<tr>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="1">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Time on model fitting
</div>
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="3">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
3.3
</div>
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="3">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
20.82
</div>
</th>
</tr>
<tr>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="1">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Time on prediction
</div>
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="3">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
19.79
</div>
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="3">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
11.43
</div>
</th>
</tr>
<tr>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="1">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Total time
</div>
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="3">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
23.09
</div>
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="3">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
32.25
</div>
</th>
</tr>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
9am
</th>
<th style="text-align:right;">
1pm
</th>
<th style="text-align:right;">
5pm
</th>
<th style="text-align:right;">
9am
</th>
<th style="text-align:right;">
1pm
</th>
<th style="text-align:right;">
5pm
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
9am-1pm
</td>
<td style="text-align:right;">
0.7179
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
0.6930
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
<tr>
<td style="text-align:left;">
1pm-5pm
</td>
<td style="text-align:right;">
0.6188
</td>
<td style="text-align:right;">
0.7463
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
0.6317
</td>
<td style="text-align:right;">
0.7279
</td>
<td style="text-align:right;">
</td>
</tr>
<tr>
<td style="text-align:left;">
5pm-9pm
</td>
<td style="text-align:right;">
0.5915
</td>
<td style="text-align:right;">
0.6340
</td>
<td style="text-align:right;">
0.7127
</td>
<td style="text-align:right;">
0.5881
</td>
<td style="text-align:right;">
0.6427
</td>
<td style="text-align:right;">
0.6842
</td>
</tr>
</tbody>
</table>
<ul>
<li>fGFPCA does not always have higher AUC, but definitely most of the
time.</li>
<li>In fact there are two scenarios where GLMMadaptive outperforms
fGFPCA slightly: 1) given 9am to predict 1-5pm; and 2) given 1pm and
predict 5-9pm. Both of them have a 4-hour interval apart from the
maximum observation time and and prediction window. Could it have
anything to do the cyclic trend in the data? Is 4h the length of a
cycle? I tried to look at it but really couldn’t see anything…there are
too many points lumped together!</li>
</ul>
<pre class="r"><code>df %&gt;% filter(id %in% rand_id2) %&gt;%
  ggplot()+
  geom_point(aes(x=sind, y=Y), size = 0.2)+
  geom_vline(xintercept = c(540, 780, 1020, 1260))+
  scale_x_continuous(breaks = c(540, 780, 1020, 1260),
                     labels = c(&quot;9am&quot;, &quot;1pm&quot;, &quot;5pm&quot;, &quot;9pm&quot;))+
  facet_wrap(~id)</code></pre>
<p><img src="ProgressReport_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
</div>
<div id="discussion" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> Discussion</h2>
<ul>
<li>What would be the reference method to compare our performance to?
Original GLMMadaptvie perhaps is feasible on the entire dataset. But
what should we use if we wanna do sub-sample comparison? Should we even
do that for this data application (or simulation alone)?</li>
<li>There are still three participants with numeric problem! Shall we
extend the observation track even more? I suspect that may cause still
the same issue just on different subjects.</li>
<li>Is interpolation really necessary? It does not improve performance
very much but consumes time. Also, should I try more sophisticated
methods for grid extension?</li>
<li>Should I calculate subject-wise AUC and average them? But some
subjects are all zero within the prediction window, making AUC for this
single subjects impossible to calculate…</li>
</ul>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
