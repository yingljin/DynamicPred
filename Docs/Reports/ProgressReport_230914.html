<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Ying Jin" />

<meta name="date" content="2023-05-16" />

<title>Progress Report</title>

<script src="ProgressReport_files/header-attrs-2.21/header-attrs.js"></script>
<script src="ProgressReport_files/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="ProgressReport_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="ProgressReport_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="ProgressReport_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="ProgressReport_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="ProgressReport_files/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="ProgressReport_files/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="ProgressReport_files/tocify-1.9.1/jquery.tocify.js"></script>
<script src="ProgressReport_files/navigation-1.1/tabsets.js"></script>
<script src="ProgressReport_files/navigation-1.1/codefolding.js"></script>
<link href="ProgressReport_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="ProgressReport_files/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div id="header">

<div class="btn-group pull-right float-right">
<button type="button" class="btn btn-default btn-xs btn-secondary btn-sm dropdown-toggle" data-toggle="dropdown" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu dropdown-menu-right" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Progress Report</h1>
<h4 class="author">Ying Jin</h4>
<h4 class="date">2023-05-16</h4>

</div>


<div id="nhanes-data-application" class="section level1" number="1">
<h1><span class="header-section-number">1</span> NHANES data
application</h1>
<div id="data-overview" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> Data overview</h2>
<ul>
<li>8763 subjects, 1440 measures each</li>
<li>no missingness</li>
</ul>
<pre class="r"><code>N &lt;- length(unique(df$SEQN)) # sample size 8763
J &lt;- max(df$sind) # 1440 measures per subject</code></pre>
<pre class="r"><code>rand_id &lt;- sample(unique(df$SEQN), size = 4) # &quot;toy&quot; sample

df %&gt;% 
  filter(SEQN %in% rand_id) %&gt;%
  ggplot()+
  geom_point(aes(x=sind, y=Z), size = 0.5)+
  facet_wrap(~SEQN)+
  labs(x=&quot;Time&quot;, y = &quot;Activity&quot;, title = &quot;A brief overview of the outcome&quot;)</code></pre>
<p><img src="ProgressReport_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
</div>
<div id="fgfpca-estimation" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> fGFPCA
Estimation</h2>
<pre class="r"><code># code
source(here(&quot;Code/GLMM-FPCA.R&quot;)) 
# use pred_latent function to estimate latent function 
source(here(&quot;Code/OutSampMLE.R&quot;))
# source(here(&quot;Code/OutsampBayes.R&quot;))</code></pre>
<div id="binning" class="section level3" number="1.2.1">
<h3><span class="header-section-number">1.2.1</span> Binning</h3>
<ul>
<li>Bin every 10 measures</li>
</ul>
<pre class="r"><code># bin data
bin_w &lt;- 10 # bin width
n_bin &lt;- J/bin_w # number of bins
brks &lt;- seq(0, J, by = bin_w) # cutoff points
mid &lt;- (brks+bin_w/2)[1:n_bin] # mid points


df$bin &lt;- cut(df$sind, breaks = brks, include.lowest = T, labels = mid)
df$bin &lt;- as.numeric(as.character(df$bin))
# head(df)

df %&gt;% 
  filter(SEQN %in% rand_id) %&gt;%
  group_by(SEQN, bin) %&gt;%
  summarise(num = sum(Z)) %&gt;%
  ggplot()+
  geom_point(aes(x=bin, y=num), size = 0.5)+
  facet_wrap(~SEQN)+
  labs(x=&quot;Time&quot;, y = &quot;Activity&quot;, title = &quot;Number of active nimutes within each bin&quot;)</code></pre>
<p><img src="ProgressReport_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
<div id="local-glmms" class="section level3" number="1.2.2">
<h3><span class="header-section-number">1.2.2</span> Local GLMMs</h3>
<ul>
<li>This is where near-unidentifiability issues first appear</li>
<li>Though point estimation not necessarily affected</li>
</ul>
<pre class="r"><code># split by each bin
df &lt;- df %&gt;% rename(id = SEQN, Y=Z)
df_bin_lst &lt;- split(df, f = df$bin)

# fit local GLMM and estimate latent function
# near-unidentifiability issues 
df_est_latent &lt;- lapply(df_bin_lst, function(x){pred_latent(x)}) 
df_est_latent &lt;- bind_rows(df_est_latent)

# keep midpoint values
df_est_latent &lt;- df_est_latent %&gt;%
  select(-sind, -Y) %&gt;% distinct(.)</code></pre>
<pre class="r"><code>df %&gt;% 
  filter(id %in% rand_id) %&gt;%
  left_join(df_est_latent, by = c(&quot;id&quot;, &quot;bin&quot;)) %&gt;%
  mutate(eta_hat = exp(eta_hat)/(1+exp(eta_hat))) %&gt;%
  ggplot()+
  geom_line(aes(x=sind, y=eta_hat, group = id))+
  geom_point(aes(x=sind, y = Y, group = id), size = 0.5)+
  facet_wrap(~id, scales = &quot;free&quot;)+
  geom_vline(xintercept = c(465, 545, 555, 1185, 1195, 1205), col = &quot;red&quot;,
             alpha = 0.5, linewidth = 0.5)+
  labs(x = &quot;Time&quot;, y = &quot;Estimated latent function (probablity scale)&quot;)</code></pre>
<p><img src="ProgressReport_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<pre class="r"><code># overview at estimated latent function
df %&gt;% 
  filter(id %in% rand_id) %&gt;%
  group_by(id, bin) %&gt;%
  summarise(num = sum(Y)) %&gt;%
  left_join(df_est_latent, by = c(&quot;id&quot;, &quot;bin&quot;)) %&gt;%
  # mutate(eta_hat = exp(eta_hat)/(1+exp(eta_hat))) %&gt;%
  ggplot()+
  geom_line(aes(x=bin, y=eta_hat, group = id))+
  geom_point(aes(x=bin, y = num, group = id), size = 0.5)+
  facet_wrap(~id, scales = &quot;free&quot;)+
  geom_vline(xintercept = c(465, 545, 555, 1185, 1195, 1205), col = &quot;red&quot;,
             linewidth = 0.5, alpha = 0.6)+
  labs(x = &quot;Bin&quot;, y = &quot;Estimated latent function/number of active minutes&quot;)</code></pre>
<p><img src="ProgressReport_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<div id="take-a-closer-look-at-the-unidentifiable-bins"
class="section level4" number="1.2.2.1">
<h4><span class="header-section-number">1.2.2.1</span> Take a closer
look at the unidentifiable bins</h4>
<pre class="r"><code>try_fit_lst&lt;-list()
for(i in seq_along(df_bin_lst)){
  this_glm &lt;- tryCatch({glmer(Y ~ 1 + (1|id), data = df_bin_lst[[i]],
                             family = binomial)},
                       warning=function(w){NA})
  try_fit_lst[[i]] &lt;- this_glm
  
}

# bins where glmer returned errors
names(df_bin_lst)[is.na(try_fit_lst)]</code></pre>
<p>The fitting problem occurs in the following bins: 465, 545, 555,
1185, 1195, 1205. All of them had two warnings: 1) Non-convergence; 2)
near-unidenetifiability. Summary results show that standar error
estimates of fixed effects (intercept) are very close two zero.</p>
<p>Below is an example of bin 465:</p>
<pre class="r fold-show"><code>issue_fit &lt;- glmer(Y ~ 1 + (1|id), data = df_bin_lst[[&#39;465&#39;]], family = binomial)</code></pre>
<pre><code>## Warning in checkConv(attr(opt, &quot;derivs&quot;), opt$par, ctrl = control$checkConv, :
## Model failed to converge with max|grad| = 1.32945 (tol = 0.002, component 1)</code></pre>
<pre><code>## Warning in checkConv(attr(opt, &quot;derivs&quot;), opt$par, ctrl = control$checkConv, : Model is nearly unidentifiable: very large eigenvalue
##  - Rescale variables?</code></pre>
<pre class="r fold-show"><code>summary(issue_fit)</code></pre>
<pre><code>## Generalized linear mixed model fit by maximum likelihood (Laplace
##   Approximation) [glmerMod]
##  Family: binomial  ( logit )
## Formula: Y ~ 1 + (1 | id)
##    Data: df_bin_lst[[&quot;465&quot;]]
## 
##      AIC      BIC   logLik deviance df.resid 
##  60650.0  60668.7 -30323.0  60646.0    87628 
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.6465 -0.0945 -0.0945  0.1827  3.0637 
## 
## Random effects:
##  Groups Name        Variance Std.Dev.
##  id     (Intercept) 19.72    4.441   
## Number of obs: 87630, groups:  id, 8763
## 
## Fixed effects:
##               Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -2.972e+00  5.318e-05  -55896   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## optimizer (Nelder_Mead) convergence code: 0 (OK)
## Model failed to converge with max|grad| = 1.32945 (tol = 0.002, component 1)
## Model is nearly unidentifiable: very large eigenvalue
##  - Rescale variables?</code></pre>
<p>However, this issue seems to disappear after changing nAGQ. This
argument controls the number of nodes in adaptive Gauss-Hermite
quadrature approximation. If nAGQ=0, optimitization is done through
penalized iteratively reweighted least square (PIRLS); if nAGQ=1,
Laplace approximation and if nAGQ&gt;1, adaptive Gauss-Hermite
Quadrature.</p>
<p>Reference: <a href="https://arxiv.org/pdf/2202.07864.pdf"
class="uri">https://arxiv.org/pdf/2202.07864.pdf</a> (In fact based on
this reference the recommended number of node is 0)</p>
<p>Another reference: <a
href="https://stats.stackexchange.com/questions/544937/when-is-it-appropriate-to-set-nagq-0-in-glmer"
class="uri">https://stats.stackexchange.com/questions/544937/when-is-it-appropriate-to-set-nagq-0-in-glmer</a>.
According to this one, when setting nAGQ=0, fixed effect parameters are
not optimized.</p>
<p>For example set nAGQ=2 for bin 465:</p>
<pre class="r fold-show"><code>issue_fit &lt;- glmer(Y ~ 1 + (1|id), data = df_bin_lst[[&#39;465&#39;]], family = binomial, nAGQ=2)

summary(issue_fit)</code></pre>
<pre><code>## Generalized linear mixed model fit by maximum likelihood (Adaptive
##   Gauss-Hermite Quadrature, nAGQ = 2) [glmerMod]
##  Family: binomial  ( logit )
## Formula: Y ~ 1 + (1 | id)
##    Data: df_bin_lst[[&quot;465&quot;]]
## 
##      AIC      BIC   logLik deviance df.resid 
##  61618.9  61637.7 -30807.4  61614.9    87628 
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.5614 -0.1231 -0.1231  0.2093  3.0087 
## 
## Random effects:
##  Groups Name        Variance Std.Dev.
##  id     (Intercept) 12.87    3.587   
## Number of obs: 87630, groups:  id, 8763
## 
## Fixed effects:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -2.27006    0.04844  -46.86   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<!-- https://www.learn-mlms.com/07-module-7.html -->
<!-- https://rstudio-pubs-static.s3.amazonaws.com/33653_57fc7b8e5d484c909b615d8633c01d51.html -->
<p>So I decided to re-do latent function estimation while adjusting the
nAGQ parameter. Below are some findings:</p>
<ul>
<li>nAGQ=0 for all bins: no warnings appeared. This is supposed to be
fast but not accurate.</li>
<li>nAGQ=1, 2, 3: warnings appeared in some bins.</li>
<li>nAGQ=5: no warnings!</li>
</ul>
<pre class="r"><code># re-fit local GLMM and check numeric issues
try_fit_lst&lt;-list()
for(i in seq_along(df_bin_lst)){
  this_glm &lt;- tryCatch({glmer(Y ~ 1 + (1|id), data = df_bin_lst[[i]], family = binomial, nAGQ=5)},
                       warning=function(w){NA})
  try_fit_lst[[i]] &lt;- this_glm
  
}

# bins where glmer returned errors
names(df_bin_lst)[is.na(try_fit_lst)]
glmer(Y ~ 1 + (1|id), data = df_bin_lst[[&#39;565&#39;]], family = binomial, nAGQ=3)</code></pre>
<p>So perhaps we could try either setting nAGQ = 0 or a variable
nAGQ:</p>
<p>Below compares the result for nAGQ = 0 and nAGQ = 5:</p>
<pre class="r"><code># re-define pre_latent funciton with nAGQ = 0
pred_latent_0AGQ &lt;- function(df){
    this_glm &lt;- glmer(Y ~ 1 + (1|id), data = df, family = binomial, nAGQ = 0)
    eta_hat &lt;- predict(this_glm, type = &quot;link&quot;)
    df$eta_hat &lt;- eta_hat
    return(df)
}

# re-fit local GLMMs
t1=Sys.time()
df_est_latent_0AGQ &lt;- lapply(df_bin_lst, function(x){pred_latent_0AGQ(x)}) 
t2=Sys.time()
# head(df_est_latent_0AGQ[[1]])
df_est_latent_0AGQ &lt;- bind_rows(df_est_latent_0AGQ)
t_0AGQ &lt;- t2-t1

# keep midpoint values
df_est_latent_0AGQ &lt;- df_est_latent_0AGQ %&gt;%
  select(-sind, -Y) %&gt;% distinct(.)</code></pre>
<pre class="r"><code># re-define pre_latent funciton with nAGQ = 0
pred_latent_5AGQ &lt;- function(df){
    this_glm &lt;- glmer(Y ~ 1 + (1|id), data = df, family = binomial, nAGQ = 5)
    eta_hat &lt;- predict(this_glm, type = &quot;link&quot;)
    df$eta_hat &lt;- eta_hat
    return(df)
}

# re-fit local GLMMs
t1=Sys.time()
df_est_latent_5AGQ &lt;- lapply(df_bin_lst, function(x){pred_latent_5AGQ(x)}) 
t2=Sys.time()
# head(df_est_latent_0AGQ[[1]])
df_est_latent_5AGQ &lt;- bind_rows(df_est_latent_5AGQ)
t_5AGQ &lt;- t2-t1

# keep midpoint values
df_est_latent_5AGQ &lt;- df_est_latent_5AGQ %&gt;%
  select(-sind, -Y) %&gt;% distinct(.)</code></pre>
<ul>
<li>Time spent: 4.21580400069555 for nAGQ=0, 13.256231602033 for
nAGQ=5</li>
</ul>
<pre class="r"><code>ggarrange(
# nAQG=0
df %&gt;% 
  filter(id %in% rand_id) %&gt;%
  left_join(df_est_latent_0AGQ, by = c(&quot;id&quot;, &quot;bin&quot;)) %&gt;%
  mutate(eta_hat = exp(eta_hat)/(1+exp(eta_hat))) %&gt;%
  ggplot()+
  geom_line(aes(x=sind, y=eta_hat, group = id))+
  geom_point(aes(x=sind, y = Y, group = id), size = 0.5)+
  facet_wrap(~id, scales = &quot;free&quot;)+
  # geom_vline(xintercept = c(465, 545, 555, 1185, 1195, 1205), col = &quot;red&quot;,
             # alpha = 0.5, linewidth = 0.5)+
  labs(x = &quot;Time&quot;, y = &quot;Estimated latent function (probablity scale)&quot;, title = &quot;nAGQ=0&quot;),
#nAGQ=5
df %&gt;% 
  filter(id %in% rand_id) %&gt;%
  left_join(df_est_latent_5AGQ, by = c(&quot;id&quot;, &quot;bin&quot;)) %&gt;%
  mutate(eta_hat = exp(eta_hat)/(1+exp(eta_hat))) %&gt;%
  ggplot()+
  geom_line(aes(x=sind, y=eta_hat, group = id))+
  geom_point(aes(x=sind, y = Y, group = id), size = 0.5)+
  facet_wrap(~id, scales = &quot;free&quot;)+
  # geom_vline(xintercept = c(465, 545, 555, 1185, 1195, 1205), col = &quot;red&quot;,
             # alpha = 0.5, linewidth = 0.5)+
  labs(x = &quot;Time&quot;, y = &quot;Estimated latent function (probablity scale)&quot;, title = &quot;nAGQ=5&quot;), nrow = 1)</code></pre>
<p><img src="ProgressReport_files/figure-html/unnamed-chunk-14-1.png" width="1152" /></p>
<pre class="r"><code># overview at estimated latent function
ggarrange(
#nAGQ=0
df %&gt;% 
  filter(id %in% rand_id) %&gt;%
  group_by(id, bin) %&gt;%
  summarise(num = sum(Y)) %&gt;%
  left_join(df_est_latent_0AGQ, by = c(&quot;id&quot;, &quot;bin&quot;)) %&gt;%
  # mutate(eta_hat = exp(eta_hat)/(1+exp(eta_hat))) %&gt;%
  ggplot()+
  geom_line(aes(x=bin, y=eta_hat, group = id))+
  geom_point(aes(x=bin, y = num, group = id), size = 0.5)+
  facet_wrap(~id, scales = &quot;free&quot;)+
  # geom_vline(xintercept = c(465, 545, 555, 1185, 1195, 1205), col = &quot;red&quot;,
  #            linewidth = 0.5, alpha = 0.6)+
  labs(x = &quot;Bin&quot;, y = &quot;Estimated latent function/number of active minutes&quot;, title = &quot;nAGQ=0&quot;)+
  ylim(-25, 10),

#nAGQ=5
df %&gt;% 
  filter(id %in% rand_id) %&gt;%
  group_by(id, bin) %&gt;%
  summarise(num = sum(Y)) %&gt;%
  left_join(df_est_latent_5AGQ, by = c(&quot;id&quot;, &quot;bin&quot;)) %&gt;%
  # mutate(eta_hat = exp(eta_hat)/(1+exp(eta_hat))) %&gt;%
  ggplot()+
  geom_line(aes(x=bin, y=eta_hat, group = id))+
  geom_point(aes(x=bin, y = num, group = id), size = 0.5)+
  facet_wrap(~id, scales = &quot;free&quot;)+
  # geom_vline(xintercept = c(465, 545, 555, 1185, 1195, 1205), col = &quot;red&quot;,
  #            linewidth = 0.5, alpha = 0.6)+
  labs(x = &quot;Bin&quot;, y = &quot;Estimated latent function/number of active minutes&quot;, title = &quot;nAGQ=5&quot;)+
  ylim(-25, 10),

nrow = 1
)</code></pre>
<p><img src="ProgressReport_files/figure-html/unnamed-chunk-15-1.png" width="1152" /></p>
<p>Obviously the lower bound of estimated latent function got a lot
lower using AGQ than PIRLS. Nothing else seems to change much.</p>
</div>
</div>
<div id="fpca" class="section level3" number="1.2.3">
<h3><span class="header-section-number">1.2.3</span> FPCA</h3>
<p>I would also like to compare the difference between AGQ (nAGQ=5) and
PIRLS (nAGQ=0)</p>
<ul>
<li>AGQ has 16 eigenfunctions while PIRLS has 18</li>
</ul>
<pre class="r"><code># nAGQ=0
mat_est_unique_0AGQ &lt;- matrix(df_est_latent_0AGQ$eta_hat, nrow=N, ncol=n_bin, byrow = F) 
# row index subject, column binned time
fpca_mod_0AGQ &lt;- fpca.face(mat_est_unique_0AGQ, pve = 0.95, argvals = mid, var=T)

K_0AGQ &lt;- ncol(fpca_mod_0AGQ$efunctions) # 18 eigenfunctions</code></pre>
<pre class="r"><code># nAGQ=5
mat_est_unique_5AGQ &lt;- matrix(df_est_latent_5AGQ$eta_hat, nrow=N, ncol=n_bin, byrow = F) 
# row index subject, column binned time
fpca_mod_5AGQ &lt;- fpca.face(mat_est_unique_5AGQ, pve = 0.95, argvals = mid, var=T)

K_5AGQ &lt;- ncol(fpca_mod_5AGQ$efunctions) # 18 eigenfunctions</code></pre>
<ul>
<li>Population mean function</li>
</ul>
<pre class="r"><code># population mean
par(mfrow=c(1, 2))
plot(mid, fpca_mod_0AGQ$mu, type = &quot;l&quot;, xlab = &quot;bin&quot;, ylab = &quot;mean&quot;, main = &quot;nAGQ=0&quot;, ylim = c(-25, 0))
plot(mid, fpca_mod_5AGQ$mu, type = &quot;l&quot;, xlab = &quot;bin&quot;, ylab = &quot;mean&quot;, main = &quot;nAGQ=5&quot;, ylim = c(-25, 0))</code></pre>
<p><img src="ProgressReport_files/figure-html/unnamed-chunk-18-1.png" width="672" />
- PC functions</p>
<pre class="r"><code># eigenfunctions
par(mfrow=c(4,2))

plot(mid, fpca_mod_0AGQ$efunctions[, 1], type=&quot;l&quot;, xlab=&quot;bin&quot;, ylab=&quot;PC1&quot;, main = &quot;nAGQ=0&quot;)
plot(mid, fpca_mod_5AGQ$efunctions[, 1], type=&quot;l&quot;, xlab=&quot;bin&quot;, ylab=&quot;PC1&quot;, main = &quot;nAGQ=5&quot;)

plot(mid, fpca_mod_0AGQ$efunctions[, 2], type=&quot;l&quot;, xlab=&quot;bin&quot;, ylab=&quot;PC2&quot;)
plot(mid, fpca_mod_5AGQ$efunctions[, 2], type=&quot;l&quot;, xlab=&quot;bin&quot;, ylab=&quot;PC3&quot;)

plot(mid, fpca_mod_0AGQ$efunctions[, 3], type=&quot;l&quot;, xlab=&quot;bin&quot;, ylab=&quot;PC3&quot;)
plot(mid, fpca_mod_5AGQ$efunctions[, 3], type=&quot;l&quot;, xlab=&quot;bin&quot;, ylab=&quot;PC3&quot;)

plot(mid, fpca_mod_0AGQ$efunctions[, 4], type=&quot;l&quot;, xlab=&quot;bin&quot;, ylab=&quot;PC4&quot;)
plot(mid, fpca_mod_5AGQ$efunctions[, 4], type=&quot;l&quot;, xlab=&quot;bin&quot;, ylab=&quot;PC4&quot;)</code></pre>
<p><img src="ProgressReport_files/figure-html/unnamed-chunk-19-1.png" width="576" /></p>
<ul>
<li>Individual covariance matrix</li>
</ul>
<pre class="r"><code># covariance
heatmap(fpca_mod_0AGQ$VarMats[[which(unique(df_est_latent$id)==rand_id[1])]],
        Rowv = NA, Colv = NA, main = &quot;nAGQ=0&quot;)</code></pre>
<p><img src="ProgressReport_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<pre class="r"><code>heatmap(fpca_mod_5AGQ$VarMats[[which(unique(df_est_latent$id)==rand_id[1])]],
        Rowv = NA, Colv = NA, main = &quot;nAGQ=5&quot;)</code></pre>
<p><img src="ProgressReport_files/figure-html/unnamed-chunk-20-2.png" width="672" /></p>
<p>Compared PIRLS and AGQ (5 nodes):</p>
<ul>
<li>The lower bound of mean function went up</li>
<li>Shapes of PC functions didn’t change a lot</li>
<li>Covariance matrix looks a lot more different! Near-diagonal
correlation got higher and off-diagonal got lower.</li>
</ul>
</div>
</div>
<div id="dynamic-prediction" class="section level2" number="1.3">
<h2><span class="header-section-number">1.3</span> Dynamic
prediction</h2>
<div id="maximum-likelihood" class="section level3" number="1.3.1">
<h3><span class="header-section-number">1.3.1</span> Maximum
likelihood</h3>
<ul>
<li>Boundary hitting issues with numeric maximization</li>
<li>Warning message: Steady solution not reached</li>
<li>Large variation in both predicted values and scores. The numeric
solution from maximum likelihood is obviously not stable. It is very
likely to fail to converge.</li>
<li>Perhaps it can be fixed with numeric options? But I’d rather not go
that route because it depend on chance a lot.</li>
<li>After setting nAGQ=0, the problem got better for some subjects, but
not the others.</li>
</ul>
<pre class="r"><code># Out-of-sample prediction
# maximum observations time: 360, 720, 1080
# nAGQ=0
df_pred &lt;- df_est_latent_0AGQ %&gt;% filter(id %in% rand_id)
df_pred[, &#39;pred_t360&#39;] &lt;- df_pred[, &#39;pred_t720&#39;] &lt;- df_pred[, &#39;pred_t1080&#39;] &lt;- NA
score_out_mat &lt;- array(NA, dim = c(length(rand_id), K_0AGQ, 3))
# dim indexes subject, eigenfunction and max obs time respectively

# nAGQ=5
df_pred2 &lt;- df_est_latent_5AGQ %&gt;% filter(id %in% rand_id)
df_pred2[, &#39;pred_t360&#39;] &lt;- df_pred2[, &#39;pred_t720&#39;] &lt;- df_pred2[, &#39;pred_t1080&#39;] &lt;- NA
score_out_mat2 &lt;- array(NA, dim = c(length(rand_id), K_5AGQ, 3))</code></pre>
<pre class="r"><code># prediction for a single subject
# nAGQ=0
for(i in rand_id){
  df_i &lt;- df %&gt;% filter(id==i) %&gt;% mutate(bin=as.numeric(bin))
  # prediction
  pred_t360 &lt;- out_samp_dyn_pred(df_new = df_i %&gt;% filter(sind&lt;=360),
                                 fpca_fit = fpca_mod_0AGQ, K = K_0AGQ)
  pred_t720 &lt;- out_samp_dyn_pred(df_new = df_i %&gt;% filter(sind&lt;=720),
                                 fpca_fit = fpca_mod_0AGQ, K = K_0AGQ)
  pred_t1080 &lt;- out_samp_dyn_pred(df_new = df_i %&gt;% filter(sind&lt;=1080),
                                  fpca_fit = fpca_mod_0AGQ, K = K_0AGQ)
  # prediction in container
  df_pred[df_pred$id==i, &#39;pred_t360&#39;] &lt;- pred_t360$eta_pred
  df_pred[df_pred$id==i, &#39;pred_t720&#39;] &lt;- pred_t720$eta_pred
  df_pred[df_pred$id==i, &#39;pred_t1080&#39;] &lt;- pred_t1080$eta_pred
  # score in container
  # score_out_mat[i, ,1] &lt;- pred_t360$score_out
  # score_out_mat[i, ,2] &lt;- pred_t720$score_out
  # score_out_mat[i, ,3] &lt;- pred_t1440$score_out
}

# df_pred</code></pre>
<pre class="r"><code># prediction for a single subject
# nAGQ=5
for(i in rand_id){
  df_i &lt;- df %&gt;% filter(id==i) %&gt;% mutate(bin=as.numeric(bin))
  # prediction
  pred_t360 &lt;- out_samp_dyn_pred(df_new = df_i %&gt;% filter(sind&lt;=360),
                                 fpca_fit = fpca_mod_5AGQ, K = K_5AGQ)
  pred_t720 &lt;- out_samp_dyn_pred(df_new = df_i %&gt;% filter(sind&lt;=720),
                                 fpca_fit = fpca_mod_5AGQ, K = K_5AGQ)
  pred_t1080 &lt;- out_samp_dyn_pred(df_new = df_i %&gt;% filter(sind&lt;=1080),
                                  fpca_fit = fpca_mod_5AGQ, K = K_5AGQ)
  # prediction in container
  df_pred2[df_pred2$id==i, &#39;pred_t360&#39;] &lt;- pred_t360$eta_pred
  df_pred2[df_pred2$id==i, &#39;pred_t720&#39;] &lt;- pred_t720$eta_pred
  df_pred2[df_pred2$id==i, &#39;pred_t1080&#39;] &lt;- pred_t1080$eta_pred
  # score in container
  # score_out_mat[i, ,1] &lt;- pred_t360$score_out
  # score_out_mat[i, ,2] &lt;- pred_t720$score_out
  # score_out_mat[i, ,3] &lt;- pred_t1440$score_out
}

# df_pred</code></pre>
<pre class="r"><code>df_pred$pred_t360[df_pred$bin&lt;=360] &lt;- NA
df_pred$pred_t720[df_pred$bin&lt;=720] &lt;- NA
df_pred$pred_t1080[df_pred$bin&lt;=1080] &lt;- NA
# df_pred %&gt;% filter(id == rand_id[1])

df_pred2$pred_t360[df_pred2$bin&lt;=360] &lt;- NA
df_pred2$pred_t720[df_pred2$bin&lt;=720] &lt;- NA
df_pred2$pred_t1080[df_pred2$bin&lt;=1080] &lt;- NA</code></pre>
<pre class="r"><code>ggarrange(
df_pred %&gt;%
  ggplot()+
  geom_line(aes(x=bin, y=eta_hat, group = id))+
  geom_line(aes(x=bin, y = pred_t360, col = &quot;360&quot;, group = id))+
  geom_line(aes(x=bin, y = pred_t720, col = &quot;720&quot;, group = id))+
  geom_line(aes(x=bin, y = pred_t1080, col = &quot;1080&quot;, group = id))+
  facet_wrap(~id,scales = &quot;free&quot;)+
  labs(title = &quot;nAGQ=0&quot;),

df_pred2 %&gt;%
  ggplot()+
  geom_line(aes(x=bin, y=eta_hat, group = id))+
  geom_line(aes(x=bin, y = pred_t360, col = &quot;360&quot;, group = id))+
  geom_line(aes(x=bin, y = pred_t720, col = &quot;720&quot;, group = id))+
  geom_line(aes(x=bin, y = pred_t1080, col = &quot;1080&quot;, group = id))+
  facet_wrap(~id,scales = &quot;free&quot;)+
  labs(title = &quot;nAGQ=5&quot;),

nrow = 1
)</code></pre>
<p><img src="ProgressReport_files/figure-html/unnamed-chunk-25-1.png" width="1152" /></p>
</div>
<div id="bayes-method-like-laplace-approximation" class="section level3"
number="1.3.2">
<h3><span class="header-section-number">1.3.2</span> Bayes method like
Laplace approximation</h3>
<pre class="r"><code>#PIRLS
df_pred &lt;- df_est_latent_0AGQ %&gt;% filter(id %in% rand_id)
df_pred[, &#39;pred_t360&#39;] &lt;- df_pred[, &#39;pred_t720&#39;] &lt;- df_pred[, &#39;pred_t1080&#39;] &lt;- NA
score_out_mat &lt;- array(NA, dim = c(length(rand_id), K_0AGQ, 3))

# AGQ
df_pred2 &lt;- df_est_latent_5AGQ %&gt;% filter(id %in% rand_id)
df_pred2[, &#39;pred_t360&#39;] &lt;- df_pred2[, &#39;pred_t720&#39;] &lt;- df_pred2[, &#39;pred_t1080&#39;] &lt;- NA
score_out_mat &lt;- array(NA, dim = c(length(rand_id), K_5AGQ, 3))</code></pre>
<pre class="r"><code># unique id
# prediction for a single subject
source(here(&quot;Code/OutsampBayes.R&quot;))</code></pre>
<pre><code>## 
## Attaching package: &#39;LaplacesDemon&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:lubridate&#39;:
## 
##     dst, interval</code></pre>
<pre><code>## The following object is masked from &#39;package:purrr&#39;:
## 
##     partial</code></pre>
<pre><code>## 
## Attaching package: &#39;mvtnorm&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:LaplacesDemon&#39;:
## 
##     dmvt, rmvt</code></pre>
<pre class="r"><code># PRILS
for(i in rand_id){
  df_i &lt;- df %&gt;% filter(id==i) 
  
  # prediction 
  ## up to 360
  pred_t1 &lt;- out_pred_laplace(fpca_mod_0AGQ, df_i %&gt;% filter(sind&lt;=360))
  eta_pred_t1 &lt;- fpca_mod_0AGQ$mu+fpca_mod_0AGQ$efunctions%*%pred_t1$score_out
  df_pred[df_pred$id == i, &#39;pred_t360&#39;] &lt;- eta_pred_t1
  ## up to 720
  pred_t2 &lt;- out_pred_laplace(fpca_mod_0AGQ, df_i %&gt;% filter(sind&lt;=720))
  eta_pred_t2 &lt;- fpca_mod_0AGQ$mu+fpca_mod_0AGQ$efunctions%*%pred_t2$score_out
  df_pred[df_pred$id == i, &#39;pred_t720&#39;] &lt;- eta_pred_t2
  ## up to 1080
  pred_t3 &lt;- out_pred_laplace(fpca_mod_0AGQ, df_i %&gt;% filter(sind&lt;=1080))
  eta_pred_t3 &lt;- fpca_mod_0AGQ$mu+fpca_mod_0AGQ$efunctions%*%pred_t3$score_out
  df_pred[df_pred$id == i, &#39;pred_t1080&#39;] &lt;- eta_pred_t3

}

df_pred$pred_t360[df_pred$bin&lt;=360] &lt;- NA
df_pred$pred_t720[df_pred$bin&lt;=720] &lt;- NA
df_pred$pred_t1080[df_pred$bin&lt;=1080] &lt;- NA</code></pre>
<pre class="r"><code># AGQ
for(i in rand_id){
  df_i &lt;- df %&gt;% filter(id==i) 
  
  # prediction 
  ## up to 360
  pred_t1 &lt;- out_pred_laplace(fpca_mod_5AGQ, df_i %&gt;% filter(sind&lt;=360))
  eta_pred_t1 &lt;- fpca_mod_5AGQ$mu+fpca_mod_5AGQ$efunctions%*%pred_t1$score_out
  df_pred2[df_pred2$id == i, &#39;pred_t360&#39;] &lt;- eta_pred_t1
  ## up to 720
  pred_t2 &lt;- out_pred_laplace(fpca_mod_5AGQ, df_i %&gt;% filter(sind&lt;=720))
  eta_pred_t2 &lt;- fpca_mod_5AGQ$mu+fpca_mod_5AGQ$efunctions%*%pred_t2$score_out
  df_pred2[df_pred2$id == i, &#39;pred_t720&#39;] &lt;- eta_pred_t2
  ## up to 1080
  pred_t3 &lt;- out_pred_laplace(fpca_mod_5AGQ, df_i %&gt;% filter(sind&lt;=1080))
  eta_pred_t3 &lt;- fpca_mod_5AGQ$mu+fpca_mod_5AGQ$efunctions%*%pred_t3$score_out
  df_pred2[df_pred2$id == i, &#39;pred_t1080&#39;] &lt;- eta_pred_t3

}

df_pred2$pred_t360[df_pred2$bin&lt;=360] &lt;- NA
df_pred2$pred_t720[df_pred2$bin&lt;=720] &lt;- NA
df_pred2$pred_t1080[df_pred2$bin&lt;=1080] &lt;- NA
# df_pred %&gt;% filter(id == rand_id[1])</code></pre>
<pre class="r"><code>ggarrange(
df_pred %&gt;%
  ggplot()+
  geom_line(aes(x=bin, y=eta_hat, group = id))+
  geom_line(aes(x=bin, y = pred_t360, col = &quot;360&quot;, group = id))+
  geom_line(aes(x=bin, y = pred_t720, col = &quot;720&quot;, group = id))+
  geom_line(aes(x=bin, y = pred_t1080, col = &quot;1080&quot;, group = id))+
  facet_wrap(~id,scales = &quot;free&quot;)+
  labs(title = &quot;nAGQ=0&quot;),
  
df_pred2 %&gt;%
  ggplot()+
  geom_line(aes(x=bin, y=eta_hat, group = id))+
  geom_line(aes(x=bin, y = pred_t360, col = &quot;360&quot;, group = id))+
  geom_line(aes(x=bin, y = pred_t720, col = &quot;720&quot;, group = id))+
  geom_line(aes(x=bin, y = pred_t1080, col = &quot;1080&quot;, group = id))+
  facet_wrap(~id,scales = &quot;free&quot;)+
  labs(title = &quot;nAGQ=5&quot;), nrow = 1
)</code></pre>
<p><img src="ProgressReport_files/figure-html/unnamed-chunk-29-1.png" width="1152" /></p>
</div>
<div id="a-closer-look-at-the-maximum-likelihood-solution"
class="section level3" number="1.3.3">
<h3><span class="header-section-number">1.3.3</span> A closer look at
the maximum likelihood solution</h3>
<ul>
<li>Take participant 74440 with observation up to 720 as example</li>
<li>Solve the score equation. Roots are obviously very large and
differentiations are nowhere near zero. Steady solution was not
reached</li>
<li>Does it have to do with numeric options? But it would be difficult
to find a numeric option that suits all participants. Increasing maxiter
did not help.</li>
</ul>
<pre class="r"><code># numerically maximize log likelihood
try_fit1 &lt;- multiroot(f=llh_div, start = rep(0, K_0AGQ), 
                      df_new=df %&gt;% filter(id == 74440 &amp; sind&lt;=720), 
          fpca_fit=fpca_mod_0AGQ)</code></pre>
<pre><code>## Warning in stode(y, times, func, parms = parms, ...): steady-state not reached</code></pre>
<pre class="r"><code>try_fit1$root</code></pre>
<pre><code>##  [1]  2200.528737 -1626.674098  -441.709038   200.628848   -42.945781
##  [6]   150.604435   100.306408   -80.853015   -89.794708    11.226366
## [11]   -82.696908   -51.980302   -25.683901   -35.021109    -2.970762
## [16]    41.845647    -8.190575   -37.523270</code></pre>
<ol style="list-style-type: decimal">
<li>When calculating the likelihood for a single subject, we would have
a likelihood for each bin, and take the product across all bins. The
assumes bins are independent conditional on FPCA estimates within the
same individual. Is that valid?</li>
<li>Does it have to do with number of eigenfunctions? I have used all of
the 18/16 eigenfunctions so far. Maybe try only 4 eigenfunctions
instead?</li>
</ol>
<p>This one helps when observed track is long, but still failed to
converge when the observed track is short…Just the opposite from using
all eigenfunctions!</p>
<pre class="r fold-show"><code># new function for score equation that uses k eigenfunctions
llh_div_kpc &lt;- function(xi, df_new, fpca_fit, kpc){
  
  # observations 
  ns &lt;- as.vector(table(df_new$bin)) # number of observations
  hs &lt;- df_new %&gt;% group_by(bin) %&gt;% summarize_at(&quot;Y&quot;, sum) %&gt;% 
    select(Y) %&gt;% unlist()# number of success
  nf &lt;- ns-hs # number of failure
  max_bin &lt;- length(unique(df_new$bin)) # assume no skipped bins 
  
  # fpca objects
  phi_m &lt;- fpca_fit$efunctions[1:max_bin, 1:kpc]
  f0_m &lt;- fpca_fit$mu[1:max_bin]
  eta_s &lt;- f0_m + phi_m %*% xi
  tao &lt;- diag(fpca_fit$evalues[1:kpc])
  
  # derivative
  val1 &lt;- hs-ns*(exp(eta_s)/(1+exp(eta_s)))
  div &lt;- colSums(apply(phi_m, 2, function(x)x*val1))-xi%*%solve(tao)
  return(div)
}

# observation up to 360
multiroot(f=llh_div_kpc, start = rep(0, 4), 
          df_new=df %&gt;% filter(id == 74440 &amp; sind&lt;=360), 
          fpca_fit=fpca_mod_0AGQ, kpc=4)</code></pre>
<pre><code>## Warning in stode(y, times, func, parms = parms, ...): steady-state not reached</code></pre>
<pre><code>## $root
## [1] -300.4800  397.5951 -134.7040  -16.7734
## 
## $f.root
##          [,1]      [,2]     [,3]     [,4]
## [1,] 2.427681 -13.43247 -7.03011 10.60862
## 
## $iter
## [1] 100
## 
## $estim.precis
## [1] 9.368242</code></pre>
<pre class="r fold-show"><code># observation up to 720
multiroot(f=llh_div_kpc, start = rep(0, 4), 
          df_new=df %&gt;% filter(id == 74440 &amp; sind&lt;=720), 
          fpca_fit=fpca_mod_0AGQ, kpc=4)</code></pre>
<pre><code>## $root
## [1] -16.013352 -20.727776   5.397181  -8.300871
## 
## $f.root
##               [,1]         [,2]          [,3]         [,4]
## [1,] -1.964764e-11 3.785144e-11 -8.103362e-11 1.151784e-11
## 
## $iter
## [1] 7
## 
## $estim.precis
## [1] 3.751264e-11</code></pre>
<pre class="r fold-show"><code># observation up to 1080
multiroot(f=llh_div_kpc, start = rep(0, 4), 
          df_new=df %&gt;% filter(id == 74440 &amp; sind&lt;=1080), 
          fpca_fit=fpca_mod_0AGQ, kpc=4)</code></pre>
<pre><code>## $root
## [1]  7.065175  9.132904 10.570788  5.117815
## 
## $f.root
##               [,1]         [,2]          [,3]         [,4]
## [1,] -6.089752e-08 7.916887e-08 -2.769592e-07 4.142851e-08
## 
## $iter
## [1] 6
## 
## $estim.precis
## [1] 1.146135e-07</code></pre>
</div>
</div>
<div id="discussion" class="section level2" number="1.4">
<h2><span class="header-section-number">1.4</span> Discussion</h2>
<ol style="list-style-type: decimal">
<li><p>In the local GLMMs step: perhaps also mess around with nAGQ for
the simulation study, since we would have a “true” latent function to
compare the performance of PIRLS, Laplace and AGQ. We can decide based
on the results.</p></li>
<li><p>For the prediction part: Bayes is probably a better approached
since 1) base R cannot do multivariate optimition; and 2) Laplace seems
to numerically stable.</p></li>
</ol>
<p>However, LaplaceDemon would fail for some subjects. Perhaps I should
switch to stan since it is more tractable?</p>
<ol start="3" style="list-style-type: decimal">
<li>Manuscript problems:</li>
</ol>
<ul>
<li>“Out-of-sample” predictions needs to be done on samples that are not
used for model fitting.(Now we are kind of pretending a new observation
happens to have the same record with one of the training samples)</li>
<li>Splitting samples in simulation and data application?</li>
</ul>
<!-- ## Failed prediction  -->
<!-- - Laplace approximation failed for some subjects -->
<!-- - Only when the observed track is short (up to 360) -->
<!-- - These subjects have something in common -->
<!-- ```{r} -->
<!-- # compare subjects that failed at Laplace Approximation with others -->
<!-- rand_id <- sample(unique(df$SEQN), size = 4) -->
<!-- com_id <- c(rand_id, sample(skip_id, 4)) -->
<!-- df_est_latent %>% filter(id %in% com_id) %>% -->
<!--   left_join(df %>% select(SEQN, Z, sind) %>% -->
<!--               rename(id = SEQN, Y=Z, bin = sind), by = c("id", "bin")) %>% -->
<!--   mutate(type = ifelse(id %in% rand_id, "Prediction", "Failed prediction")) %>% -->
<!--   ggplot(aes(x=bin, y=eta_hat, col = type))+ -->
<!--   geom_line()+ -->
<!--   geom_vline(xintercept = 360)+ -->
<!--   facet_wrap(~type+id, nrow = 2, ncol = 4)+ -->
<!--   labs(y = "Estimated latend function", x = "Time", col = "") -->
<!-- ``` -->
<!-- ## Subjects with complete prediction -->
<!-- ```{r fig_prob, warning=FALSE} -->
<!-- df_est_latent %>% -->
<!--   filter(id %in% rand_id) %>% -->
<!--   mutate(pred_t1080 = ifelse(bin<=1080, NA, pred_t1080), -->
<!--          pred_t720 = ifelse(bin<=720, NA, pred_t720), -->
<!--          pred_t360 = ifelse(bin<=360, NA, pred_t360)) %>% -->
<!--   mutate_at(vars(pred_t360, pred_t720, pred_t1080), function(x)exp(x)/(1+exp(x))) %>% -->
<!--   left_join(df %>% select(SEQN, Z, sind) %>% -->
<!--               rename(id = SEQN, Y=Z, bin = sind), by = c("id", "bin")) %>% -->
<!--   ggplot()+ -->
<!--     geom_line(aes(x=bin, y = pred_t360, col = "360"), linetype = "dashed")+ -->
<!--     geom_line(aes(x=bin, y = pred_t720, col = "720"),linetype = "dashed")+ -->
<!--     geom_line(aes(x=bin, y = pred_t1080, col = "1080"),linetype = "dashed")+ -->
<!--     geom_point(aes(x=bin, y = Y, col = "Outcome"), size = 0.5)+ -->
<!--     facet_wrap(~id)+ -->
<!--     labs(x = "Time", y = "Estimated latent function (probablity scale)", -->
<!--          col = "Maximum observation time") -->
<!-- ``` -->
<!-- ```{r fig_eta, warning=FALSE} -->
<!-- df_est_latent %>% -->
<!--   filter(id %in% rand_id) %>% -->
<!--   mutate(pred_t1080 = ifelse(bin<=1080, NA, pred_t1080), -->
<!--          pred_t720 = ifelse(bin<=720, NA, pred_t720), -->
<!--          pred_t360 = ifelse(bin<=360, NA, pred_t360)) %>% -->
<!--   ggplot()+ -->
<!--     geom_line(aes(x=bin, y = pred_t360, col = "360"), linetype = "dashed")+ -->
<!--     geom_line(aes(x=bin, y = pred_t720, col = "720"),linetype = "dashed")+ -->
<!--     geom_line(aes(x=bin, y = pred_t1080, col = "1080"),linetype = "dashed")+ -->
<!--     geom_line(aes(x=bin, y = eta_hat, col = "eta_hat"), linetype = "solid" )+ -->
<!--     facet_wrap(~id)+ -->
<!--     labs(x = "Time", y = "Estimated latent function") -->
<!-- ``` -->
<!-- ## AUC on subjects with complete prediction -->
<!-- - Use interpolation to fill in gaps between bins -->
<!-- ```{r} -->
<!-- # subjects with complete prediction -->
<!-- N <- length(unique(df$SEQN))-length(skip_id) -->
<!-- eval_id <- setdiff(unique(df$SEQN), skip_id) -->
<!-- J <- max(df$sind) -->
<!-- # containter: row index subject, column index time -->
<!-- eta_mat_t360 <- eta_mat_t720 <- eta_mat_t1080 <- matrix(NA, nrow = N, ncol = J) -->
<!-- # interpolation -->
<!-- for(i in seq_along(eval_id)){ -->
<!--    eta_mat_t360[i, ] <- approx(x=df_est_latent$bin[df_est_latent$id==eval_id[i]], -->
<!--                                y=df_est_latent$pred_t360[df_est_latent$id==eval_id[i]],  -->
<!--                                xout = 1:J)$y -->
<!--    eta_mat_t720[i, ] <- approx(x=df_est_latent$bin[df_est_latent$id==eval_id[i]], -->
<!--                                y=df_est_latent$pred_t720[df_est_latent$id==eval_id[i]],  -->
<!--                                xout = 1:J)$y -->
<!--    eta_mat_t1080[i, ] <- approx(x=df_est_latent$bin[df_est_latent$id==eval_id[i]], -->
<!--                                y=df_est_latent$pred_t1080[df_est_latent$id==eval_id[i]],  -->
<!--                                xout = 1:J)$y -->
<!--   } -->
<!-- ``` -->
<!-- ```{r} -->
<!-- # true outcome -->
<!-- Y_mat <- df$Z[df$SEQN %in% eval_id] -->
<!-- Y_mat <- matrix(Y_mat, nrow = N, ncol = J, byrow = T)  -->
<!-- ``` -->
<!-- ```{r AUC_fGFPCA} -->
<!-- # maximum observations time -->
<!-- t_vec <- c(360, 720, 1080) -->
<!-- # AUC container -->
<!-- auc_mat <- matrix(NA, 3, 3) -->
<!-- colnames(auc_mat) <- c("360", "720", "1080") -->
<!-- rownames(auc_mat) <- c("(360, 720]", "(720, 1080]", "(1080, 1440]") -->
<!-- # Calculate AUC -->
<!-- auc_mat[1, 1] <- performance(prediction(as.vector(eta_mat_t360[, 361:720]),  -->
<!--                                    as.vector(Y_mat[, 361:720])), -->
<!--                         "auc")@y.values[[1]] -->
<!-- auc_mat[2, 1] <- performance(prediction(as.vector(eta_mat_t360[, 721:1080]),  -->
<!--                                    as.vector(Y_mat[, 721:1080])), -->
<!--                         "auc")@y.values[[1]] -->
<!-- auc_mat[3, 1] <- performance(prediction(as.vector(eta_mat_t360[, 1081:1435]),  -->
<!--                                    as.vector(Y_mat[, 1081:1435])), -->
<!--                         "auc")@y.values[[1]] -->
<!-- auc_mat[2, 2] <- performance(prediction(as.vector(eta_mat_t720[, 721:1080]),  -->
<!--                                    as.vector(Y_mat[, 721:1080])), -->
<!--                         "auc")@y.values[[1]] -->
<!-- auc_mat[3, 2] <- performance(prediction(as.vector(eta_mat_t720[, 1081:1435]),  -->
<!--                                    as.vector(Y_mat[, 1081:1435])), -->
<!--                         "auc")@y.values[[1]] -->
<!-- auc_mat[3, 3] <- performance(prediction(as.vector(eta_mat_t1080[, 1081:1435]),  -->
<!--                                    as.vector(Y_mat[, 1081:1435])), -->
<!--                         "auc")@y.values[[1]] -->
<!-- auc_mat -->
<!-- ``` -->
<!-- ## Reference method -->
<!-- - GLMMadaptive? -->
<!-- - fcr on lowess smoothed track? -->
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
