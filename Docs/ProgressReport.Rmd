---
title: "Progress Report"
author: "Ying Jin"
date: "2023-09-15"
output: 
  html_document:
    self_contained: no
    number_sections: true
    code_folding: hide
    toc: true
    toc_depth: 3
    toc_float: true
    font: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

set.seed(516)

library(here)
library(tidyverse)
library(ggpubr)
library(refund)
library(lme4)
library(ROCR)
library(GLMMadaptive)
library(kableExtra)
library(mvtnorm)
library(mgcv)
library(splines)
theme_set(theme_minimal())

df <- read_rds(here("Data/nhanes_bi.rds"))
# load(here("Data/ApplOutput_fGFPCA.RData"))

```


# NHANES data

## Data overview

- 8763 subjects, 1440 measures each
- no missingness

```{r}
N <- length(unique(df$SEQN)) # sample size 8763
J <- max(df$sind) # 1440 measures per subject
```


```{r}
rand_id <- sample(unique(df$SEQN), size = 4) # "toy" sample

df %>% 
  filter(SEQN %in% rand_id) %>%
  ggplot()+
  geom_point(aes(x=sind, y=Z), size = 0.5)+
  facet_wrap(~SEQN)+
  labs(x="Time", y = "Activity", title = "A brief overview of the outcome")
```

## fGFPCA model fitting

```{r}
# code
source(here("Code/GLMM-FPCA.R")) 
# use pred_latent function to estimate latent function 
source(here("Code/OutSampMLE.R"))
# source(here("Code/OutsampBayes.R"))
```

### Bin Data

- Bin every 10 consecutive observations

```{r, message=FALSE}
# bin data
bin_w <- 10 # bin width
n_bin <- J/bin_w # number of bins
brks <- seq(0, J, by = bin_w) # cutoff points
mid <- (brks+bin_w/2)[1:n_bin] # mid points


df$bin <- cut(df$sind, breaks = brks, include.lowest = T, labels = mid)
df$bin <- as.numeric(as.character(df$bin))
# head(df)

df %>% 
  filter(SEQN %in% rand_id) %>%
  group_by(SEQN, bin) %>%
  summarise(num = sum(Z)) %>%
  ggplot()+
  geom_point(aes(x=bin, y=num), size = 0.5)+
  facet_wrap(~SEQN)+
  labs(x="Time", y = "Activity", title = "Number of active nimutes within each bin")

df <- df %>% rename(id = SEQN, Y=Z)
```
### Data split

```{r}
id_vec <- unique(df$id)
train_id <- sample(id_vec, size = 0.8*N, replace = FALSE) # 7010 subjects for training
test_id <- setdiff(id_vec, train_id) # 1753 subjects for testing
```

- 80% (7017) subjects for model fitting, 20% (1753) subjects for out-of-sample prediction

### Local GLMM

- Used glmer with PIRLS (nAGQ=0) for local GLMMs step
- Fit on the training set

```{r, cache=TRUE}
# fit model on the training set
df_train <- df %>% filter(id %in% train_id)
train_bin_lst <- split(df_train, f = df_train$bin)

# local GLMM and estimate latent function
# use PIRLS (nAGQ=0) to avoid near-unidentifiability issues 
t1=Sys.time()
df_est_latent <- lapply(train_bin_lst, function(x){pred_latent(x, n_node = 0)}) 
t2= Sys.time()
t_local_glmm <- t2-t1 
```


```{r}
df_est_latent <- bind_rows(df_est_latent) %>% 
  select(-sind, -Y) %>% distinct(.) 
# example estimated latent function
df %>% 
  filter(id %in% rand_id) %>%
  left_join(df_est_latent, by = c("id", "bin")) %>%
  mutate(eta_hat = exp(eta_hat)/(1+exp(eta_hat))) %>%
  ggplot()+
  geom_line(aes(x=sind, y=eta_hat, group = id))+
  geom_point(aes(x=sind, y = Y, group = id), size = 0.5)+
  facet_wrap(~id, scales = "free")+
  labs(x = "Time", y = "Estimated latent function (probablity scale)")
```

```{r, message=FALSE}

df %>% 
  filter(id %in% sample(train_id, size = 4)) %>%
  group_by(id, bin) %>%
  summarise(num = sum(Y)) %>%
  left_join(df_est_latent, by = c("id", "bin")) %>%
  ggplot()+
  geom_point(aes(x=bin, y=num), size = 0.5)+
  geom_line(aes(x=bin, y=eta_hat, group = id))+
  facet_wrap(~id)+
  labs(x="Time", y = "Estimated latent function (original scale")
```

### FPCA

```{r}
mat_est_unique <- matrix(df_est_latent$eta_hat,
                         nrow=length(train_id), 
                         ncol=n_bin, byrow = F) # row index subject, column binned time
#dim(mat_est_unique)

t1 <- Sys.time()
fpca_mod <- fpca.face(mat_est_unique, argvals = mid, var=T)
t2 <- Sys.time()
t_fpca <- t2-t1 # 3.21 seconds to fit fPCA model

# dim(fpca_mod$efunctions) # 27 eigenfunctions total
# fpca_mod$evalues
K <- 4
```

```{r}
plot(mid, fpca_mod$mu, type = "l", xlab = "bin", ylab = "mean")
```


```{r, fig.height=8, fig.width=8}
par(mfrow=c(2,2))

plot(mid, fpca_mod$efunctions[, 1], type="l", xlab="bin", ylab="PC1")
plot(mid, fpca_mod$efunctions[, 2], type="l", xlab="bin", ylab="PC2")
plot(mid, fpca_mod$efunctions[, 3], type="l", xlab="bin", ylab="PC3")
plot(mid, fpca_mod$efunctions[, 4], type="l", xlab="bin", ylab="PC4")
```

```{r}
# plot correlation matrix
heatmap(cov2cor(fpca_mod$VarMats[[1]]), Rowv = NA, Colv = NA, main = "Correlation")
```

### Debias (step 4)

According to the fGFPCA paper, in this step we do two things: 

1. Interpolate the eigenfunctions with B-spline basis back to the original grid
2. Debias the eigenvalues with GLMM (mgcv::bam, mgcm::gamm or gamm4:gamm) to be used for Laplace approximation

- The bias was caused by the misspecification of latent process (assume constant effect over smooth function). 
- What needs re-evaluation are eigenvalues (variance estimates). They will be used in Laplace Approximation for out-of-sample prediction. 
- We do not need to debias individual scores because the scores for training sample will not be used for out-of-sample prediction
- How are the eigenvalues biased? 
- It looks like the FPCA step now estimates mean and eigenfunctions, while each eigenfunction serves as a random slope. Can we fit such model with an offset of mean/intercept function? 

First I'd like to try to fit the debias GLMM model on the full training set with un-interpolated eigenfunctions. The model failed because of vector memory exhaustion. Shall I: 
- Decrease the density of grid (increase bin width)?
- Decrease sample size? 
- Increase memory (re-set to 100GB). But still took a lot of time (more than one hour). The model actually did not finish fitting. 

```{r, eval=FALSE, class.source = "fold-show"}
df_phi <- data.frame(bin=mid, fpca_mod$efunctions[, 1:4])
colnames(df_phi) <- c("bin", paste0("phi", 1:4))

df_train <- df_train %>% left_join(df_phi, by = "bin")
df_train$id <- as.factor(df_train$id)

# usethis::edit_r_environ()
t1 <- Sys.time()
debias_glmm <- bam(Y ~ s(bin, bs="cr", k=10)+
                     s(id, by=phi1, bs="re")+
                     s(id, by=phi2, bs="re")+
                     s(id, by=phi3, bs="re")+
                     s(id, by=phi4, bs="re"), 
                   family = binomial, data=df_train, method = "fREML")
t2 <- Sys.time()
t_debias <- t2=t1

```

Then I would like to interpolate eigenfunctions using B-spline basis functions: 

- Would you interpolate mean and eigenfunctions separately (in my case, five interpolations)? 
- I did not find the reeval_efunctions function

```{r, fig.height=5, fig.width=5, class.source = "fold-show"}
# mean
interp_mean <- interpSpline(mid, fpca_mod$mu)
full_mean <- predict(interp_mean, x=1:J)
plot(mid, fpca_mod$mu, ylab = "mean", pch = 20)
lines(full_mean$x, full_mean$y, col = "blue")

# PCs
interp_phi1 <- interpSpline(mid, fpca_mod$efunctions[, 1])
full_phi1 <- predict(interp_phi1, x=1:J)
plot(mid, fpca_mod$efunctions[, 1], ylab = "pc1", pch=20)
lines(full_phi1$x, full_phi1$y, col = "blue")

interp_phi2 <- interpSpline(mid, fpca_mod$efunctions[, 2])
full_phi2 <- predict(interp_phi2, x=1:J)
plot(mid, fpca_mod$efunctions[, 2], ylab = "pc2", pch=20)
lines(full_phi2$x, full_phi2$y, col = "blue")

interp_phi3 <- interpSpline(mid, fpca_mod$efunctions[, 3])
full_phi3 <- predict(interp_phi3, x=1:J)
plot(mid, fpca_mod$efunctions[, 3], ylab = "pc3", pch=20)
lines(full_phi3$x, full_phi3$y, col = "blue")

interp_phi4 <- interpSpline(mid, fpca_mod$efunctions[, 4])
full_phi4 <- predict(interp_phi4, x=1:J)
plot(mid, fpca_mod$efunctions[, 4], ylab = "pc4", pch=20)
lines(full_phi4$x, full_phi4$y, col = "blue")
```

Perhaps I should try with the simulation data. 

## Out-of-sample prediction

follow up on the last report, I'll add some details here.

Let's set: 

- $s$: bin index
- $t_{m_s}$: midpoint time of bin s
- $\mathscr{T}_s$: all observation points in bin s
- $Y_i^s$ all observations in bin s from subject i. $Y_i^S := \{Y_i(t_j), t_j \in \mathscr{T}_s\}$

Let's assume we have a new subject $u$ with maximum observation time $T_u$. Then the log-likelihood of this new subject would be:

$$
l_u=\sum_{t_{m_s}<T_u}log(h(Y_u^s))+\hat{\eta}_u(t_{m_s})T(Y_u^s)-log(A[\hat{\eta}_u(t_{m_s})])
$$


where $\hat{\eta}_u(t_{m_s}) = \hat{\mu}_0(t_{m_s})+\sum_{k=1}^K \xi_{uk}\hat{\phi}(t_{m_s})$. 

From FPCA model (or the debias step to be added), we obtain $\hat{\mu}_0$, $\hat{\phi}_k$, as well as the variance estimates of $\xi_k$: $\hat{\lambda}_k$, and variance of residual process $\hat{\sigma^2}$.

With all these estimates, we wanna use Laplace approximation to find the $\xi_{uk}$ that maximizes $l_u$. 


I am using a package called **LaplacesDemon**, following this example https://cran.r-project.org/web/packages/LaplacesDemon/vignettes/LaplacesDemonTutorial.pdf

```{r, class.source = "fold-show"}
library(LaplacesDemon)
```


Now I am going to use one subject as an example to go over the approximation procedure:

First we write out the model: 

- Prior distribution: $\xi_{uk} \sim N(0, \hat{\lambda}_k)$
- Posterior distribution: 


\begin{aligned}
l(\mathbf{Y_u}|\mathbf{\xi}_u) &= \sum_{t_{m_s}<T_u}l(Y_u^s|\mathbf{\xi}_u) \\

Y_u^s|\mathbf{\xi}_u & \sim Binomial(n_s, p_s) \\

g^{-1}(p_s) = \hat{\eta}(t_{m_s}) &= \hat{\mu}_0(t_{m_s})+\sum_{k=1}^K \xi_{uk}\hat{\phi}(t_{m_s})
\end{aligned}



And $n_s$ is the number of observations in bin s. 

Bayes theroem:

$$
l(\mathbf{\xi}_u|\mathbf{Y}_u) \propto l(\mathbf{Y}_u|\mathbf{\xi}_u)+l(\mathbf{\xi}_u)
$$


```{r, class.source = "fold-show"}
## model
Model <- function(parm, Data){
  xi <- parm[Data$pos.xi]
  
  # log-prior
  xi.prior <- dmvnorm(xi, mean = rep(0, Data$J), sigma=Data$tao, log = TRUE)
  
  # log-posterior likelihood
  eta <- Data$f0+Data$X %*% xi
  p <- exp(eta)/(1+exp(eta))
  LL <- sum(dbinom(x=Data$y, size = Data$n, prob=p, log = TRUE)) # log likelihood of Y|xi
  LP <- LL+sum(xi.prior) # unnormalized joint log likelihood of (Y, xi)
  
  # output
  Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP,
                   yhat=Data$y, parm=parm)
  return(Modelout)
}
```


- Then we feed in data for one observation

```{r, class.source = "fold-show"}
rand_id <- test_id[15]
df_new <- df %>% filter(id == rand_id & sind <= 540) # observation until 9am

# put data into correct format
ns <- as.vector(table(df_new$bin)) # number of observations
hs <- df_new %>% group_by(bin) %>% summarize_at("Y", sum) %>% select(Y) %>% unlist()# number of success
nf <- ns-hs # number of failure
max_bin <- length(unique(df_new$bin)) # assume new skipped bins  
df_new2 <- data.frame(bin = unique(df_new$bin), ns, hs, nf)  
  
# extract estimates from FPCA model to be used in prior distribution
tao <- diag(fpca_mod$evalues[1:K]) # variance of xi

# extract estimates from FPCA model to be used in posterior distribution
f0 <- fpca_mod$mu[1:max_bin]
N <- nrow(df_new2) # "sample size" which is in fact number of observed bins in this case for a new subject
n <- df_new2$ns # number of experiements at each bin
y <- df_new2$hs # outcome, number of 1
X <- fpca_mod$efunctions[1:max_bin, 1:K] # eigen/PC functions
J <- K # number of parameters/scores
mon.names <- "LP"

# parameter names
name_lst <- as.list(rep(0, J))
names(name_lst) <- paste("xi", 1:J, sep = "")
parm.names <- as.parm.names(name_lst) # names of parameters to estimate
pos.xi <- grep("xi", parm.names)

# data
MyData <- list(J=J, X=X, mon.names=mon.names,
                 parm.names=parm.names, pos.xi=pos.xi, y=y, n=n, tao=tao, f0=f0)
```


```{r, class.source = "fold-show"}
# fit laplace approximation
# initial values generated by GIV function in the same package
Fit <- LaplaceApproximation(Model, Data=MyData)
```

- Summarizing output

```{r, class.source = "fold-show", fig.height=8, fig.width=8}
# summarizing output
print(Fit)
plot(Fit, MyData)
```

```{r}
# prediction
eta_pred_out <- fpca_mod$mu+fpca_mod$efunctions[, 1:K]%*%Fit$Summary1[, "Mode"]

data.frame(bin = mid, eta_pred = eta_pred_out) %>% 
  right_join(df %>% filter(id==rand_id) %>%  select(Y, sind, bin), by = "bin") %>%
  ggplot()+
  geom_point(aes(x=sind, y = Y))+
  geom_line(aes(x=sind, y = exp(eta_pred)/(1+exp(eta_pred))))+
  labs(title = rand_id)
  
```



# Full data application output

## fGFPCA

- Still need to project eigenfunctions to the full grid

```{r}
# results generated from DataApplNHANES.R
load(here("Data/ApplOutput_fGFPCA.RData"))
```




```{r message=FALSE, warning=FALSE}
df_test_full <- df %>% 
  filter(id %in% test_id) %>%
  filter(!id %in% skip_id) %>%
  left_join(df_test %>% select(id, bin, pred_t540, pred_t780, pred_t1020), by = c("id", "bin"))

rand_id2 <- sample(unique(df_test$id), 4)
# without interpolation, assume constant latent function value in each bin
df_test_full%>%
  filter(id %in% rand_id2) %>%
  mutate_at(vars(pred_t540, pred_t780, pred_t1020), function(x)exp(x)/(1+exp(x))) %>%
  ggplot()+
    geom_line(aes(x=bin, y = pred_t540, col = "9am"))+
    geom_line(aes(x=bin, y = pred_t780, col = "1pm"))+
    geom_line(aes(x=bin, y = pred_t1020, col = "5pm"))+
    geom_point(aes(x=bin, y = Y, col = "Outcome"), size = 0.2)+
    facet_wrap(~id)+
    labs(x = "Time", y = "Estimated latent function (probablity scale)",
         title = "Constant latent function within bin")
```

### Numeric problems 

- Three subjects had failed approximation, all happened with the shortest observed track
- Does it have to do with still the length of observed track? Or the arbitrary choices in the binning procedure (bin width). 
- Per last meeting, adding some details on laplace approximation


```{r, fig.height=3, fig.width=9}
df %>% filter(id %in% skip_id) %>% 
  ggplot()+
  geom_point(aes(x=sind, y=Y), size = 0.2)+
  facet_wrap(~id)+
  geom_vline(xintercept = c(540))
```


<!-- Calculate AUC -->


```{r}
# break by prediction window
lst_auc <- df_test_full %>% 
  mutate(window = cut(sind, breaks = c(0, 540, 780, 1020, 1260, 1440), 
                      labels = c("0-9am", "9am-1pm", "1pm-5pm", "5pm-9pm","9pm-12pm"), 
                      include.lowest = T))
lst_auc <- split(lst_auc, f=lst_auc$window)
```


```{r}
# no interpolation
# up to 9am
auc_t540 <- lst_auc[2:4] %>% 
  lapply(function(x)performance(prediction(x$pred_t540, x$Y), measure = "auc"))
auc_t540 <- lapply(auc_t540, function(x){x@y.values[[1]]}) %>% unlist()

# up to 1pm
auc_t780 <- lst_auc[3:4] %>% 
  lapply(function(x)performance(prediction(x$pred_t780, x$Y), measure = "auc"))
auc_t780 <- lapply(auc_t780, function(x){x@y.values[[1]]}) %>% unlist()

# up to 5pm
auc_t1020 <- lst_auc[4] %>% 
  lapply(function(x)performance(prediction(x$pred_t1020, x$Y), measure = "auc"))
auc_t1020 <- lapply(auc_t1020, function(x){x@y.values[[1]]}) %>% unlist()
```




## GLMMadaptvie

- when fitting model with **random intercept + slope** on the same full training set (even after scaling covariates): Error: vector memory exhausted (limit reached?).
- when fitting model with **only random intercept** on the same full training set: Error regarding a large coefficient value. One fix is to re-scale covariates. So I re-scaled minute index by dividing them by J, so the range is within (0, 1], and tried again. This fitting procedure took 20.72 minutes to finish



```{r}
load(here("Data/ApplOutput_GLMMadaptive.RData"))
```

- `r round(t_est_adglmm, 2)` minutes on model fitting and `r round(t_pred_adglmm, 2)` minutes on out-of-sample prediction. 

- Below is the model fit on the training set

```{r}
summary(adglmm_mod)
```



```{r}
df_test_full2 <- df_test_full %>%
  filter(id %in% rand_id2)%>%
  select(id, sind, Y)

df_test_full2$pred_t540 <- df_test_full2$pred_t780 <- df_test_full2$pred_t1020 <- NA 
for(i in rand_id2){
  df_test_full2[df_test_full2$id==i & df_test_full2$sind>540, "pred_t540"]<- adglmm_pred_t540$newdata2 %>% filter(id ==i) %>% select(pred)
  
  df_test_full2[df_test_full2$id==i & df_test_full2$sind>780, "pred_t780"]<- adglmm_pred_t780$newdata2 %>% filter(id ==i) %>% select(pred)
  
  df_test_full2[df_test_full2$id==i & df_test_full2$sind>1020, "pred_t1020"]<- adglmm_pred_t1020$newdata2 %>% filter(id ==i) %>% select(pred)
}

# colSums(is.na(df_test_full2))
```



- Below is an example of predicted probablity on four random subjects

```{r, warning=FALSE}
df_test_full2 %>% 
  mutate_at(vars(pred_t540, pred_t780, pred_t1020), 
            function(x)exp(x)/(1+exp(x))) %>%
  ggplot()+
    geom_line(aes(x=sind, y = pred_t540, col = "9am"))+
    geom_line(aes(x=sind, y = pred_t780, col = "1pm"))+
    geom_line(aes(x=sind, y = pred_t1020, col = "5pm"))+
    geom_point(aes(x=sind, y = Y, col = "Outcome"), size = 0.2)+
    facet_wrap(~id)+
    labs(x = "Time", y = "Estimated latent function (probablity scale)")
```

<!-- Calculate AUC  -->



```{r}
# up to 540
J <- 1440
auc_t540_adglmm <- adglmm_pred_t540$newdata2 %>%
  filter(!id %in% skip_id) %>%
  mutate(sind=sind*J) %>%
  mutate(window = cut(sind, breaks = c(540, 780, 1020, 1260, 1440), 
                      labels = c("9am-1pm", "1pm-5pm", "5pm-9pm","9pm-12pm"), 
                      include.lowest = T))
auc_t540_adglmm <- split(auc_t540_adglmm, f=auc_t540_adglmm$window)
auc_t540_adglmm <- auc_t540_adglmm[1:3] %>% 
  lapply(function(x)performance(prediction(x$pred, x$Y), measure = "auc"))
auc_t540_adglmm <- lapply(auc_t540_adglmm, function(x){x@y.values[[1]]}) %>% unlist()

# up to 780
auc_t780_adglmm <- adglmm_pred_t780$newdata2 %>%
  filter(!id %in% skip_id) %>%
  mutate(sind=sind*J) %>%
  mutate(window = cut(sind, breaks = c(780, 1020, 1260, 1440), 
                      labels = c("1pm-5pm", "5pm-9pm","9pm-12pm"), 
                      include.lowest = T))
auc_t780_adglmm <- split(auc_t780_adglmm, f=auc_t780_adglmm$window)
auc_t780_adglmm <- auc_t780_adglmm[1:2] %>% 
  lapply(function(x)performance(prediction(x$pred, x$Y), measure = "auc"))
auc_t780_adglmm <- lapply(auc_t780_adglmm, function(x){x@y.values[[1]]}) %>% unlist()

# up to 1020
auc_t1020_adglmm <- adglmm_pred_t1020$newdata2 %>%
  filter(!id %in% skip_id) %>%
  mutate(sind=sind*J) %>%
  mutate(window = cut(sind, breaks = c(1020, 1260, 1440), 
                      labels = c("5pm-9pm","9pm-12pm"), 
                      include.lowest = T))
auc_t1020_adglmm <- split(auc_t1020_adglmm, f=auc_t1020_adglmm$window)
auc_t1020_adglmm <- auc_t1020_adglmm[1] %>% 
  lapply(function(x)performance(prediction(x$pred, x$Y), measure = "auc"))
auc_t1020_adglmm <- lapply(auc_t1020_adglmm, function(x){x@y.values[[1]]}) %>% unlist()


```


## Compare fGFPCA and GLMMadaptive

- To use the same sample for comparison, I'd exclude the subjects with failed Laplace approximation from the test set for both methods

```{r}
options(knitr.kable.NA = '')
tb_auc <- data.frame(auc_t540, 
           c(NA, auc_t780), 
           c(NA, NA, auc_t1020), 
        
           auc_t540_adglmm,
           c(NA, auc_t780_adglmm), 
           c(NA, NA, auc_t1020_adglmm))

colnames(tb_auc) <- rep(c("9am", "1pm", "5pm"), 2)
```


```{r}
# time
t_fit <- c(t_local_glmm+t_fpca, t_est_adglmm)
units(t_fit) <- "mins"

t_test <- c(t_pred, t_pred_adglmm)
# units(t_pred)

t_total <- t_fit+t_test


idx1 <- idx2 <- idx3 <- c(1, 3, 3)
names(idx1) <- c("Total time", as.character(round(t_total, 2)))
names(idx2) <- c("Time on prediction", as.character(round(t_test, 2)))
names(idx3) <- c("Time on model fitting", as.character(round(t_fit, 2)))

t_fit <- as.character(round(t_fit, 2))
t_test <- as.character(round(t_test, 2))
t_total <- as.character(round(t_total, 2))

```



```{r}
tb_auc %>%
  kable(digit = 4, table.attr = "style = \"color: black;\"") %>%
  kable_styling(full_width = F) %>% 
  add_header_above(header=idx1) %>%
  add_header_above(header=idx2) %>%
  add_header_above(header=idx3) %>%
  add_header_above(c(" "=1, "fGFPCA"=3, "GLMMadaptive"=3))
```

- fGFPCA does not always have higher AUC, but definitely most of the time. 
- In fact there are two scenarios where GLMMadaptive outperforms fGFPCA slightly: 1) given 9am to predict 1-5pm; and 2) given 1pm and predict 5-9pm. Both of them have a 4-hour interval apart from the maximum observation time and and prediction window. Could it have anything to do the cyclic trend in the data? Is 4h the length of a cycle? I tried to look at it but really couldn't see anything...there are too many points lumped together! 


```{r}
df %>% filter(id %in% rand_id2) %>%
  ggplot()+
  geom_point(aes(x=sind, y=Y), size = 0.2)+
  geom_vline(xintercept = c(540, 780, 1020, 1260))+
  scale_x_continuous(breaks = c(540, 780, 1020, 1260),
                     labels = c("9am", "1pm", "5pm", "9pm"))+
  facet_wrap(~id)
```

## Discussion

- What would be the reference method to compare our performance to? Original GLMMadaptvie perhaps is feasible on the entire dataset. But what should we use if we wanna do sub-sample comparison? Should we even do that for this data application (or simulation alone)? 
- There are still three participants with numeric problem! Shall we extend the observation track even more? I suspect that may cause still the same issue just on different subjects. 
- Is interpolation really necessary? It does not improve performance very much but consumes time. Also, should I try more sophisticated methods for grid extension? 
- Should I calculate subject-wise AUC and average them? But some subjects are all zero within the prediction window, making AUC for this single subjects impossible to calculate...

