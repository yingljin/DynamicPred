---
title: "Progress Report on One Simulated Dataset"
author: "Ying Jin"
date: "`r Sys.Date()`"
output: 
  html_document:
    self_contained: yes
    number_sections: true
    code_folding: hide
    toc: true
    toc_depth: 3
    toc_float: true
    font: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

set.seed(516)


library(here)
library(tidyverse)
library(ggpubr)
library(refund)
library(lme4)
library(ROCR)
library(GLMMadaptive)
library(kableExtra)
library(knitr)
library(mvtnorm)
library(mgcv)
library(splines)
library(LaplacesDemon)
theme_set(theme_minimal())

# load simulation results
# load(here("Data/SimOutput_fGFPCA.RData"))
```


I am now on the way to add the debias step to the whole algorithm. Just so that I have a set of "true parameters" for comparison, I am gonna use the simulation dataset for this part.

# Generation mechanism

- I am gonna generate only one dataset as an example
- 500 individuals each with 1000 measures

\[\begin{aligned}
Y_i(t) & \sim Bernoulli(\frac{exp(\eta_i(t))}{1+exp(\eta_i(t))}) \\
\eta_i(t) &= f_0(t)+ \xi_{i1}\sqrt{2}sin(2\pi t)+\xi_{i2}\sqrt{2}cos(2\pi t)+\xi_{i3}\sqrt{2}sin(4\pi t)+\xi_{i4}\sqrt{2}cos(4\pi t)
\end{aligned}\]

where:

- $t$ is equal-spaced on $[0, 1]$
- $f_0(t)=0$
- $\xi_k \sim N(0, \lambda_k)$, and $\lambda_k = 1, 0.5, 0.25, 0.125$ for k = 1, 2, 3, 4 respectively. 
- In fact, since there is a constant factor $\sqrt{2}$ before each eigenfunction so that they are orthogonal

```{r gen_data, class.source='show'}
N <- 500 # sample size
J <- 1000 # number of observation points

t = seq(0,1,len=J) # observations points

# mean function
f_0 <- function(s) 0 

#eigenfunctions 
K <- 4 # number of eigenfunctions
phi <- sqrt(2)*cbind(sin(2*pi*t),cos(2*pi*t),
                     sin(4*pi*t),cos(4*pi*t))

# eigenvalues
lambda = 0.5^(0:(K-1)) 
```

```{r}
# generated training data 
## score
xi <- matrix(rnorm(N*K),N,K)
xi <- xi %*% diag(sqrt(lambda))
  
# subject-specific random effect
b_i <- xi %*% t(phi); # of size N by J
  
# latent gaussian function
eta_i <- t(vapply(1:N, function(x){
    f_0(t) + b_i[x,]
  }, numeric(J)))
  
# outcome binary function
Y_i <- matrix(rbinom(N*J, size=1, prob=plogis(eta_i)), 
                N, J, byrow=FALSE)
  
# format into dataframe
# id = subject identifier (factor variable)
# sind = numeric value corresponding to the observed functional domain
# Y = functional response (binary)
# sind_inx = numeric value associated with order of "sind"
#            this is not necessary, but may be of convenience when you implement the method
train_df <- data.frame(id = factor(rep(1:N, each=J)),
                 t = rep(t, N), 
                 Y = as.vector(t(Y_i)),
                 eta_i = as.vector(t(eta_i)),
                 sind = rep(1:J, N))

# visualization
rand_id <- sample(N, size = 4)
train_df %>% filter(id %in% rand_id) %>% 
  ggplot()+
  geom_point(aes(x=sind, y=Y), size = 0.5)+
  geom_line(aes(x=sind, y=plogis(eta_i)), col = "red")+
  #geom_line(aes(x=sind_inx, y=eta_i), col = "blue")+
  facet_wrap(~id)
```

```{r}
# functions
source(here("Code/GLMM-FPCA.R")) 
source(here("Code/OutSampBayes.R"))
```


# fGFPCA model fitting

- Training sample: N=500

## Bin data

- Bin every 10 consecutive observations

```{r, message=FALSE}
# bin data
bin_w <- 10 # bin width
n_bin <- J/bin_w # number of bins
brks <- seq(0, J, by = bin_w) # cutoff points
mid <- (brks+bin_w/2)[1:n_bin] # mid points

train_df$bin <- cut(train_df$sind, breaks = brks, include.lowest = T, labels = mid)
train_df$bin <- as.numeric(as.character(train_df$bin))
# unique(df$bin)

train_df %>% 
  filter(id %in% rand_id) %>%
  group_by(id, bin) %>%
  summarise(num = sum(Y)) %>%
  ggplot()+
  geom_point(aes(x=bin, y=num), size = 0.5)+
  facet_wrap(~id)+
  labs(x="Time", y = "Activity", title = "Number of active nimutes within each bin")
```

## Local GLMM

- Used glmer with PIRLS (nAGQ=0) for local GLMMs step
- Fit on the training set (N=500)
- Use original time index ([0, 1]) as independent variable


```{r}
# fit model on the training set
train_bin_lst <- split(train_df, f = train_df$bin)
# length(train_bin_lst)
# lapply(train_bin_lst, dim)

# local GLMM and estimate latent function
# use PIRLS (nAGQ=0) to avoid near-unidentifiability issues 
t1=Sys.time()
df_est_latent <- lapply(train_bin_lst, function(x){pred_latent(x, n_node = 0)}) 
t2= Sys.time()
t_local_glmm <- t2-t1 # less than 5 seconds
```


```{r}
df_est_latent <- bind_rows(df_est_latent) 
# head(df_est_latent)

# example estimated latent function
train_id <- unique(train_df$id)
rand_id <- sample(train_id, 4)

df_est_latent %>% 
  filter(id %in% rand_id) %>%
  mutate(eta_hat = exp(eta_hat)/(1+exp(eta_hat))) %>%
  mutate(eta_i = exp(eta_i)/(1+exp(eta_i))) %>%
  ggplot()+
  geom_line(aes(x=t, y=eta_hat, group = id, col = "estimated"))+
  geom_line(aes(x=t, y=eta_i, group = id, col = "true"))+
  geom_point(aes(x=t, y = Y, group = id), size = 0.5)+
  facet_wrap(~id, scales = "free")+
  labs(x = "Time", y = "Estimated latent function (probablity scale)")
```

## FPCA

```{r}
uni_eta_hat <- df_est_latent %>% filter(bin==sind)
mid_t <- df_est_latent$t[df_est_latent$bin==df_est_latent$sind]
mid_t <- unique(mid_t) # the time points correspoinding to bin midpoints

mat_est_unique <- matrix(uni_eta_hat$eta_hat,
                         nrow=length(train_id), 
                         ncol=n_bin, byrow = F) 
# row index subject, column binned time
# dim(mat_est_unique)

t1 <- Sys.time()
fpca_mod <- fpca.face(mat_est_unique, argvals = mid_t, var=T)
t2 <- Sys.time()
t_fpca <- t2-t1 # less than one second
```

- Estimated mean

```{r}
plot(mid_t, fpca_mod$mu, type = "l", xlab = "bin", ylab = "Mean")
```

- Estimated eigenfunctions

```{r, fig.height=8, fig.width=8}
par(mfrow=c(2,2))

plot(mid_t, fpca_mod$efunctions[, 1], type="l", xlab="bin", ylab="PC1")
plot(mid_t, fpca_mod$efunctions[, 2], type="l", xlab="bin", ylab="PC2")
plot(mid_t, fpca_mod$efunctions[, 3], type="l", xlab="bin", ylab="PC3")
plot(mid_t, fpca_mod$efunctions[, 4], type="l", xlab="bin", ylab="PC4")
```

- Correlation matrix

```{r}
# plot correlation matrix
heatmap(cov2cor(fpca_mod$VarMats[[1]]), Rowv = NA, Colv = NA, main = "Correlation")
```

- Estimated eigenvalues

```{r, class.source='fold-show'}
# estimated eigenvalues from FPCA (biased)
fpca_mod$evalues[1:4]
```


## Debias (step 4)

According to the fGFPCA paper, in this step we do two things: 

1. Project the eigenfunctions with B-spline basis back to the original grid
2. Debias the eigenvalues with GLMM (mgcv::bam, mgcm::gamm or gamm4:gamm) to be used for Laplace approximation


### Projection with B-spline basis

<!-- https://bookdown.org/ssjackson300/Machine-Learning-Lecture-Notes/splines.html -->

- Use fastGFPCA::reeval_efunctions. Cannot install the package so used the source code directly.
- I would like to use time points on the [0, 1] scale as evaluation points when creating basis functions.
- Also, take number of knots and order from the FPCA model in step 3 (knots = 35, cubic spline basis). 
- We don't project the mean function here. Instead, we will use the re-evaluated mean function from the next part (bam model used for debias)

```{r, class.source='fold-show'}
# order of b splines
p <- 3 

# knots
knots <- 35
knots_values <- seq(-p, knots + p, length = knots + 1 + 2 *p)/knots
knots_values <- knots_values * (max(mid_t) - min(mid_t)) + min(mid_t)

# evaluate B-splines on binned grid
B <- spline.des(knots = knots_values, x = mid_t, ord = p + 1,
                            outer.ok = TRUE)$design
# evaluate B-splines on original grid
Bnew <- spline.des(knots = knots_values, x = unique(train_df$t), ord = p + 1,
                  outer.ok = TRUE)$design

# project binned eigenfunctions onto the original grid
efunctions_new <- matrix(NA, J, K)
for(k in 1:K){
    lm_mod <- lm(fpca_mod$efunctions[,k] ~ B-1)
    efunctions_new[,k] <- Bnew %*% coef(lm_mod)
}

```


```{r, fig.height=8, fig.width=8}
# visualization
par(mfrow=c(2,2))

plot(mid_t, fpca_mod$efunctions[, 1], xlab="bin", ylab="PC1", pch=20, cex = 0.5)
lines(t, efunctions_new[, 1], col="blue")

plot(mid_t, fpca_mod$efunctions[, 2], xlab="bin", ylab="PC2", pch=20, cex = 0.5)
lines(t, efunctions_new[, 2], col="blue")

plot(mid_t, fpca_mod$efunctions[, 3], xlab="bin", ylab="PC3", pch=20, cex = 0.5)
lines(t, efunctions_new[, 3], col="blue")

plot(mid_t, fpca_mod$efunctions[, 4], xlab="bin", ylab="PC4", pch=20, cex = 0.5)
lines(t, efunctions_new[, 4], col="blue")
```


### Debias with eigenvalues

- The bias was caused by the misspecification of latent process (assume constant effect over smooth function). 
- What needs re-evaluation are eigenvalues (variance estimates) and the mean function. They will be used in Laplace Approximation for out-of-sample prediction. 
- We do not need to debias individual scores because the scores for training sample will not be used for out-of-sample prediction. And out-of-sample scores are not estimated by FPCA but Laplace approximation.
- I will use the projected eigenfunctions from above. 
- I am using mgcv::bam and setting method = "fREML" and discrete = TRUE to speed up computation

```{r}
# dim(fpca_mod$efunctions)
df_phi <- data.frame(t = t, efunctions_new)
colnames(df_phi) <- c("t", paste0("phi", 1:4))

# train_df$bin <- as.numeric(as.character(train_df$bin))
train_df <- train_df %>% left_join(df_phi, by = "t")
train_df$id <- as.factor(train_df$id)
```


```{r, class.source = "fold-show"}
# usethis::edit_r_environ()
t1 <- Sys.time()
debias_glmm <- bam(Y ~ s(t, bs="cc", k=10)+
                     s(id, by=phi1, bs="re")+
                     s(id, by=phi2, bs="re")+
                     s(id, by=phi3, bs="re")+
                     s(id, by=phi4, bs="re"), 
                   family = binomial, data=train_df, 
                   method = "fREML",
                   discrete = TRUE)
t2 <- Sys.time()
t_debias <- t2-t1 # less than 45 seconds
```

We will need the mean function from this model, derived below:

The shape of the function depends very much on how the basis functions are constructed. For example, cubic spline would lead to a linear mean and cyclic cubic spline would lead to the function below. What's a good way to decided? 


```{r, class.source = "fold-show"}
new_mu <- predict(debias_glmm, type = "terms")
length(unique(new_mu[,1]))

new_mu <- unique(new_mu[,1])
plot(t, new_mu, type = "l")
```


And now I try to extract the variance of scores (debiased eigenvalues), using the conclusion that connects smoothing parameters to variance

$$\hat{p}_k=1/\hat{\lambda}_k^2$$

```{r, class.source='fold-show'}
# transformed smoothing parameters
new_lambda <- 1/debias_glmm$sp[2:5]
new_lambda
# new_lambda/lambda
# new_lambda/fpca_mod$evalues[1:4]
```


This eigenvalues, as well as the eigenfunctions, are actually not on the same scale of true values. By looking into the fpca.face source code, eigenfunctions are scaled by the square root of the number of total observations points. Since our model was fitted on the binnd grid, the actual eigenvaleus and eigenfunctions would be:

\[\begin{aligned}
\phi_k(t) & = \sqrt{n_{bin}}\hat{\phi}_k(t)  \\
\lambda_k & = \hat{\lambda}_k/n_{bin}
\end{aligned}\]

Let's rescale these quatities as below

- Re-scaled eigenvalues

```{r}
scaled_lambda <- new_lambda/n_bin
```

- Re-scaled eigenfunctions

```{r, fig.height=8, fig.width=8}
efunctions_scaled <- efunctions_new*sqrt(n_bin)

par(mfrow=c(2,2))

plot(t, efunctions_scaled[, 1], type="l", xlab="bin", ylab="PC1")
plot(t, efunctions_scaled[, 2], type="l", xlab="bin", ylab="PC2")
plot(t, efunctions_scaled[, 3], type="l", xlab="bin", ylab="PC3")
plot(t, efunctions_scaled[, 4], type="l", xlab="bin", ylab="PC4")

```

- Both much closer to their true values

# Out-of-sample prediction with Laplace approximation

I will now estimate individual score of test sample using Laplace Approximation, and calculated the predicted track.

- Generate additional 200 subjects for out-of-sample performance evaluation
- Maximum observations time: 0.2, 0.4, 0.6, 0.8
- Prediction window: 0.2-0.4, 0.4-0.6, 0.6-0.8, 0.8-1.0

## Generate test data 

```{r}
# generated testing data 
N_test <- 200
# xi_test <- matrix(rnorm(N_test*K), N_test, K)
xi_test <- rmvnorm(N_test, mean = rep(0, K), sigma = diag(lambda))
b_test <- xi_test %*% t(phi) # of size N by J
# dim(b_test)
  
# latent gaussian function
eta_test <- t(vapply(1:N_test, function(x){
    f_0(x) + b_test[x,]
  }, numeric(J)))

# outcome binary function
Y_test <- matrix(rbinom(N_test*J, size=1, prob=plogis(eta_test)), 
                N_test, J, byrow=FALSE)
  
# format into dataframe
test_df <- data.frame(id = factor(rep(1:N_test, each=J)),
                 t = rep(t, N_test), 
                 Y = as.vector(t(Y_test)),
                 eta_i = as.vector(t(eta_test)),
                 sind = rep(1:J, N_test))
# bin
# test_df$bin <- cut(test_df$sind, breaks = brks, include.lowest = T, labels = mid)
# test_df$bin <- as.numeric(as.character(test_df$bin))
test_df$id <- as.factor(test_df$id)
test_id <- unique(test_df$id)
```


## Laplace approximation




```{r model}
# the model
## now, with expanded mean and eigenfuncitons, a Bernoulli distribution at each time point
Model <- function(parm, Data){
  xi <- parm[Data$pos.xi]
  
  # log-prior
  xi.prior <- dmvnorm(xi, mean = rep(0, Data$K), sigma=Data$tao, log = TRUE)
  
  # log-posterior likelihood
  eta <- Data$f0+Data$X %*% xi
  p <- exp(eta)/(1+exp(eta))
  LL <- sum(dbern(x=Data$y, prob=p, log = TRUE)) # log likelihood of Y|xi
  LP <- LL+sum(xi.prior) # joint log likelihood of (Y, xi)
  
  # output
  Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LL,
                   yhat=Data$y, parm=parm)
  return(Modelout)
}
```


```{r}
# parameter names
name_lst <- as.list(rep(0, K))
names(name_lst) <- paste("xi", 1:K, sep = "")
parm.names <- as.parm.names(name_lst)
pos.xi <- grep("xi", parm.names)
# PGF <- function(Data) {
#   xi <- rnorm(Data$K)
#   return(xi)
# }
mon.names <- "LP"
```


```{r pred_debias_scale, results='hide'}
# approximation process
# container
pred_list <- list()
converge_vec <- matrix(NA, N_test, 4)
# score <- array(NA, dim=c(length(test_id), K, 4))
# dimensions: subject, eigenfunction, maximum observation time

# prediction for a single subject
t1 <- Sys.time()
# per subject
# seq_along(test_id)
for(i in seq_along(test_id)){
  df_i <- test_df %>% filter(id==test_id[i])
  
  # per max obs time
  for(tmax in c(0.2, 0.4, 0.6, 0.8)){
    df_it <- df_i %>% filter(t <= tmax)
    max_rid <- nrow(df_it)
  
    # into a list
    MyData <- list(K=K, 
                   X=efunctions_scaled[1:max_rid, ], 
                   mon.names=mon.names,
                   parm.names=parm.names, 
                   pos.xi=pos.xi, 
                   y=df_it$Y, 
                   tao=diag(scaled_lambda), f0=new_mu[1:max_rid])
    
    
    # fit laplace approximation
    Fit <- LaplaceApproximation(Model, parm = rep(0, K), Data=MyData, Method = "NM", Iterations = 1000,
                                CovEst = "Identity")
    converge_vec[i, which(c(0.2, 0.4, 0.6, 0.8)==tmax)] <- Fit$Converged
    # Fit <- LaplacesDemon(Model, Data=MyData, Initial.Values = parm = rep(0, K),  Method = "BFGS")
    score <- Fit$Summary1[, "Mode"]
    
    # prediction
    eta_pred_out <- new_mu+efunctions_new%*%score
    df_i[ , paste0("pred", tmax)] <- eta_pred_out[,1]
  }
  
  # df_i$pred0.2[df_i$t<=0.2] <- NA
  # df_i$pred0.4[df_i$t<=0.4] <- NA
  # df_i$pred0.6[df_i$t<=0.6] <- NA
  # df_i$pred0.8[df_i$t<=0.8] <- NA
  
  pred_list[[i]] <- df_i
}
t2 <- Sys.time()
t_pred <- t2-t1 # About 3.5 minutes
# mean(converge_vec)
```


```{r}
df_pred <- bind_rows(pred_list)

df_pred$pred0.2[df_pred$t<=0.2] <- NA
df_pred$pred0.4[df_pred$t<=0.4] <- NA
df_pred$pred0.6[df_pred$t<=0.6] <- NA
df_pred$pred0.8[df_pred$t<=0.8] <- NA

rand_test_id <- sample(test_id, 4)

p1<-df_pred %>%
  filter(id %in% rand_test_id) %>%
  mutate_at(vars(eta_i, pred0.2, pred0.4, pred0.6, pred0.8), function(x){exp(x)/(1+exp(x))}) %>%
  ggplot()+
  geom_point(aes(x=t, y=Y), size = 0.2)+
  geom_line(aes(x=t, y=eta_i, col = "True"))+
  geom_line(aes(x=t, y=pred0.2, col = "0.2"), linetype="dashed", na.rm=T)+
  geom_line(aes(x=t, y=pred0.4, col = "0.4"), linetype="dashed", na.rm=T)+
  geom_line(aes(x=t, y=pred0.6, col = "0.6"), linetype="dashed", na.rm=T)+
  geom_line(aes(x=t, y=pred0.8, col = "0.8"), linetype="dashed", na.rm=T)+
  facet_wrap(~id)+
  labs(title = "fGFPCA")

```


```{r}
# calcualte ISE
err1 <- df_pred %>%
  mutate(err1 = (pred0.2-eta_i)^2,
         err2 = (pred0.4-eta_i)^2,
         err3 = (pred0.6-eta_i)^2,
         err4 = (pred0.8-eta_i)^2) %>%
  select(id, t, starts_with("err"))

err1$window = cut(err1$t, breaks = seq(0, 1, by = 0.2), include.lowest = T)
# table(err1$window)
err1 <- split(err1, f = err1$window)

tb1 <- lapply(err1, function(x){
  x %>%  group_by(id) %>% 
    summarise_at(vars(err1, err2, err3, err4), sum) %>%
    summarise_at(vars(err1, err2, err3, err4), mean)
}) %>% bind_rows(.id = "Window")
```

```{r}
# calcualte AUC
auc_df1 <- df_pred %>%
  mutate(window = cut(t, breaks = seq(0, 1, by = 0.2), include.lowest = T)) %>%
  select(Y, starts_with("pred"), window)

obs_track <- seq(0.2, 0.8, by = 0.2)
pred_window <- unique(auc_df1$window)
auc_mat <- matrix(NA, length(pred_window), length(obs_track))

for(j in seq_along(obs_track)){
  for(i in (j+1):5){
    this_Y <- auc_df1$Y[auc_df1$window==pred_window[i]]
    this_pred <- auc_df1[auc_df1$window==pred_window[i],
                         paste0("pred", obs_track[j])]
    this_perf <- performance(prediction(this_pred, this_Y), measure = "auc")
    auc_mat[i ,j] <- this_perf@y.values[[1]]
  }
}

```




## Numeric stability

Following up on the discrepancy in predictive performance using scaled and unscaled estimates of eigenfunctions and eigenvalues, it is likely to be a issue of numeric stability. I would like to compare a few optimizers to see if they make any difference on the results. I'll record computation time, convergence rate using scaled and unscaled estimates, a brief summary of performance.

1. BFGS 
- computation time: 3.43min
- convergance rate: 94% (unscaled), 75% (scaled)
- performance is not too different

2. Nelder-Mead
- computation time: 3.48 min (unscaled), 3.03 min (scaled)
- convergance rate: 100% (unscaled), 100% (scaled)
- performance very similar. Scaled parameters can sometimes outperform unscaled ones.

3. Newton-Ralphson
- ran into numeric problems when using scaled parameter

Over all, I think Nelder-Mead is probably our bdst option. It converged under all situations and is also fast. When looking at ISE, scaled and unscaled estiamtes had similar performance. In fact they outperform each other five times each. 


Think about the estimates. The scaled estimates of eigenvalues are much smaller than the unscaled ones, meaning the prior distribution of score using scaled estimates would be much more centered thant the unscaled ones. This would also drive the posterior distribution to be in similar shapes. Since the distribution from scaled eigenvalues has such high density at certain values, could it be that the sampling quickly got trapped in a small interval that is far from the true optimal values? It did not happen when using Nelder-Mead optimizer and set maxiter=1000. 

Below I look at the convergence path for one arbitrary subject

```{r, results='hide'}
this_id <- sample(test_id, 1)
this_t <- 0.4

# data
df_it <- test_df %>% filter(id==this_id & t <= this_t)
max_rid <- nrow(df_it)

# use unscaled params
MyData_unscaled <- list(K=K, 
               X=efunctions_new[1:max_rid, ], 
               mon.names=mon.names,
               parm.names=parm.names, 
               pos.xi=pos.xi, 
               y=df_it$Y, 
               tao=diag(new_lambda), f0=new_mu[1:max_rid])
Fit_unscaled <- LaplaceApproximation(Model, parm = rep(0, K), Data=MyData_unscaled,
                                     Method = "BFGS", Iterations = 1000,
                            CovEst = "Identity")

# use scaled params
MyData_scaled <- list(K=K, 
               X=efunctions_scaled[1:max_rid, ], 
               mon.names=mon.names,
               parm.names=parm.names, 
               pos.xi=pos.xi, 
               y=df_it$Y, 
               tao=diag(scaled_lambda), f0=new_mu[1:max_rid])
Fit_scaled <- LaplaceApproximation(Model, parm = rep(0, K), 
                                   Data=MyData_scaled, Method = "BFGS", 
                                   Iterations = 1000,
                            CovEst = "Identity")

## iterative distribution, compared to true value
true_xi_it <- data.frame(name = colnames(Fit_unscaled$History),
                         true_xi = xi_test[this_id, ])
```


```{r}
ggarrange(
Fit_unscaled$History %>% as.data.frame() %>%
  mutate(iter = 1:nrow(Fit_unscaled$History)) %>%
  pivot_longer(starts_with("xi")) %>%
  left_join(true_xi_it, by = "name") %>%
  mutate(value = value/sqrt(n_bin)) %>%
  mutate_at(vars(value, true_xi), abs) %>%
  ggplot()+
  geom_point(aes(x=iter, y=value), size = 0.2)+
  geom_line(aes(x=iter, y = true_xi, col = "red"))+
  facet_wrap(~name)+
  labs(title = "Unscaled parameter"), 

Fit_scaled$History %>% as.data.frame() %>%
  mutate(iter = 1:nrow(Fit_scaled$History)) %>%
  pivot_longer(starts_with("xi")) %>%
  left_join(true_xi_it, by = "name") %>%
  mutate_at(vars(value, true_xi), abs) %>%
  ggplot()+
  geom_point(aes(x=iter, y=value), size = 0.2)+
  geom_line(aes(x=iter, y = true_xi, col = "red"))+
  facet_wrap(~name)+
  labs(title = "Scaled parameter"), nrow = 1, common.legend = T)
```

In this case the sampling bounced up and down, which is NM is supposed to do I think? But it reaches a stable status quickly with a few dozens of iterations. 

Potential drawback of Nelder-Mead: large dimensional problems.

# Reference method

## GLMMadaptive linear time effect

- Here we can fit a model with random intercept and slope for time. It is doable on 500 datasets, but obviously too simple for the data generation scheme

$$g(E(Y_i(t))) = \beta_0+\beta_1t+b_{i0}+b_{i1}t$$


```{r}
# model fit
t1 <- Sys.time()
fit_adglmm <- mixed_model(Y ~ t, random = ~ t | id, 
                          data =  train_df, family = binomial())
t2 <- Sys.time()
# t2-t1
```





```{r}
# out-of-sample prediction
pred_lin_glmm <- list(
  predict(fit_adglmm, 
          newdata = test_df %>% filter(t<=0.2),
          newdata2 =  test_df %>% filter(t>0.2),
          type = "subject_specific", type_pred = "link",
          return_newdata = TRUE)$newdata2,
  
  predict(fit_adglmm, 
          newdata = test_df %>% filter(t<=0.4),
          newdata2 =  test_df %>% filter(t>0.4),
          type = "subject_specific", type_pred = "link",
          return_newdata = TRUE)$newdata2,
  
  predict(fit_adglmm, 
          newdata = test_df %>% filter(t<=0.6),
          newdata2 =  test_df %>% filter(t>0.6),
          type = "subject_specific", type_pred = "link",
          return_newdata = TRUE)$newdata2,
  
  predict(fit_adglmm, 
          newdata = test_df %>% filter(t<=0.8),
          newdata2 =  test_df %>% filter(t>0.8),
          type = "subject_specific", type_pred = "link",
          return_newdata = TRUE)$newdata2
  )
```


```{r}
pred_lin_glmm <- lapply(pred_lin_glmm, 
                        function(df){df %>% select(id, t, pred)})

pred_lin_glmm_df <- test_df %>%
  left_join(pred_lin_glmm[[1]], by = c("id", "t")) %>%
  rename(pred0.2 = pred) %>%
  left_join(pred_lin_glmm[[2]], by = c("id", "t")) %>%
  rename(pred0.4 = pred) %>%
  left_join(pred_lin_glmm[[3]], by = c("id", "t")) %>%
  rename(pred0.6 = pred) %>%
  left_join(pred_lin_glmm[[4]], by = c("id", "t")) %>%
  rename(pred0.8 = pred) 
```

```{r}
p2 <- pred_lin_glmm_df %>%
  filter(id %in% rand_test_id) %>%
  mutate_at(vars(eta_i, pred0.2, pred0.4, pred0.6, pred0.8), function(x){exp(x)/(1+exp(x))}) %>%
  ggplot()+
  geom_point(aes(x=t, y=Y), size = 0.2)+
  geom_line(aes(x=t, y=eta_i, col = "True"))+
  geom_line(aes(x=t, y=pred0.2, col = "0.2"), linetype="dashed", na.rm=T)+
  geom_line(aes(x=t, y=pred0.4, col = "0.4"), linetype="dashed", na.rm=T)+
  geom_line(aes(x=t, y=pred0.6, col = "0.6"), linetype="dashed", na.rm=T)+
  geom_line(aes(x=t, y=pred0.8, col = "0.8"), linetype="dashed", na.rm=T)+
  facet_wrap(~id)+
  labs(title = "GLMMadaptive")
```


```{r, fig.height=5, fig.width=10}
ggarrange(p1, p2, nrow = 1, common.legend = T)
```

```{r}
# calcualte ISE
err2 <- pred_lin_glmm_df %>%
  mutate(err1 = (pred0.2-eta_i)^2,
         err2 = (pred0.4-eta_i)^2,
         err3 = (pred0.6-eta_i)^2,
         err4 = (pred0.8-eta_i)^2) %>%
  select(id, t, starts_with("err"))

err2$window = cut(err2$t, breaks = seq(0, 1, by = 0.2), include.lowest = T)
# table(err1$window)
err2 <- split(err2, f = err2$window)

tb2 <- lapply(err2, function(x){
  x %>%  group_by(id) %>% 
    summarise_at(vars(err1, err2, err3, err4), sum) %>%
    summarise_at(vars(err1, err2, err3, err4), mean)
}) %>% bind_rows(.id = "Window")
```

```{r}
# calcualte ISE
options(knitr.kable.NA = "")

ise <- full_join(tb1, tb2, by = "Window")
colnames(ise) <-c("Window",  rep(seq(0.2, 0.8, by =0.2), 2))
ise %>%
  kable(digits = 3, table.attr = "style = \"color: black;\"",
        caption = "ISE") %>%
  kable_styling(full_width = F) %>% 
  add_header_above(c(" "= 1, "fGFPCA" = 4, "GLMMadaptive" = 4)) %>%
  add_header_above(c(" "=1, "Maximum observation time" = 8))



```



```{r}
# calcualte AUC
auc_df2 <- pred_lin_glmm_df %>%
  mutate(window = cut(t, breaks = seq(0, 1, by = 0.2), include.lowest = T)) %>%
  select(Y, starts_with("pred"), window)


auc_mat2 <- matrix(NA, length(pred_window), length(obs_track))

for(j in seq_along(obs_track)){
  for(i in (j+1):5){
    this_Y <- auc_df2$Y[auc_df2$window==pred_window[i]]
    this_pred <- auc_df2[auc_df2$window==pred_window[i],
                         paste0("pred", obs_track[j])]
    this_perf <- performance(prediction(this_pred, this_Y), measure = "auc")
    auc_mat2[i ,j] <- this_perf@y.values[[1]]
  }
}

```

```{r}
# calcualte AUC
options(knitr.kable.NA = "")

auc <- cbind(auc_mat, auc_mat2)
rownames(auc) <- unique(auc_df1$window)
colnames(auc) <-c(rep(seq(0.2, 0.8, by = 0.2), 2))
auc %>%
  kable(digits = 3, table.attr = "style = \"color: black;\"",
        caption = "AUC") %>%
  kable_styling(full_width = F) %>% 
  add_header_above(c(" "= 1, "fGFPCA" = 4, "GLMMadaptive" = 4)) %>%
  add_header_above(c(" "=1, "Maximum observation time" = 8))
```


## Flexible GLMM

Due to computation time, I took only 10 subjects for training set. In this case, the model took 30 minutes to fit. Perhaps we could do a few more iterations? 

I use five basis functions for both fixed and random effects.

$$g(E(Y_i(t))) = \sum_{k=1}^5\zeta_{k}B_k(t)+\sum_{l=1}^5\xi_{il}\phi_l(t)$$


```{r}
# subset 10 subjects from training dataset 
df_train_sub <- train_df %>% filter(id %in% 1:10) %>%
  filter(t >= 0.375 & t <= 0.708)
df_train_sub$id <- droplevels(df_train_sub$id)

df_test_sub <- test_df %>% filter(id %in% 1:10) %>% 
  filter(t >= 0.375 & t <= 0.708)
df_test_sub$id <- droplevels(df_test_sub$id)
# levels(df_test_sub$id)
```


```{r flex_adglmm, cache=T}
t1 <- Sys.time()
fit_adglmm_dense <- mixed_model(fixed = Y ~ 0+ns(t, df = 5), 
                          random = ~ 0+ns(t, df = 5) | id, 
                          data =  df_train_sub, family = binomial())
t2 <- Sys.time()
t2-t1
```


As we have talked about from last meeting, I wanna use ns() wrapped in the GLMMadaptive::mixed_model() to see if this could speed things up. The model fit took 
`r round(t2-t1, 2)` mins to fit. 

```{r}
# given observation from 0.2, 0.4, 0.6, 0.8, predict the rest
pred_nonlin_adglmm <- list(
  predict(fit_adglmm_dense, 
          newdata = df_test_sub %>% filter(t<=0.2),
          newdata2 =  df_test_sub %>% filter(t>0.2),
          type = "subject_specific", type_pred = "link",
          return_newdata = TRUE)$newdata2,
  predict(fit_adglmm_dense, 
          newdata = df_test_sub %>% filter(t<=0.4),
          newdata2 =  df_test_sub %>% filter(t>0.4),
          type = "subject_specific", type_pred = "link",
          return_newdata = TRUE)$newdata2,
  predict(fit_adglmm_dense, 
          newdata = df_test_sub %>% filter(t<=0.6),
          newdata2 =  df_test_sub %>% filter(t>0.6),
          type = "subject_specific", type_pred = "link",
          return_newdata = TRUE)$newdata2,
  predict(fit_adglmm_dense, 
          newdata = df_test_sub %>% filter(t<=0.8),
          newdata2 =  df_test_sub %>% filter(t>0.8),
          type = "subject_specific", type_pred = "link",
          return_newdata = TRUE)$newdata2
  )
```


```{r}
pred_nonlin_adglmm <- lapply(pred_nonlin_adglmm, 
                        function(df){df %>% select(id, t, pred)})

pred_nonlin_adglmm_df <- df_test_sub %>%
  left_join(pred_nonlin_adglmm[[1]], by = c("id", "t")) %>%
  rename(pred0.2 = pred) %>%
  left_join(pred_nonlin_adglmm[[2]], by = c("id", "t")) %>%
  rename(pred0.4 = pred) %>%
  left_join(pred_nonlin_adglmm[[3]], by = c("id", "t")) %>%
  rename(pred0.6 = pred) %>%
  left_join(pred_nonlin_adglmm[[4]], by = c("id", "t")) %>%
  rename(pred0.8 = pred) 

# pred_nonlin_adglmm_df %>% filter(t>0.4)
```


```{r}
p1_nonlin <- pred_nonlin_adglmm_df %>%
  filter(id %in% 1:4) %>%
  mutate_at(vars(eta_i, pred0.2, pred0.4, pred0.6, pred0.8),
            function(x){exp(x)/(1+exp(x))}) %>%
  ggplot()+
  geom_point(aes(x=t, y=Y), size = 0.2)+
  geom_line(aes(x=t, y=eta_i, col = "True"))+
  geom_line(aes(x=t, y=pred0.2, col = "0.2"), linetype="dashed", na.rm=T)+
  geom_line(aes(x=t, y=pred0.4, col = "0.4"), linetype="dashed", na.rm=T)+
  geom_line(aes(x=t, y=pred0.6, col = "0.5"), linetype="dashed", na.rm=T)+
  geom_line(aes(x=t, y=pred0.8, col = "0.8"), linetype="dashed", na.rm=T)+
  facet_wrap(~id)+
  labs(title = "GLMMadaptive + spline basis prediction")

```


```{r}
# calcualte ISE
err3 <- pred_nonlin_adglmm_df %>%
  mutate(err1 = (pred0.2-eta_i)^2,
         err2 = (pred0.4-eta_i)^2,
         err3 = (pred0.6-eta_i)^2,
         err4 = (pred0.8-eta_i)^2) %>%
  select(id, t, starts_with("err"))

err3$window = cut(err3$t, breaks = seq(0, 1, by = 0.2), include.lowest = T)
# table(err1$window)
err3 <- split(err3, f = err3$window)

tb3 <- lapply(err3, function(x){
  x %>%  group_by(id) %>% 
    summarise_at(vars(err1, err2, err3, err4), sum) %>%
    summarise_at(vars(err1, err2, err3, err4), mean)
}) %>% bind_rows(.id = "Window")
```

For the comparing fGFPCA, if we decrease the measurement density to 100 points, a lot of numeric problems popped up in the either local models or Laplace Approximation process. It looks like when observation track is short, GLMMadaptive has a better prediction.  

I wonder if we should decrease the obervation track instead of decrease density. Let's say we only think about 9am-5pm (t from 0.375 to 0.208). 

```{r fGFPCA}
# bin data
# J_sub <- length(unique(df_train_sub$t))
# df_train_sub$sind <- rep(1:J_sub, 10)
# t_sub <- unique(df_train_sub$t)
# bin_w <- 4
  
# n_bin_sub <- J_sub/bin_w # number of bins
# brks <- seq(0, J_sub, by = bin_w) # cutoff points on time index
# mid <- (brks+bin_w/2)[1:n_bin_sub] # interval mid point time index
# mid_t <- t_sub[mid] 
# 
# df_train_sub$bin <- cut(df_train_sub$sind,
#                         breaks = brks, include.lowest = T, labels = mid)
# df_train_sub$bin <- as.numeric(as.character(df_train_sub$bin))
```


```{r}
# fit fGFPCA model on the training set
t1 <- Sys.time()

## Step 2: local GLMM and estimate latent function
## on training set
bin_lst_sub <- split(df_train_sub, f = df_train_sub$bin)
df_est_latent_sub <- lapply(bin_lst_sub, function(x){pred_latent(x, n_node = 0)}) 
# boundary hitting in this process
df_est_latent_sub <- bind_rows(df_est_latent_sub) 
# range(df_est_latent_sub$eta_hat)
```


```{r}
## Step 3: FPCA
uni_eta_hat_sub <- df_est_latent_sub %>% filter(bin==sind)
mat_est_unique <- matrix(uni_eta_hat_sub$eta_hat,
                         nrow=10, 
                         ncol=n_bin, byrow = F) 
fpca_mod_sub <- fpca.face(mat_est_unique, argvals = mid_t, var=T)
```

```{r}
## Step 4: project and debias
## Projection
# values used for projection
p <- 3 # order of b splines 
knots <- 35 # number of knots (same from FPCA model)
knots_values <- seq(-p, knots + p, length = knots + 1 + 2 *p)/knots
knots_values <- knots_values * (max(mid_t) - min(mid_t)) + min(mid_t)
B <- spline.des(knots = knots_values, x = mid_t, ord = p + 1,
                outer.ok = TRUE)$design  # evaluate B-splines on binned grid
Bnew <- spline.des(knots = knots_values, x = t, ord = p + 1,
                   outer.ok = TRUE)$design  # evaluate B-splines on original grid
df_phi <- matrix(NA, J, K) 
for(k in 1:K){
  lm_mod <- lm(fpca_mod_sub$efunctions[,k] ~ B-1)
  df_phi[,k] <- Bnew %*% coef(lm_mod)
}# project binned eigenfunctions onto the original grid
```

```{r}
## debias
df_phi <- data.frame(t = t, df_phi)
colnames(df_phi) <- c("t", paste0("phi", 1:4))
df_train_sub <- df_train_sub %>% 
  select(-starts_with("phi")) %>%
  left_join(df_phi, by = "t")
train_df$id <- as.factor(train_df$id)
debias_glmm <- bam(Y ~ s(t, bs="cc", k=10)+
                     s(id, by=phi1, bs="re")+
                     s(id, by=phi2, bs="re")+
                     s(id, by=phi3, bs="re")+
                     s(id, by=phi4, bs="re"), 
                   family = binomial, data=df_train_sub, 
                   method = "fREML",
                   discrete = TRUE)
new_mu <- predict(debias_glmm, type = "terms")[1:J, 1] # extract re-evaluated mean
new_lambda <- 1/debias_glmm$sp[2:5] # extract re-evaluated lambda
# rescale
new_phi <- df_phi %>% select(starts_with("phi"))*sqrt(n_bin)
new_phi <- as.matrix(new_phi)
new_lambda <- new_lambda/n_bin
t2 <- Sys.time()

```


```{r, results='hide'}
df_pred_sub<- df_test_sub
converge_st <- matrix(NA, 10, 4)

for(i in 1:10){
    df_i <- df_test_sub %>% filter(id==i)
    
    # per max obs time
    for(tmax in c(0.2, 0.4, 0.6, 0.8)){
      df_it <- df_i %>% filter(t <= tmax)
      max_rid <- nrow(df_it)
      
      # into a list
      MyData <- list(K=K, 
                     X=new_phi[1:max_rid, ], 
                     mon.names=mon.names,
                     parm.names=parm.names, 
                     pos.xi=pos.xi, 
                     y=df_it$Y, 
                     tao=diag(new_lambda), f0=new_mu[1:max_rid])
      
      
      # fit laplace approximation
      Fit <- LaplaceApproximation(Model, parm = rep(0, K), 
                                  Data=MyData, Method = "NM", Iterations = 1000,
                                  CovEst = "Identity")
      converge_st[i, which(c(0.2, 0.4, 0.6, 0.8)==tmax)] <- Fit$Converged
      score <- Fit$Summary1[, "Mode"]
      
      # prediction
      eta_pred_out <- new_mu+new_phi%*%score
      df_pred_sub[df_pred_sub$id==i, paste0("pred", tmax)] <- eta_pred_out[,1]
    }
}
    

```



```{r}
df_pred_sub$pred0.2[df_pred_sub$t<=0.2] <- NA
df_pred_sub$pred0.4[df_pred_sub$t<=0.4] <- NA
df_pred_sub$pred0.6[df_pred_sub$t<=0.6] <- NA
df_pred_sub$pred0.8[df_pred_sub$t<=0.8] <- NA

p2_fgfpca <- df_pred_sub %>%
  filter(id %in% 1:4) %>%
  mutate_at(vars(eta_i, pred0.2, pred0.4, pred0.6, pred0.8),
            function(x){exp(x)/(1+exp(x))}) %>%
  ggplot()+
  geom_point(aes(x=t, y=Y), size = 0.2)+
  geom_line(aes(x=t, y=eta_i, col = "True"))+
  geom_line(aes(x=t, y=pred0.2, col = "0.2"), linetype="dashed", na.rm=T)+
  geom_line(aes(x=t, y=pred0.4, col = "0.4"), linetype="dashed", na.rm=T)+
  geom_line(aes(x=t, y=pred0.6, col = "0.5"), linetype="dashed", na.rm=T)+
  geom_line(aes(x=t, y=pred0.8, col = "0.8"), linetype="dashed", na.rm=T)+
  facet_wrap(~id)+
  labs(title = "fGFPCA")

```


```{r}
# calcualte ISE
err4 <- df_pred_sub %>%
  mutate(err1 = (pred0.2-eta_i)^2,
         err2 = (pred0.4-eta_i)^2,
         err3 = (pred0.6-eta_i)^2,
         err4 = (pred0.8-eta_i)^2) %>%
  select(id, t, starts_with("err"))

err4$window = cut(err4$t, breaks = seq(0, 1, by = 0.2), include.lowest = T)
# table(err1$window)
err4 <- split(err4, f = err4$window)

tb4 <- lapply(err4, function(x){
  x %>%  group_by(id) %>% 
    summarise_at(vars(err1, err2, err3, err4), sum) %>%
    summarise_at(vars(err1, err2, err3, err4), mean)
}) %>% bind_rows(.id = "Window")
```

```{r}
# calcualte ISE
options(knitr.kable.NA = "")

ise <- full_join(tb4, tb3, by = "Window")
colnames(ise) <-c("Window",  rep(seq(0.2, 0.8, by =0.2), 2))
ise %>%
  kable(digits = 3, table.attr = "style = \"color: black;\"",
        caption = "ISE") %>%
  kable_styling(full_width = F) %>% 
  add_header_above(c(" "= 1, "fGFPCA" = 4, "GLMMadaptive" = 4)) %>%
  add_header_above(c(" "=1, "Maximum observation time" = 8))
```

```{r, fig.height=5, fig.width=10}
ggarrange(p1_nonlin, p2_fgfpca, nrow = 1, common.legend = T)
```


## Random thouds

