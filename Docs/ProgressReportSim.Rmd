---
title: "Progress Report on One Simulated Dataset"
author: "Ying Jin"
date: "2023-09-15"
output: 
  html_document:
    self_contained: no
    number_sections: true
    code_folding: hide
    toc: true
    toc_depth: 3
    toc_float: true
    font: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

set.seed(516)

library(here)
library(tidyverse)
library(ggpubr)
library(refund)
library(lme4)
library(ROCR)
library(GLMMadaptive)
library(kableExtra)
library(knitr)
library(mvtnorm)
library(mgcv)
library(splines)
theme_set(theme_minimal())

# df <- read_rds(here("Data/nhanes_bi.rds"))
# load(here("Data/ApplOutput_fGFPCA.RData"))

# code
source(here("Code/GLMM-FPCA.R")) 
# source(here("Code/OutSampMLE.R"))
source(here("Code/OutsampBayes.R"))
# source(here("Code/OutsampBayes.R"))

```


I am now on the way to add the debias step to the whole algorithm. Just so that I have a set of "true parameters" for comparison, I am gonna use the simulation dataset for this part.

# Generation mechanism

- I am gonna generate only one dataset as an example
- 500 individuals each with 1000 measures

\[\begin{aligned}
Y_i(t) & \sim Bernoulli(\frac{exp(\eta_i(t))}{1+exp(\eta_i(t))}) \\
\eta_i(t) &= f_0(t)+ \xi_{i1}\sqrt{2}sin(2\pi t)+\xi_{i2}\sqrt{2}cos(2\pi t)+\xi_{i3}\sqrt{2}sin(4\pi t)+\xi_{i4}\sqrt{2}cos(4\pi t)
\end{aligned}\]

where:

- $t$ is equal-spaced on $[0, 1]$
- $f_0(t)=0$
- $\xi_k \sim N(0, \lambda_k)$, and $\lambda_k = 1, 0.5, 0.25, 0.125$ for k = 1, 2, 3, 4 respectively. 
- In fact, since there is a constant factor $\sqrt{2}$ before each eigenfunction, perhaps the real eigenvalues are $2, 1, 0,5, 0.25$. 


```{r gen_data, class.source='show'}
N <- 500 # sample size
J <- 1000 # number of observation points

t = seq(0,1,len=J) # observations points

# mean function
f_0 <- function(s) 0 

#eigenfunctions 
K <- 4 # number of eigenfunctions
phi <- sqrt(2)*cbind(sin(2*pi*t),cos(2*pi*t),
                     sin(4*pi*t),cos(4*pi*t))

# eigenvalues
lambda = 0.5^(0:(K-1)) 

# generated data 
## score
xi <- matrix(rnorm(N*K),N,K)
xi <- xi %*% diag(sqrt(lambda))
  
# subject-specific random effect
b_i <- xi %*% t(phi); # of size N by J
  
# latent gaussian function
eta_i <- t(vapply(1:N, function(x){
    f_0(t) + b_i[x,]
  }, numeric(J)))
  
# outcome binary function
Y_i <- matrix(rbinom(N*J, size=1, prob=plogis(eta_i)), 
                N, J, byrow=FALSE)
  
# format into dataframe
# id = subject identifier (factor variable)
# sind = numeric value corresponding to the observed functional domain
# Y = functional response (binary)
# sind_inx = numeric value associated with order of "sind"
#            this is not necessary, but may be of convenience when you implement the method
df <- data.frame(id = factor(rep(1:N, each=J)),
                 t = rep(t, N), 
                 Y = as.vector(t(Y_i)),
                 eta_i = as.vector(t(eta_i)),
                 sind = rep(1:J, N))
# visualization
rand_id <- sample(N, size = 4)
df %>% filter(id %in% rand_id) %>% 
  ggplot()+
  geom_point(aes(x=sind, y=Y), size = 0.5)+
  geom_line(aes(x=sind, y=plogis(eta_i)), col = "red")+
  #geom_line(aes(x=sind_inx, y=eta_i), col = "blue")+
  facet_wrap(~id)
```

# fGFPCA model fitting

## Bin data

- Bin every 10 consecutive observations

```{r, message=FALSE}
# bin data
bin_w <- 10 # bin width
n_bin <- J/bin_w # number of bins
brks <- seq(0, J, by = bin_w) # cutoff points
mid <- (brks+bin_w/2)[1:n_bin] # mid points

df$bin <- cut(df$sind, breaks = brks, include.lowest = T, labels = mid)
df$bin <- as.numeric(as.character(df$bin))
# unique(df$bin)

df %>% 
  filter(id %in% rand_id) %>%
  group_by(id, bin) %>%
  summarise(num = sum(Y)) %>%
  ggplot()+
  geom_point(aes(x=bin, y=num), size = 0.5)+
  facet_wrap(~id)+
  labs(x="Time", y = "Activity", title = "Number of active nimutes within each bin")
```

## Data split

- 400 (80%) subjects for training, 100 (20%) for out-of-sample prediction

```{r}
train_id <- sample(unique(df$id), size = N*0.8)
# length(train_id)
test_id <- setdiff(unique(df$id), train_id)

train_df <- df %>% filter(id %in% train_id)
test_df <- df %>% filter(id %in% test_id)
```

## Local GLMM

- Used glmer with PIRLS (nAGQ=0) for local GLMMs step
- Fit on the training set

```{r, cache=TRUE}
# fit model on the training set
train_bin_lst <- split(train_df, f = train_df$bin)

# local GLMM and estimate latent function
# use PIRLS (nAGQ=0) to avoid near-unidentifiability issues 
t1=Sys.time()
df_est_latent <- lapply(train_bin_lst, function(x){pred_latent(x, n_node = 0)}) 
t2= Sys.time()
t_local_glmm <- t2-t1 
```


```{r}
df_est_latent <- bind_rows(df_est_latent) 
# head(df_est_latent)

# example estimated latent function
rand_id <- sample(train_id, 4)

df_est_latent %>% 
  filter(id %in% rand_id) %>%
  mutate(eta_hat = exp(eta_hat)/(1+exp(eta_hat))) %>%
  mutate(eta_i = exp(eta_i)/(1+exp(eta_i))) %>%
  ggplot()+
  geom_line(aes(x=sind, y=eta_hat, group = id, col = "estimated"))+
  geom_line(aes(x=sind, y=eta_i, group = id, col = "true"))+
  geom_point(aes(x=sind, y = Y, group = id), size = 0.5)+
  facet_wrap(~id, scales = "free")+
  labs(x = "Time", y = "Estimated latent function (probablity scale)")
```

## FPCA

```{r}
uni_eta_hat <- df_est_latent %>% filter(bin==sind)

mat_est_unique <- matrix(uni_eta_hat$eta_hat,
                         nrow=length(train_id), 
                         ncol=n_bin, byrow = F) 
# row index subject, column binned time
# dim(mat_est_unique)

t1 <- Sys.time()
fpca_mod <- fpca.face(mat_est_unique, argvals = mid, var=T)
t2 <- Sys.time()
t_fpca <- t2-t1 # 3.21 seconds to fit fPCA model
```

- Estimated mean

```{r}
plot(mid, fpca_mod$mu, type = "l", xlab = "bin", ylab = "Mean")
```

- Estimated eigenfunctions

```{r, fig.height=8, fig.width=8}
par(mfrow=c(2,2))

plot(mid, fpca_mod$efunctions[, 1], type="l", xlab="bin", ylab="PC1")
plot(mid, fpca_mod$efunctions[, 2], type="l", xlab="bin", ylab="PC2")
plot(mid, fpca_mod$efunctions[, 3], type="l", xlab="bin", ylab="PC3")
plot(mid, fpca_mod$efunctions[, 4], type="l", xlab="bin", ylab="PC4")
```

- Correlation matrix

```{r}
# plot correlation matrix
heatmap(cov2cor(fpca_mod$VarMats[[1]]), Rowv = NA, Colv = NA, main = "Correlation")
```

- Estimated eigenvalues

```{r, class.source='fold-show'}
# estimated eigenvalues from FPCA (biased)
fpca_mod$evalues[1:4]
```


## Debias (step 4)

According to the fGFPCA paper, in this step we do two things: 

1. Project the eigenfunctions with B-spline basis back to the original grid
2. Debias the eigenvalues with GLMM (mgcv::bam, mgcm::gamm or gamm4:gamm) to be used for Laplace approximation


### Porjection with B-spline basis

- Use fastGFPCA::reeval_efunctions. Cannot install the package so used the source code directly
- Not sure how to decide the knots. I am using the bin boundaries for now (0, 10, 20, ...,1000), which is 101 knots. 

```{r, class.source='fold-show'}
# order of b splines
p <- 3 

# number of knots
# not sure how to determine
# maybe for now use the bin endpoints? 
# knot_pos <- seq(0, J, by = 50)

# evaluate B-splines on binned grid
B <- spline.des(knots = brks, x = mid, ord = p + 1,
                            outer.ok = TRUE)$design
# evaluate B-splines on original grid
Bnew <- spline.des(knots = brks, x = 1:J, ord = p + 1,
                  outer.ok = TRUE)$design

# project binned eigenfunctions onto the original grid
efunctions_new <- matrix(NA, J, K)
for(k in 1:K){
    lm_mod <- lm(fpca_mod$efunctions[,k] ~ B-1)
    efunctions_new[,k] <- Bnew %*% coef(lm_mod)
}
```


```{r, fig.height=8, fig.width=8}
# visualization
par(mfrow=c(2,2))

plot(mid, fpca_mod$efunctions[, 1], xlab="bin", ylab="PC1", pch=20, cex = 0.5)
lines(1:J, efunctions_new[, 1], col="blue")

plot(mid, fpca_mod$efunctions[, 2], xlab="bin", ylab="PC2", pch=20, cex = 0.5)
lines(1:J, efunctions_new[, 2], col="blue")

plot(mid, fpca_mod$efunctions[, 3], xlab="bin", ylab="PC3", pch=20, cex = 0.5)
lines(1:J, efunctions_new[, 3], col="blue")

plot(mid, fpca_mod$efunctions[, 4], xlab="bin", ylab="PC4", pch=20, cex = 0.5)
lines(1:J, efunctions_new[, 4], col="blue")


```

- The projected eigenfunctions seem to be more wiggly than we would like.
- What is the reason for choosing projection over linear/spline interpolation? 

#### Number of knots

- Try fewer knots (11 knots)? 
- Looks like the shape does not fit to the binned grid very well. 

```{r}
# 11 knots
knot_pos <- seq(0, J, by = 100)

# evaluate B-splines on binned grid
B <- spline.des(knots = knot_pos, x = mid, ord = p + 1,
                            outer.ok = TRUE)$design
# evaluate B-splines on original grid
Bnew <- spline.des(knots = knot_pos, x = 1:J, ord = p + 1,
                  outer.ok = TRUE)$design

# project binned eigenfunctions onto the original grid
efunctions_new_11knots <- matrix(NA, J, K)
for(k in 1:K){
    lm_mod <- lm(fpca_mod$efunctions[,k] ~ B-1)
    efunctions_new_11knots[,k] <- Bnew %*% coef(lm_mod)
}
```


```{r, fig.height=8, fig.width=8}
# visualization
par(mfrow=c(2,2))

plot(mid, fpca_mod$efunctions[, 1], xlab="bin", ylab="PC1", pch=20, cex = 0.5)
lines(1:J, efunctions_new_11knots[, 1], col="blue")

plot(mid, fpca_mod$efunctions[, 2], xlab="bin", ylab="PC2", pch=20, cex = 0.5)
lines(1:J, efunctions_new_11knots[, 2], col="blue")

plot(mid, fpca_mod$efunctions[, 3], xlab="bin", ylab="PC3", pch=20, cex = 0.5)
lines(1:J, efunctions_new_11knots[, 3], col="blue")

plot(mid, fpca_mod$efunctions[, 4], xlab="bin", ylab="PC4", pch=20, cex = 0.5)
lines(1:J, efunctions_new_11knots[, 4], col="blue")


```


#### Order of spline

- Try lower order? (order = 3)
- While the middle part fits alright, the end really goes crazy off. 

```{r}
# evaluate B-splines on binned grid
B <- spline.des(knots = brks, x = mid, ord = 3,
                            outer.ok = TRUE)$design
# evaluate B-splines on original grid
Bnew <- spline.des(knots = brks, x = 1:J, ord = 3,
                  outer.ok = TRUE)$design

# project binned eigenfunctions onto the original grid
efunctions_new_2p <- matrix(NA, J, K)
for(k in 1:K){
    lm_mod <- lm(fpca_mod$efunctions[,k] ~ B-1)
    efunctions_new_2p[,k] <- Bnew %*% coef(lm_mod)
}
```


```{r, fig.height=8, fig.width=8}
# visualization
par(mfrow=c(2,2))

plot(mid, fpca_mod$efunctions[, 1], xlab="bin", ylab="PC1", pch=20, cex = 0.5)
lines(1:J, efunctions_new_2p[, 1], col="blue")

plot(mid, fpca_mod$efunctions[, 2], xlab="bin", ylab="PC2", pch=20, cex = 0.5)
lines(1:J, efunctions_new_2p[, 2], col="blue")

plot(mid, fpca_mod$efunctions[, 3], xlab="bin", ylab="PC3", pch=20, cex = 0.5)
lines(1:J, efunctions_new_2p[, 3], col="blue")

plot(mid, fpca_mod$efunctions[, 4], xlab="bin", ylab="PC4", pch=20, cex = 0.5)
lines(1:J, efunctions_new_2p[, 4], col="blue")


```


### Debias with eigenvalues

- The bias was caused by the misspecification of latent process (assume constant effect over smooth function). 
- What needs re-evaluation are eigenvalues (variance estimates). They will be used in Laplace Approximation for out-of-sample prediction. 
- We do not need to debias individual scores because the scores for training sample will not be used for out-of-sample prediction. And out-of-sample scores are not estimated by FPCA but Laplace approximation.
- Since I don't yet know what to do with the projection, I will here use the un-projected basis functions and do things on the binned grid. I am using mgcv::bam and setting method = "fREML" and discrete = TRUE to speed up computation

```{r}
# dim(fpca_mod$efunctions)
df_phi <- data.frame(bin = mid, fpca_mod$efunctions[, 1:K])
colnames(df_phi) <- c("bin", paste0("phi", 1:4))

# train_df$bin <- as.numeric(as.character(train_df$bin))
train_df <- train_df %>% left_join(df_phi, by = "bin")
train_df$id <- as.factor(train_df$id)
```

- Also, I am not exactly sure if I should regress on t (original time between 0 and 1) or the index of bin midpoints. I will do the bin midpoints for now because I think that's in the fGFPCA manuscript? 

```{r, class.source = "fold-show"}
# usethis::edit_r_environ()
t1 <- Sys.time()
debias_glmm <- bam(Y ~ s(bin, bs="cr", k=10)+
                     s(id, by=phi1, bs="re")+
                     s(id, by=phi2, bs="re")+
                     s(id, by=phi3, bs="re")+
                     s(id, by=phi4, bs="re"), 
                   family = binomial, data=train_df, 
                   method = "fREML",
                   discrete = TRUE)
t2 <- Sys.time()
t_debias <- t2-t1
```


And now I try to extract the variance of scores (debiased eigenvalues), using the conclusion that connects smoothing parameters to variance

$$\hat{p}_k=1/\hat{\lambda}_k^2$$

```{r, class.source='fold-show'}
# transformed smoothing parameters
new_lambda <- 1/debias_glmm$sp[2:5]
new_lambda
# new_lambda/lambda
# new_lambda/fpca_mod$evalues[1:4]
```

Looks like the re-evaluated variance is still inflated from the true eigenvalues, but indeed doubled the result of FPCA. 

How can we know this is the actual true eigenvalues? 

# Out-of-sample prediction with Laplace approximation

I will now estimate individual score of test sample using Laplace Approximation, and calculated the predicted track.
- Mximum observations time: 200, 400, 600. 800
- Prediction window: 200-400, 400-600, 600-800, 800-1000

## With eigenvalues from FPCA

```{r, results='hide'}
# container
pred_list <- list()
# score <- array(NA, dim=c(length(test_id), K, 4))
# dimensions: subject, eigenfunction, maximum observation time

# prediction for a single subject
t1 <- Sys.time()
for(i in seq_along(test_id)){
  df_i <- test_df %>% filter(id==test_id[i])
  # prediction 
  pred1 <- out_pred_laplace(mu = fpca_mod$mu, 
                            evalues = fpca_mod$evalues[1:K], 
                            phi_mat = fpca_mod$efunctions[, 1:K], 
                            df_new = df_i %>% filter(sind<=200), kpc = K)
  pred2 <- out_pred_laplace(mu = fpca_mod$mu, 
                            evalues = fpca_mod$evalues[1:K], 
                            phi_mat = fpca_mod$efunctions[, 1:K],
                            df_i %>% filter(sind<=400), kpc = K)
  pred3 <- out_pred_laplace(mu = fpca_mod$mu, 
                            evalues = fpca_mod$evalues[1:K], 
                            phi_mat = fpca_mod$efunctions[, 1:K],
                            df_i %>% filter(sind<=600), kpc = K)
  pred4 <- out_pred_laplace(mu = fpca_mod$mu, 
                            evalues = fpca_mod$evalues[1:K], 
                            phi_mat = fpca_mod$efunctions[, 1:K],
                            df_i %>% filter(sind<=800), kpc = K)
  
  
  pred_i <- data.frame(id = test_id[i], bin=mid, 
                       pred1=pred1$eta_pred, pred2=pred2$eta_pred,
                       pred3=pred3$eta_pred, pred4=pred4$eta_pred)
  
  pred_i$pred1[pred_i$bin<=200] <- NA
  pred_i$pred2[pred_i$bin<=400] <- NA
  pred_i$pred3[pred_i$bin<=600] <- NA
  pred_i$pred4[pred_i$bin<=800] <- NA
  
  pred_list[[i]] <- pred_i
}
t2 <- Sys.time()
t_pred <- t2-t1
```


```{r}
# overview of predicted track
rand_test_id <- sample(test_id, 4)

test_df %>%
  left_join(bind_rows(pred_list), by = c("id", "bin")) %>%
  filter(id %in% rand_test_id) %>%
  mutate_at(vars(eta_i, pred1, pred2, pred3, pred4), function(x){exp(x)/(1+exp(x))}) %>%
  ggplot()+
  geom_point(aes(x=sind, y=Y), size = 0.2)+
  geom_line(aes(x=sind, y=eta_i, col = "True"))+
  geom_line(aes(x=sind, y=pred1, col = "200"), linetype="dashed", na.rm=T)+
  geom_line(aes(x=sind, y=pred2, col = "400"), linetype="dashed", na.rm=T)+
  geom_line(aes(x=sind, y=pred3, col = "600"), linetype="dashed", na.rm=T)+
  geom_line(aes(x=sind, y=pred4, col = "800"), linetype="dashed", na.rm=T)+
  facet_wrap(~id)

```


## With debiased eigenvalues

```{r, results='hide'}
# container
pred_list2 <- list()
# score <- array(NA, dim=c(length(test_id), K, 4))
# dimensions: subject, eigenfunction, maximum observation time

# prediction for a single subject
t1 <- Sys.time()
for(i in seq_along(test_id)){
  df_i <- test_df %>% filter(id==test_id[i])
  # prediction 
  pred1 <- out_pred_laplace(mu = fpca_mod$mu, 
                            evalues = new_lambda, 
                            phi_mat = fpca_mod$efunctions[, 1:K], 
                            df_new = df_i %>% filter(sind<=200), kpc = K)
  pred2 <- out_pred_laplace(mu = fpca_mod$mu, 
                            evalues = new_lambda, 
                            phi_mat = fpca_mod$efunctions[, 1:K],
                            df_i %>% filter(sind<=400), kpc = K)
  pred3 <- out_pred_laplace(mu = fpca_mod$mu, 
                            evalues = new_lambda,
                            phi_mat = fpca_mod$efunctions[, 1:K],
                            df_i %>% filter(sind<=600), kpc = K)
  pred4 <- out_pred_laplace(mu = fpca_mod$mu, 
                            evalues = new_lambda,
                            phi_mat = fpca_mod$efunctions[, 1:K],
                            df_i %>% filter(sind<=800), kpc = K)
  
  pred_i <- data.frame(id = test_id[i], bin=mid, 
                       pred1=pred1$eta_pred, pred2=pred2$eta_pred,
                       pred3=pred3$eta_pred, pred4=pred4$eta_pred)
  
  pred_i$pred1[pred_i$bin<=200] <- NA
  pred_i$pred2[pred_i$bin<=400] <- NA
  pred_i$pred3[pred_i$bin<=600] <- NA
  pred_i$pred4[pred_i$bin<=800] <- NA
  
  pred_list2[[i]] <- pred_i
}
t2 <- Sys.time()
t_pred <- t2-t1
```


```{r}
# overview of predicted track
test_df %>%
  left_join(bind_rows(pred_list2), by = c("id", "bin")) %>%
  filter(id %in% rand_test_id) %>%
  mutate_at(vars(eta_i, pred1, pred2, pred3, pred4), function(x){exp(x)/(1+exp(x))}) %>%
  ggplot()+
  geom_point(aes(x=sind, y=Y), size = 0.2)+
  geom_line(aes(x=sind, y=eta_i, col = "True"))+
  geom_line(aes(x=sind, y=pred1, col = "200"), linetype="dashed", na.rm=T)+
  geom_line(aes(x=sind, y=pred2, col = "400"), linetype="dashed", na.rm=T)+
  geom_line(aes(x=sind, y=pred3, col = "600"), linetype="dashed", na.rm=T)+
  geom_line(aes(x=sind, y=pred4, col = "800"), linetype="dashed", na.rm=T)+
  facet_wrap(~id)
```

```{r}
t_1sim <- (t_local_glmm+t_fpca+t_debias+t_pred)
```

- Time spent on one iteration: `r round(t_1sim, 2)` seconds
- Did debiased eigenvalues improve performance? The lines above look like so. I also wanna see the squared errors and AUC.

## ISE

```{r}
# calcualte ISE
err1 <- test_df %>%
  left_join(bind_rows(pred_list), by = c("id", "bin")) %>%
  filter(bin==sind) %>%
  mutate(err1 = (pred1-eta_i)^2,
         err2 = (pred2-eta_i)^2,
         err3 = (pred3-eta_i)^2,
         err4 = (pred4-eta_i)^2) %>%
  select(id, bin, starts_with("err"))

err1$window = cut(err1$bin, breaks = seq(0, 1000, by = 200), include.lowest = T)
err1 <- split(err1, f = err1$window)

tb1 <- lapply(err1, function(x){
  x %>%  group_by(id) %>% 
    summarise_at(vars(err1, err2, err3, err4), sum) %>%
    summarise_at(vars(err1, err2, err3, err4), mean)
}) %>% bind_rows(.id = "Window")
```

```{r}
# calcualte ISE
err2 <- test_df %>%
  left_join(bind_rows(pred_list2), by = c("id", "bin")) %>%
  filter(bin==sind) %>%
  mutate(err1 = (pred1-eta_i)^2,
         err2 = (pred2-eta_i)^2,
         err3 = (pred3-eta_i)^2,
         err4 = (pred4-eta_i)^2) %>%
  select(id, bin, starts_with("err"))

err2$window = cut(err2$bin, breaks = seq(0, 1000, by = 200), include.lowest = T)
err2 <- split(err2, f = err2$window)

tb2 <- lapply(err2, function(x){
  x %>%  group_by(id) %>% 
    summarise_at(vars(err1, err2, err3, err4), sum) %>%
    summarise_at(vars(err1, err2, err3, err4), mean)
}) %>% bind_rows(.id="Window")

```


```{r}
# calcualte ISE
options(knitr.kable.NA = "")

ise <- full_join(tb1, tb2, by = "Window")
colnames(ise) <-c("Window",  rep(seq(200, 800, by =200), 2))
ise %>%
  kable(digits = 3, table.attr = "style = \"color: black;\"") %>%
  kable_styling(full_width = F) %>% 
  add_header_above(c(" "= 1, "FPCA eigenvalues" = 4, "Debiased eigenvalues" = 4)) %>%
  add_header_above(c(" "=1, "Maximum observation time" = 8))
```


- Looks like the debiased eigenvalues helped on most cases except the very last two, when trying to predict (800, 1000].

## AUC

```{r}
# calcualte AUC
# FPCA eigenvalues
auc_df1 <- test_df %>%
  left_join(bind_rows(pred_list), by = c("id", "bin")) %>%
  filter(bin==sind) %>%
  mutate(window = cut(bin, breaks = seq(0, 1000, by = 200), include.lowest = T)) %>%
  select(Y, starts_with("pred"), window)

obs_track <- seq(200, 800, by = 200)
pred_window <- unique(auc_df1$window)
auc_mat <- matrix(NA, length(pred_window), length(obs_track))

for(j in seq_along(obs_track)){
  for(i in (j+1):5){
    this_Y <- auc_df1$Y[auc_df1$window==pred_window[i]]
    this_pred <- auc_df1[auc_df1$window==pred_window[i],
                         paste0("pred", j)]
    this_perf <- performance(prediction(this_pred, this_Y), measure = "auc")
    auc_mat[i ,j] <- this_perf@y.values[[1]]
  }
}
```


```{r}
# calcualte AUC
# FPCA eigenvalues
auc_df2 <- test_df %>%
  left_join(bind_rows(pred_list2), by = c("id", "bin")) %>%
  filter(bin==sind) %>%
  mutate(window = cut(bin, breaks = seq(0, 1000, by = 200), include.lowest = T)) %>%
  select(Y, starts_with("pred"), window)

auc_mat2 <- matrix(NA, length(pred_window), length(obs_track))

for(j in seq_along(obs_track)){
  for(i in (j+1):5){
    this_Y <- auc_df2$Y[auc_df2$window==pred_window[i]]
    this_pred <- auc_df2[auc_df2$window==pred_window[i],
                         paste0("pred", j)]
    this_perf <- performance(prediction(this_pred, this_Y), measure = "auc")
    auc_mat2[i ,j] <- this_perf@y.values[[1]]
  }
}
```


```{r}
# calcualte ISE
options(knitr.kable.NA = "")

auc <- cbind(auc_mat, auc_mat2)
rownames(auc) <- unique(auc_df1$window)
colnames(auc) <-c(rep(seq(200, 800, by =200), 2))
auc %>%
  kable(digits = 3, table.attr = "style = \"color: black;\"") %>%
  kable_styling(full_width = F) %>% 
  add_header_above(c(" "= 1, "FPCA eigenvalues" = 4, "Debiased eigenvalues" = 4)) %>%
  add_header_above(c(" "=1, "Maximum observation time" = 8))
```

